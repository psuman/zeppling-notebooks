{
  "paragraphs": [
    {
      "text": "%md\n### Structured API\n- The Structured APIs are a tool for manipulating all sorts of data, from unstructured log files to semi-structured CSV files and highly structured Parquet files. These APIs refer to three core types of distributed collection APIs\n    - Datasets\n    - DataFrames\n    - SQL tables and views\n\n- Majority of the Structured APIs apply to both batch and streaming computation. \n- This means that when you work with the Structured APIs, it should be simple to migrate from batch to streaming (or vice versa) with little to no effort\n- The Structured APIs are the fundamental abstraction that you will use to write the majority of your data flows\n- DataFrames and Datasets are (distributed) table-like collections with well-defined rows and columns.\n- To Spark, DataFrames and Datasets represent immutable, lazily evaluated plans that specify what operations to apply to data residing at a location to generate some output\n- When we perform an action on a DataFrame, we instruct Spark to perform the actual transformations and return the result. These represent plans of how to manipulate rows and columns to compute the user’s desired result.",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 3:14:45 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eStructured API\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eThe Structured APIs are a tool for manipulating all sorts of data, from unstructured log files to semi-structured CSV files and highly structured Parquet files. These APIs refer to three core types of distributed collection APIs\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDatasets\u003c/li\u003e\n\u003cli\u003eDataFrames\u003c/li\u003e\n\u003cli\u003eSQL tables and views\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMajority of the Structured APIs apply to both batch and streaming computation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThis means that when you work with the Structured APIs, it should be simple to migrate from batch to streaming (or vice versa) with little to no effort\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThe Structured APIs are the fundamental abstraction that you will use to write the majority of your data flows\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eDataFrames and Datasets are (distributed) table-like collections with well-defined rows and columns.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eTo Spark, DataFrames and Datasets represent immutable, lazily evaluated plans that specify what operations to apply to data residing at a location to generate some output\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhen we perform an action on a DataFrame, we instruct Spark to perform the actual transformations and return the result. These represent plans of how to manipulate rows and columns to compute the user’s desired result.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533477972082_1237321597",
      "id": "20180805-140612_236552655",
      "dateCreated": "Aug 5, 2018 2:06:12 PM",
      "dateStarted": "Aug 5, 2018 3:14:45 PM",
      "dateFinished": "Aug 5, 2018 3:14:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Schemas\n\n- A schema defines the column names and types of a DataFrame. \n- You can define schemas manually or read a schema from a data source (often called schema on read). \n- Schemas consist of types, meaning that you need a way of specifying what lies where.\n\n### Structured Spark Types\n\n- Spark is effectively a programming language of its own. \n- Internally, Spark uses an engine called Catalyst that maintains its own type information through the planning and processing of work. \n- In doing so, this opens up a wide variety of execution optimizations that make significant differences.\n- Spark types map directly to the different language APIs that Spark maintains and there exists a lookup table for each of these in Scala, Java, Python, SQL, and R. \n- Even if we use Spark’s Structured APIs from Python or R, the majority of our manipulations will operate strictly on Spark types, not Python types.\n\n\n##### For example, the following code does not perform addition in Scala or Python; it actually performs addition purely in Spark\n\n- This addition operation happens because Spark will convert an expression written in an input language to Spark’s internal Catalyst representation of that same type information. It then will operate on that internal representation\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 3:16:49 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eSchemas\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eA schema defines the column names and types of a DataFrame.\u003c/li\u003e\n\u003cli\u003eYou can define schemas manually or read a schema from a data source (often called schema on read).\u003c/li\u003e\n\u003cli\u003eSchemas consist of types, meaning that you need a way of specifying what lies where.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eStructured Spark Types\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSpark is effectively a programming language of its own.\u003c/li\u003e\n\u003cli\u003eInternally, Spark uses an engine called Catalyst that maintains its own type information through the planning and processing of work.\u003c/li\u003e\n\u003cli\u003eIn doing so, this opens up a wide variety of execution optimizations that make significant differences.\u003c/li\u003e\n\u003cli\u003eSpark types map directly to the different language APIs that Spark maintains and there exists a lookup table for each of these in Scala, Java, Python, SQL, and R.\u003c/li\u003e\n\u003cli\u003eEven if we use Spark’s Structured APIs from Python or R, the majority of our manipulations will operate strictly on Spark types, not Python types.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eFor example, the following code does not perform addition in Scala or Python; it actually performs addition purely in Spark\u003c/h5\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533478061529_1975809623",
      "id": "20180805-140741_2118785956",
      "dateCreated": "Aug 5, 2018 2:07:41 PM",
      "dateStarted": "Aug 5, 2018 2:20:27 PM",
      "dateFinished": "Aug 5, 2018 2:20:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.range(1000).toDF(\"number\")\ndf.select(df.col(\"number\") + 10).take(2)",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 2:27:08 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": true,
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df: org.apache.spark.sql.DataFrame \u003d [number: bigint]\nres11: Array[org.apache.spark.sql.Row] \u003d Array([10], [11])\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533478820127_-1262383784",
      "id": "20180805-142020_746071309",
      "dateCreated": "Aug 5, 2018 2:20:20 PM",
      "dateStarted": "Aug 5, 2018 2:25:57 PM",
      "dateFinished": "Aug 5, 2018 2:25:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndf \u003d spark.range(500).toDF(\"number\")\ndf.select(df[\"number\"] + 10).take(2)",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 2:26:42 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[Row((number + 10)\u003d10), Row((number + 10)\u003d11)]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533478893646_-803070218",
      "id": "20180805-142133_885484607",
      "dateCreated": "Aug 5, 2018 2:21:33 PM",
      "dateStarted": "Aug 5, 2018 2:26:42 PM",
      "dateFinished": "Aug 5, 2018 2:26:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### DataFrames\n- Dataframes are untyped API\u0027s.\n- To say that DataFrames are untyped is aslightly inaccurate; they have types, but Spark maintains them completely and only checks whether those types line up to those specified in the schema at runtime\n- when you’re using DataFrames, you’re taking advantage of Spark’s optimized internal format. This format applies the same efficiency gains to all of Spark’s language APIs\n\n### DataSets\n- check whether types conform to the specification at compile time.\n- Datasets are only available to Java Virtual Machine (JVM)–based languages (Scala and Java)\n- we specify types with case classes for scala or Java beans for java\n\nTo Spark (in Scala), DataFrames are simply Datasets of Type Row. \n\nThe “Row” type is Spark’s internal representation of its optimized in-memory format for computation. \n\nThis format makes for highly specialized and efficient computation because rather than using JVM types, which can cause high garbage-collection and object instantiation costs.\n\nSpark can operate on its own internal format without incurring any of those costs\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 2:33:37 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eDataFrames\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDataframes are untyped API\u0027s.\u003c/li\u003e\n\u003cli\u003eTo say that DataFrames are untyped is aslightly inaccurate; they have types, but Spark maintains them completely and only checks whether those types line up to those specified in the schema at runtime\u003c/li\u003e\n\u003cli\u003ewhen you’re using DataFrames, you’re taking advantage of Spark’s optimized internal format. This format applies the same efficiency gains to all of Spark’s language APIs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eDataSets\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003echeck whether types conform to the specification at compile time.\u003c/li\u003e\n\u003cli\u003eDatasets are only available to Java Virtual Machine (JVM)–based languages (Scala and Java)\u003c/li\u003e\n\u003cli\u003ewe specify types with case classes for scala or Java beans for java\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo Spark (in Scala), DataFrames are simply Datasets of Type Row.\u003c/p\u003e\n\u003cp\u003eThe “Row” type is Spark’s internal representation of its optimized in-memory format for computation.\u003c/p\u003e\n\u003cp\u003eThis format makes for highly specialized and efficient computation because rather than using JVM types, which can cause high garbage-collection and object instantiation costs.\u003c/p\u003e\n\u003cp\u003eSpark can operate on its own internal format without incurring any of those costs\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533479272924_1392056885",
      "id": "20180805-142752_1404998038",
      "dateCreated": "Aug 5, 2018 2:27:52 PM",
      "dateStarted": "Aug 5, 2018 2:33:37 PM",
      "dateFinished": "Aug 5, 2018 2:33:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## Spark Types\n- ByteType\n- ShortType\n- IntegerType\n- LongType\n- FloatType\n- DoubleType\n- DecimalType\n- StringType\n- BinaryType\n- BooleanType\n- TimestampType\n- DateType\n- ArrayType\n- MapType\n- StructType\n- StructField\n\n\n## Structured API Execution\n\n![sv-imge](https://user-images.githubusercontent.com/1182329/43687201-e8993126-98ee-11e8-9a2b-99334dba938a.png)\n\n\n- Write DataFrame/Dataset/SQL Code.\n- If valid code, Spark converts this to a Logical Plan.\n- Spark transforms this Logical Plan to a Physical Plan, checking for optimizations along the way.\n- Spark then executes this Physical Plan (RDD manipulations) on the cluster.\n\n## Logical Planning\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43687225-18205992-98ef-11e8-81cc-808ad4aa4107.png)\n\n\n- it’s purely to convert the user’s set of expressions into the most optimized version. \n- It does this by converting user code into an unresolved logical plan. \n- This plan is unresolved because although your code might be valid, the tables or columns that it refers to might or might not exist. \n- Spark uses the catalog, a repository of all table and DataFrame information, to resolve columns and tables in the analyzer. \n- The analyzer might reject the unresolved logical plan if the required table or column name does not exist in the catalog.\n- If the analyzer can resolve it, the result is passed through the Catalyst Optimizer, a collection of rules that attempt to optimize the logical plan by pushing down predicates or selections\n\n## Physical Plan\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43687258-ae765a04-98ef-11e8-9dcb-6485a5af5fa3.png)\n\n- The physical plan, often called a Spark plan, specifies how the logical plan will execute on the cluster by generating different physical execution strategies and comparing them through a cost model\n- Physical planning results in a series of RDDs and transformations\n- This result is why you might have heard Spark referred to as a compiler—it takes queries in DataFrames, Datasets, and SQL and compiles them into RDD transformations for you\n- Upon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level programming interface of Spark\n- Spark performs further optimizations at runtime, generating native Java bytecode that can remove entire tasks or stages during execution. Finally the result is returned to the user\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 3:11:39 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch2\u003eSpark Types\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eByteType\u003c/li\u003e\n\u003cli\u003eShortType\u003c/li\u003e\n\u003cli\u003eIntegerType\u003c/li\u003e\n\u003cli\u003eLongType\u003c/li\u003e\n\u003cli\u003eFloatType\u003c/li\u003e\n\u003cli\u003eDoubleType\u003c/li\u003e\n\u003cli\u003eDecimalType\u003c/li\u003e\n\u003cli\u003eStringType\u003c/li\u003e\n\u003cli\u003eBinaryType\u003c/li\u003e\n\u003cli\u003eBooleanType\u003c/li\u003e\n\u003cli\u003eTimestampType\u003c/li\u003e\n\u003cli\u003eDateType\u003c/li\u003e\n\u003cli\u003eArrayType\u003c/li\u003e\n\u003cli\u003eMapType\u003c/li\u003e\n\u003cli\u003eStructType\u003c/li\u003e\n\u003cli\u003eStructField\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eStructured API Execution\u003c/h2\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43687201-e8993126-98ee-11e8-9a2b-99334dba938a.png\" alt\u003d\"sv-imge\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWrite DataFrame/Dataset/SQL Code.\u003c/li\u003e\n\u003cli\u003eIf valid code, Spark converts this to a Logical Plan.\u003c/li\u003e\n\u003cli\u003eSpark transforms this Logical Plan to a Physical Plan, checking for optimizations along the way.\u003c/li\u003e\n\u003cli\u003eSpark then executes this Physical Plan (RDD manipulations) on the cluster.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eLogical Planning\u003c/h2\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43687225-18205992-98ef-11e8-81cc-808ad4aa4107.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit’s purely to convert the user’s set of expressions into the most optimized version.\u003c/li\u003e\n\u003cli\u003eIt does this by converting user code into an unresolved logical plan.\u003c/li\u003e\n\u003cli\u003eThis plan is unresolved because although your code might be valid, the tables or columns that it refers to might or might not exist.\u003c/li\u003e\n\u003cli\u003eSpark uses the catalog, a repository of all table and DataFrame information, to resolve columns and tables in the analyzer.\u003c/li\u003e\n\u003cli\u003eThe analyzer might reject the unresolved logical plan if the required table or column name does not exist in the catalog.\u003c/li\u003e\n\u003cli\u003eIf the analyzer can resolve it, the result is passed through the Catalyst Optimizer, a collection of rules that attempt to optimize the logical plan by pushing down predicates or selections\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePhysical Plan\u003c/h2\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43687258-ae765a04-98ef-11e8-9dcb-6485a5af5fa3.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe physical plan, often called a Spark plan, specifies how the logical plan will execute on the cluster by generating different physical execution strategies and comparing them through a cost model\u003c/li\u003e\n\u003cli\u003ePhysical planning results in a series of RDDs and transformations\u003c/li\u003e\n\u003cli\u003eThis result is why you might have heard Spark referred to as a compiler—it takes queries in DataFrames, Datasets, and SQL and compiles them into RDD transformations for you\u003c/li\u003e\n\u003cli\u003eUpon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level programming interface of Spark\u003c/li\u003e\n\u003cli\u003eSpark performs further optimizations at runtime, generating native Java bytecode that can remove entire tasks or stages during execution. Finally the result is returned to the user\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533479617248_205786249",
      "id": "20180805-143337_1780461336",
      "dateCreated": "Aug 5, 2018 2:33:37 PM",
      "dateStarted": "Aug 5, 2018 3:11:39 PM",
      "dateFinished": "Aug 5, 2018 3:11:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 3:04:31 PM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1533481471296_924929561",
      "id": "20180805-150431_2065166433",
      "dateCreated": "Aug 5, 2018 3:04:31 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "spark-structured-api-overview",
  "id": "2DKGYUVMH",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}