{
  "paragraphs": [
    {
      "text": "%md\n\n### DataFrame\n- Partitioning of the DataFrame defines the layout of the DataFrame or Dataset’s physical distribution across the cluster. \n- You can set partition to be based on values in a certain column or nondeterministically.",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:22:42 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eDataFrame\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePartitioning of the DataFrame defines the layout of the DataFrame or Dataset’s physical distribution across the cluster.\u003c/li\u003e\n\u003cli\u003eYou can set partition to be based on values in a certain column or nondeterministically.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533485982293_-1802901344",
      "id": "20180805-161942_1374390528",
      "dateCreated": "Aug 5, 2018 4:19:42 PM",
      "dateStarted": "Aug 5, 2018 4:22:42 PM",
      "dateFinished": "Aug 5, 2018 4:22:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n\nif [ -e /tmp/2015-summary.json ]\nthen\n    rm -f /tmp/2015-summary.json\nfi\n\nwget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/json/2015-summary.json -O /tmp/2015-summary.json\n\nhdfs dfs -rm -f /tmp/2015-summary.json\n\n# Move downloaded JSON file from local storage to HDFS\nhdfs dfs -put /tmp/2015-summary.json /tmp\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:29:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "--2018-08-05 16:29:01--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/json/2015-summary.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 21368 (21K) [text/plain]\nSaving to: ‘/tmp/2015-summary.json’\n\n     0K .......... ..........                                 100%  900K\u003d0.02s\n\n2018-08-05 16:29:01 (900 KB/s) - ‘/tmp/2015-summary.json’ saved [21368/21368]\n\n18/08/05 16:29:04 INFO fs.TrashPolicyDefault: Moved: \u0027hdfs://sandbox-hdp.hortonworks.com:8020/tmp/2015-summary.json\u0027 to trash at: hdfs://sandbox-hdp.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/2015-summary.json\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533486162147_25268899",
      "id": "20180805-162242_159510996",
      "dateCreated": "Aug 5, 2018 4:22:42 PM",
      "dateStarted": "Aug 5, 2018 4:29:01 PM",
      "dateFinished": "Aug 5, 2018 4:29:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val flightData2015 \u003d spark.read.format(\"json\").load(\"/tmp/2015-summary.json\")\n\nflightData2015.printSchema()\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:29:42 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "flightData2015: org.apache.spark.sql.DataFrame \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nroot\n |-- DEST_COUNTRY_NAME: string (nullable \u003d true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable \u003d true)\n |-- count: long (nullable \u003d true)\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533486170339_58573487",
      "id": "20180805-162250_645725983",
      "dateCreated": "Aug 5, 2018 4:22:50 PM",
      "dateStarted": "Aug 5, 2018 4:29:42 PM",
      "dateFinished": "Aug 5, 2018 4:29:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- A schema defines the column names and types of a DataFrame. \n- We can either let a data source define the schema (called schema-on-read) or we can define it explicitly ourselves.\n\n\u003caside class\u003d\"warning\"\u003e\n    For ad hoc analysis, schema-on-read usually works just fine (although at times it can be a bit slow with plain-text file formats like CSV or JSON). However, this can also lead to precision issues like a long type incorrectly set as an integer when reading in a file. When using Spark for production Extract, Transform, and Load (ETL), it is often a good idea to define your schemas manually, especially when working with untyped data sources like CSV and JSON because schema inference can vary depending on the type of data that you read in.\n\u003c/aside\u003e",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:32:20 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cul\u003e\n\u003cli\u003eA schema defines the column names and types of a DataFrame.\u003c/li\u003e\n\u003cli\u003eWe can either let a data source define the schema (called schema-on-read) or we can define it explicitly ourselves.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003caside class\u003d\"warning\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eFor ad hoc analysis, schema-on-read usually works just fine (although at times it can be a bit slow with plain-text file formats like CSV or JSON). However, this can also lead to precision issues like a long type incorrectly set as an integer when reading in a file. When using Spark for production Extract, Transform, and Load (ETL), it is often a good idea to define your schemas manually, especially when working with untyped data sources like CSV and JSON because schema inference can vary depending on the type of data that you read in.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003c/aside\u003e\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533486328644_-1122038642",
      "id": "20180805-162528_456809827",
      "dateCreated": "Aug 5, 2018 4:25:28 PM",
      "dateStarted": "Aug 5, 2018 4:32:14 PM",
      "dateFinished": "Aug 5, 2018 4:32:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.read.format(\"json\").load(\"/tmp/2015-summary.json\").schema",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:33:09 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res16: org.apache.spark.sql.types.StructType \u003d StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533486708776_-613247354",
      "id": "20180805-163148_20770674",
      "dateCreated": "Aug 5, 2018 4:31:48 PM",
      "dateStarted": "Aug 5, 2018 4:32:50 PM",
      "dateFinished": "Aug 5, 2018 4:32:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- We can manually specify schema on a DataFrame\n- A schema is a StructType made up of a number of fields, StructFields, that have a name, type, a Boolean flag which specifies whether that column can contain missing or null values.\n- finally, users can optionally specify associated metadata with that column. The metadata is a way of storing information about this column",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:37:03 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cul\u003e\n\u003cli\u003eWe can manually specify schema on a DataFrame\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533486769173_388562161",
      "id": "20180805-163249_1203930202",
      "dateCreated": "Aug 5, 2018 4:32:49 PM",
      "dateStarted": "Aug 5, 2018 4:35:57 PM",
      "dateFinished": "Aug 5, 2018 4:35:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\nimport org.apache.spark.sql.types.Metadata\n\nval myManualSchema \u003d StructType(Array(\n                        StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n                        StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n                        StructField(\"count\", LongType, false, Metadata.fromJson(\"{\\\"hello\\\":\\\"world\\\"}\")))\n                        )\n                        \nval df \u003d spark.read.format(\"json\").schema(myManualSchema).load(\"/tmp/2015-summary.json\")\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 4:58:02 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\nimport org.apache.spark.sql.types.Metadata\nmyManualSchema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))\ndf: org.apache.spark.sql.DataFrame \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533486957216_1672725939",
      "id": "20180805-163557_1720946413",
      "dateCreated": "Aug 5, 2018 4:35:57 PM",
      "dateStarted": "Aug 7, 2018 4:58:03 AM",
      "dateFinished": "Aug 7, 2018 4:58:21 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nThere are a lot of different ways to construct and refer to columns but the two simplest ways are by using the col or column functions. To use either of these functions, you pass in a column name:",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:43:27 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eThere are a lot of different ways to construct and refer to columns but the two simplest ways are by using the col or column functions. To use either of these functions, you pass in a column name:\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533487085964_-1098266288",
      "id": "20180805-163805_1618676765",
      "dateCreated": "Aug 5, 2018 4:38:05 PM",
      "dateStarted": "Aug 5, 2018 4:43:27 PM",
      "dateFinished": "Aug 5, 2018 4:43:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{col, column}\ncol(\"someColumnName\")\ncolumn(\"someColumnName\")\n\ndf.col(\"count\")\ndf.columns\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:45:48 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{col, column}\nres38: org.apache.spark.sql.Column \u003d someColumnName\nres39: org.apache.spark.sql.Column \u003d someColumnName\nres41: org.apache.spark.sql.Column \u003d count\nres42: Array[String] \u003d Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533487407520_-1660927355",
      "id": "20180805-164327_1121258320",
      "dateCreated": "Aug 5, 2018 4:43:27 PM",
      "dateStarted": "Aug 5, 2018 4:45:48 PM",
      "dateFinished": "Aug 5, 2018 4:45:49 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Expressions\n\n- An expression is a set of transformations on one or more values in a record in a DataFrame. \n- Think of it like a function that takes as input one or more column names, resolves them, and then potentially applies more expressions to create a single value for each record in the dataset.\n- Importantly, this “single value” can actually be a complex type like a Map or Array.\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:49:36 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eExpressions\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAn expression is a set of transformations on one or more values in a record in a DataFrame.\u003c/li\u003e\n\u003cli\u003eThink of it like a function that takes as input one or more column names, resolves them, and then potentially applies more expressions to create a single value for each record in the dataset.\u003c/li\u003e\n\u003cli\u003eImportantly, this “single value” can actually be a complex type like a Map or Array.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533487420839_-446828392",
      "id": "20180805-164340_135090293",
      "dateCreated": "Aug 5, 2018 4:43:40 PM",
      "dateStarted": "Aug 5, 2018 4:49:36 PM",
      "dateFinished": "Aug 5, 2018 4:49:36 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Records and Rows\n\n- In Spark, each row in a DataFrame is a single record. Spark represents this record as an object of type Row. \n- Spark manipulates Row objects using column expressions in order to produce usable values.\n\n\nWe can also create DataFrames on the fly by taking a set of rows and converting them to a DataFrame\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:50:49 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eRecords and Rows\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIn Spark, each row in a DataFrame is a single record. Spark represents this record as an object of type Row.\u003c/li\u003e\n\u003cli\u003eSpark manipulates Row objects using column expressions in order to produce usable values.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe can also create DataFrames on the fly by taking a set of rows and converting them to a DataFrame\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533487776298_-32516768",
      "id": "20180805-164936_1225816640",
      "dateCreated": "Aug 5, 2018 4:49:36 PM",
      "dateStarted": "Aug 5, 2018 4:50:49 PM",
      "dateFinished": "Aug 5, 2018 4:50:49 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n\nval myManualSchema \u003d new StructType(Array(\n  new StructField(\"some\", StringType, true),\n  new StructField(\"col\", StringType, true),\n  new StructField(\"names\", LongType, false)))\n  \nval myRows \u003d Seq(Row(\"Hello\", null, 1L))\nval myRDD \u003d spark.sparkContext.parallelize(myRows)\nval myDf \u003d spark.createDataFrame(myRDD, myManualSchema)\nmyDf.show()\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:51:43 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\nmyManualSchema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(some,StringType,true), StructField(col,StringType,true), StructField(names,LongType,false))\nmyRows: Seq[org.apache.spark.sql.Row] \u003d List([Hello,null,1])\nmyRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d ParallelCollectionRDD[46] at parallelize at \u003cconsole\u003e:56\nmyDf: org.apache.spark.sql.DataFrame \u003d [some: string, col: string ... 1 more field]\n+-----+----+-----+\n| some| col|names|\n+-----+----+-----+\n|Hello|null|    1|\n+-----+----+-----+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533487849137_1439825328",
      "id": "20180805-165049_169006109",
      "dateCreated": "Aug 5, 2018 4:50:49 PM",
      "dateStarted": "Aug 5, 2018 4:51:43 PM",
      "dateFinished": "Aug 5, 2018 4:51:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## Select and SelectExpr",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:54:13 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch2\u003eSelect and SelectExpr\u003c/h2\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533487903356_-649994130",
      "id": "20180805-165143_1432172023",
      "dateCreated": "Aug 5, 2018 4:51:43 PM",
      "dateStarted": "Aug 5, 2018 4:54:13 PM",
      "dateFinished": "Aug 5, 2018 4:54:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.select(\"DEST_COUNTRY_NAME\").show(2)\ndf.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\ndf.select(df.col(\"DEST_COUNTRY_NAME\")).show(2)\ndf.select(column(\"DEST_COUNTRY_NAME\")).show(2)\ndf.select(expr(\"DEST_COUNTRY_NAME\")).show(2)\ndf.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 4:57:28 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n+-----------------+\nonly showing top 2 rows\n\n+-----------------+-------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-----------------+-------------------+\n|    United States|            Romania|\n|    United States|            Croatia|\n+-----------------+-------------------+\nonly showing top 2 rows\n\n+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n+-----------------+\nonly showing top 2 rows\n\n+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n+-----------------+\nonly showing top 2 rows\n\n+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n+-----------------+\nonly showing top 2 rows\n\n+-------------+\n|  destination|\n+-------------+\n|United States|\n|United States|\n+-------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533488053086_1425957693",
      "id": "20180805-165413_40321666",
      "dateCreated": "Aug 5, 2018 4:54:13 PM",
      "dateStarted": "Aug 5, 2018 4:57:25 PM",
      "dateFinished": "Aug 5, 2018 4:57:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n\ndf.selectExpr(\n    \"*\", // include all original columns\n    \"(DEST_COUNTRY_NAME \u003d ORIGIN_COUNTRY_NAME) as withinCountry\")\n  .show(2)\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 5, 2018 5:05:19 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------------+-----------------+\n|newColumnName|DEST_COUNTRY_NAME|\n+-------------+-----------------+\n|United States|    United States|\n|United States|    United States|\n+-------------+-----------------+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533488068286_-1801685506",
      "id": "20180805-165428_791688623",
      "dateCreated": "Aug 5, 2018 4:54:28 PM",
      "dateStarted": "Aug 5, 2018 5:05:20 PM",
      "dateFinished": "Aug 5, 2018 5:05:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 4:58:28 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------+---------------------------------+\n| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n+-----------+---------------------------------+\n|1770.765625|                              132|\n+-----------+---------------------------------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533488393551_-697565076",
      "id": "20180805-165953_1371102698",
      "dateCreated": "Aug 5, 2018 4:59:53 PM",
      "dateStarted": "Aug 7, 2018 4:58:28 AM",
      "dateFinished": "Aug 7, 2018 4:59:16 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- We can create data frames on the fly by taking set of rows and converting them into dataframe",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:00:39 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cul\u003e\n\u003cli\u003eWe can create data frames on the fly by taking set of rows and converting them into dataframe\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533488532761_-1598279220",
      "id": "20180805-170212_1074519358",
      "dateCreated": "Aug 5, 2018 5:02:12 PM",
      "dateStarted": "Aug 7, 2018 5:00:39 AM",
      "dateFinished": "Aug 7, 2018 5:00:43 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, LongType}\n\nval mySchema \u003d new StructType(Array(new StructField(\"some\", StringType, true),\n                              new StructField(\"col\", StringType, true),\n                              new StructField(\"names\", LongType, false)))\n\nval myRows \u003d Seq(Row(\"Hello\", null, 1L))\n\nval myRDD \u003d spark.sparkContext.parallelize(myRows)\n\nval myManDf \u003d spark.createDataFrame(myRDD, mySchema)\n\n\nmyManDf.show()\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:17:16 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, LongType}\nmySchema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(some,StringType,true), StructField(col,StringType,true), StructField(names,LongType,false))\nmyRows: Seq[org.apache.spark.sql.Row] \u003d List([Hello,null,1])\nmyRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d ParallelCollectionRDD[42] at parallelize at \u003cconsole\u003e:67\nmyManDf: org.apache.spark.sql.DataFrame \u003d [some: string, col: string ... 1 more field]\n+-----+----+-----+\n| some| col|names|\n+-----+----+-----+\n|Hello|null|    1|\n+-----+----+-----+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533618039157_1809758237",
      "id": "20180807-050039_1233698306",
      "dateCreated": "Aug 7, 2018 5:00:39 AM",
      "dateStarted": "Aug 7, 2018 5:17:16 AM",
      "dateFinished": "Aug 7, 2018 5:17:18 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- parallelized collection is the number of partition the dataset is cut into. \n- Spark will run one task for each partition of cluster.\n- Spark sets number of partition based on our cluster.\n- we can also manually set the number of partitions. This is achieved by passing number of partition as second parameter to parallelize",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:15:22 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1533618894597_-448506557",
      "id": "20180807-051454_423698811",
      "dateCreated": "Aug 7, 2018 5:14:54 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, LongType}\n\nval mySchema \u003d new StructType(Array(new StructField(\"some\", StringType, true),\n                              new StructField(\"col\", StringType, true),\n                              new StructField(\"names\", LongType, false)))\n\nval myRows \u003d Seq(Row(\"Hello\", null, 1L))\n\nval myRDD \u003d spark.sparkContext.parallelize(myRows, 5)\n\nval myManDf \u003d spark.createDataFrame(myRDD, mySchema)\n\n\nmyManDf.show()\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:15:55 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, LongType}\nmySchema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(some,StringType,true), StructField(col,StringType,true), StructField(names,LongType,false))\nmyRows: Seq[org.apache.spark.sql.Row] \u003d List([Hello,null,1])\nmyRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d ParallelCollectionRDD[36] at parallelize at \u003cconsole\u003e:64\nmyManDf: org.apache.spark.sql.DataFrame \u003d [some: string, col: string ... 1 more field]\n+-----+----+-----+\n| some| col|names|\n+-----+----+-----+\n|Hello|null|    1|\n+-----+----+-----+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533618931974_776210489",
      "id": "20180807-051531_876597538",
      "dateCreated": "Aug 7, 2018 5:15:31 AM",
      "dateStarted": "Aug 7, 2018 5:15:55 AM",
      "dateFinished": "Aug 7, 2018 5:16:00 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val myManDF1 \u003d Seq((\"Hello\", \"TWO\", 1L)).toDF(\"col1\", \"col2\", \"col3\")\nmyManDF1.show()",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:11:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "myManDF1: org.apache.spark.sql.DataFrame \u003d [col1: string, col2: string ... 1 more field]\n+-----+----+----+\n| col1|col2|col3|\n+-----+----+----+\n|Hello| TWO|   1|\n+-----+----+----+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533618415488_446160518",
      "id": "20180807-050655_434465204",
      "dateCreated": "Aug 7, 2018 5:06:55 AM",
      "dateStarted": "Aug 7, 2018 5:11:50 AM",
      "dateFinished": "Aug 7, 2018 5:11:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Creating RDD from existing RDD\n\n- Transformation mutates one RDD into another RDD, thus transformation is the way to create an RDD from already existing RDD. \n- This creates difference between Apache Spark and Hadoop MapReduce. Transformation acts as a function that intakes an RDD and produces one. \n- The input RDD does not get changed, because RDDs are immutable in nature but it produces one or more RDD by applying operations",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:19:31 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eCreating RDD from existing RDD\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTransformation mutates one RDD into another RDD, thus transformation is the way to create an RDD from already existing RDD.\u003c/li\u003e\n\u003cli\u003eThis creates difference between Apache Spark and Hadoop MapReduce. Transformation acts as a function that intakes an RDD and produces one.\u003c/li\u003e\n\u003cli\u003eThe input RDD does not get changed, because RDDs are immutable in nature but it produces one or more RDD by applying operations\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533618737560_-1012996135",
      "id": "20180807-051217_987282526",
      "dateCreated": "Aug 7, 2018 5:12:17 AM",
      "dateStarted": "Aug 7, 2018 5:19:31 AM",
      "dateFinished": "Aug 7, 2018 5:19:31 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val words \u003d spark.sparkContext.parallelize(Seq(\"prokarma\", \"india\", \"hyderabad\"))\n\nval wordPairs \u003d words.map(w \u003d\u003e (w.charAt(0), w))\n\nwordPairs.collect().foreach(println)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:23:47 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "words: org.apache.spark.rdd.RDD[String] \u003d ParallelCollectionRDD[56] at parallelize at \u003cconsole\u003e:64\nwordPairs: org.apache.spark.rdd.RDD[(Char, String)] \u003d MapPartitionsRDD[57] at map at \u003cconsole\u003e:67\n(p,prokarma)\n(i,india)\n(h,hyderabad)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533619170993_-1656422702",
      "id": "20180807-051930_1323588175",
      "dateCreated": "Aug 7, 2018 5:19:30 AM",
      "dateStarted": "Aug 7, 2018 5:23:47 AM",
      "dateFinished": "Aug 7, 2018 5:23:49 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- Sometimes, we need to pass explicit values into Spark that are just a value \n- This is basically a translation from a given programming language’s literal value to one that Spark understands",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:28:15 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cul\u003e\n\u003cli\u003eSometimes, we need to pass explicit values into Spark that are just a value\u003c/li\u003e\n\u003cli\u003eThis is basically a translation from a given programming language’s literal value to one that Spark understands\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533619319483_843857897",
      "id": "20180807-052159_109463657",
      "dateCreated": "Aug 7, 2018 5:21:59 AM",
      "dateStarted": "Aug 7, 2018 5:28:16 AM",
      "dateFinished": "Aug 7, 2018 5:28:16 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.lit\n\ndf.select(expr(\"*\"), lit(1).as(\"one\")).show(2)\n\ndf.show(2)\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:36:27 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.lit\n+-----------------+-------------------+-----+---+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|one|\n+-----------------+-------------------+-----+---+\n|    United States|            Romania|   15|  1|\n|    United States|            Croatia|    1|  1|\n+-----------------+-------------------+-----+---+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+---+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|one|\n+-----------------+-------------------+-----+---+\n|    United States|            Romania|   15|  1|\n|    United States|            Croatia|    1|  1|\n+-----------------+-------------------+-----+---+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\nres192: Array[String] \u003d Array(dest, ORIGIN_COUNTRY_NAME, count)\nres194: Array[String] \u003d Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count)\nres196: org.apache.spark.sql.DataFrame \u003d [DEST_COUNTRY_NAME: string, count: bigint]\nres198: org.apache.spark.sql.DataFrame \u003d [count: bigint]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533619695959_-97084671",
      "id": "20180807-052815_1166940121",
      "dateCreated": "Aug 7, 2018 5:28:15 AM",
      "dateStarted": "Aug 7, 2018 5:35:19 AM",
      "dateFinished": "Aug 7, 2018 5:35:24 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.withColumn(\"one\", lit(1)).show(2)\n\ndf.show(2)\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:36:37 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------------+-------------------+-----+---+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|one|\n+-----------------+-------------------+-----+---+\n|    United States|            Romania|   15|  1|\n|    United States|            Croatia|    1|  1|\n+-----------------+-------------------+-----+---+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533619744392_89362015",
      "id": "20180807-052904_293027787",
      "dateCreated": "Aug 7, 2018 5:29:04 AM",
      "dateStarted": "Aug 7, 2018 5:36:37 AM",
      "dateFinished": "Aug 7, 2018 5:36:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.withColumn(\"withInCountry\", expr(\"DEST_COUNTRY_NAME \u003d\u003d ORIGIN_COUNTRY_NAME\")).show(2)\n\ndf.show(2)\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:36:44 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620197380_-2097686483",
      "id": "20180807-053637_1575478149",
      "dateCreated": "Aug 7, 2018 5:36:37 AM",
      "dateStarted": "Aug 7, 2018 5:36:44 AM",
      "dateFinished": "Aug 7, 2018 5:36:44 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns\n\ndf.columns\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:36:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res205: Array[String] \u003d Array(dest, ORIGIN_COUNTRY_NAME, count)\nres207: Array[String] \u003d Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620204076_-1954929317",
      "id": "20180807-053644_716896787",
      "dateCreated": "Aug 7, 2018 5:36:44 AM",
      "dateStarted": "Aug 7, 2018 5:36:50 AM",
      "dateFinished": "Aug 7, 2018 5:36:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.drop(\"ORIGIN_COUNTRY_NAME\")\n\ndf.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:36:54 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res208: org.apache.spark.sql.DataFrame \u003d [DEST_COUNTRY_NAME: string, count: bigint]\nres210: org.apache.spark.sql.DataFrame \u003d [count: bigint]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620210546_-1514376502",
      "id": "20180807-053650_1768502634",
      "dateCreated": "Aug 7, 2018 5:36:50 AM",
      "dateStarted": "Aug 7, 2018 5:36:54 AM",
      "dateFinished": "Aug 7, 2018 5:36:55 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.filter(col(\"count\") \u003c 2).show(2)",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:37:52 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620214681_143506509",
      "id": "20180807-053654_934596766",
      "dateCreated": "Aug 7, 2018 5:36:54 AM",
      "dateStarted": "Aug 7, 2018 5:37:52 AM",
      "dateFinished": "Aug 7, 2018 5:37:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.where(col(\"count\") \u003c 2).show(2)",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:38:09 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620272714_-1981618197",
      "id": "20180807-053752_1328081277",
      "dateCreated": "Aug 7, 2018 5:37:52 AM",
      "dateStarted": "Aug 7, 2018 5:38:09 AM",
      "dateFinished": "Aug 7, 2018 5:38:09 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n-  if you want to specify multiple AND filters, just chain them sequentially",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:38:55 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cul\u003e\n\u003cli\u003eif you want to specify multiple AND filters, just chain them sequentially\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620289469_468108698",
      "id": "20180807-053809_311058213",
      "dateCreated": "Aug 7, 2018 5:38:09 AM",
      "dateStarted": "Aug 7, 2018 5:38:55 AM",
      "dateFinished": "Aug 7, 2018 5:38:55 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.where(col(\"count\") \u003c 2).where(col(\"ORIGIN_COUNTRY_NAME\") \u003d!\u003d \"Croatia\").show(2)",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:40:38 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|          Singapore|    1|\n|          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620335068_1713048869",
      "id": "20180807-053855_1851408842",
      "dateCreated": "Aug 7, 2018 5:38:55 AM",
      "dateStarted": "Aug 7, 2018 5:40:38 AM",
      "dateFinished": "Aug 7, 2018 5:40:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Getting Unique Rows\n- A very common use case is to extract the unique or distinct values in a DataFrame. These values can be in one or more columns. \n- The way we do this is by using the distinct method on a DataFrame, which allows us to deduplicate any rows that are in that DataFrame",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:42:00 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eGetting Unique Rows\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eA very common use case is to extract the unique or distinct values in a DataFrame. These values can be in one or more columns.\u003c/li\u003e\n\u003cli\u003eThe way we do this is by using the distinct method on a DataFrame, which allows us to deduplicate any rows that are in that DataFrame\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620397748_-1091862557",
      "id": "20180807-053957_1401571004",
      "dateCreated": "Aug 7, 2018 5:39:57 AM",
      "dateStarted": "Aug 7, 2018 5:42:01 AM",
      "dateFinished": "Aug 7, 2018 5:42:01 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:44:28 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res219: Long \u003d 125\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620520919_1181000793",
      "id": "20180807-054200_1512823401",
      "dateCreated": "Aug 7, 2018 5:42:00 AM",
      "dateStarted": "Aug 7, 2018 5:44:28 AM",
      "dateFinished": "Aug 7, 2018 5:44:33 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:44:42 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res220: Long \u003d 256\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620551304_-1697675154",
      "id": "20180807-054231_2051422271",
      "dateCreated": "Aug 7, 2018 5:42:31 AM",
      "dateStarted": "Aug 7, 2018 5:44:42 AM",
      "dateFinished": "Aug 7, 2018 5:44:48 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Random Samples\n\n- Sometimes, you might just want to sample some random records from your DataFrame. \n- You can do this by using the sample method on a DataFrame, which makes it possible for you to specify a fraction of rows to extract from a DataFrame and whether you’d like to sample with or without replacement",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:45:43 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eRandom Samples\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSometimes, you might just want to sample some random records from your DataFrame.\u003c/li\u003e\n\u003cli\u003eYou can do this by using the sample method on a DataFrame, which makes it possible for you to specify a fraction of rows to extract from a DataFrame and whether you’d like to sample with or without replacement\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620589407_-1751277844",
      "id": "20180807-054309_1041515460",
      "dateCreated": "Aug 7, 2018 5:43:09 AM",
      "dateStarted": "Aug 7, 2018 5:45:43 AM",
      "dateFinished": "Aug 7, 2018 5:45:43 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val seed \u003d 5\nval withReplacement\u003d false\nval fraction \u003d 0.5\n\nval sampleDf \u003d df.sample(withReplacement, fraction, seed)\n\nsampleDf.count()\n\nsampleDf.show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:47:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "seed: Int \u003d 5\nwithReplacement: Boolean \u003d false\nfraction: Double \u003d 0.5\nsampleDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nres225: Long \u003d 126\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|            Egypt|      United States|   15|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620743227_-501366585",
      "id": "20180807-054543_867602818",
      "dateCreated": "Aug 7, 2018 5:45:43 AM",
      "dateStarted": "Aug 7, 2018 5:47:32 AM",
      "dateFinished": "Aug 7, 2018 5:47:35 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- The fraction parameter represents the aproximate fraction of the dataset that will be returned. For instance, if you set it to 0.1, 10% (1/10) of the rows will be retu\n- fraction will be a threshold for a random-generated value (as you can see here), so the resulting dataset size can vary\n\n\n### If you want exact number of samples then we can use limit\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:52:56 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cul\u003e\n\u003cli\u003eThe fraction parameter represents the aproximate fraction of the dataset that will be returned. For instance, if you set it to 0.1, 10% (1/10) of the rows will be retu\u003c/li\u003e\n\u003cli\u003efraction will be a threshold for a random-generated value (as you can see here), so the resulting dataset size can vary\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eIf you want exact number of samples then we can use limit\u003c/h3\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533620821090_1779683761",
      "id": "20180807-054701_576367673",
      "dateCreated": "Aug 7, 2018 5:47:01 AM",
      "dateStarted": "Aug 7, 2018 5:52:56 AM",
      "dateFinished": "Aug 7, 2018 5:52:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val noOfSamples \u003d 100\n\ndf.sample(withReplacement, fraction, seed).limit(noOfSamples)",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:53:14 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "noOfSamples: Int \u003d 100\nres228: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533621176419_852773875",
      "id": "20180807-055256_330821616",
      "dateCreated": "Aug 7, 2018 5:52:56 AM",
      "dateStarted": "Aug 7, 2018 5:53:00 AM",
      "dateFinished": "Aug 7, 2018 5:53:00 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Random Splits\n\n- Random splits can be helpful when you need to break up your DataFrame into a random “splits” of the original DataFrame. \n- This is often used with machine learning algorithms to create training, validation, and test sets. \n- This method is designed to be randomized, we will also specify a seed.\n- It’s important to note that if you don’t specify a proportion for each DataFrame that adds up to one, they will be normalized so that they do",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 5:58:56 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eRandom Splits\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRandom splits can be helpful when you need to break up your DataFrame into a random “splits” of the original DataFrame.\u003c/li\u003e\n\u003cli\u003eThis is often used with machine learning algorithms to create training, validation, and test sets.\u003c/li\u003e\n\u003cli\u003eThis method is designed to be randomized, we will also specify a seed.\u003c/li\u003e\n\u003cli\u003eIt’s important to note that if you don’t specify a proportion for each DataFrame that adds up to one, they will be normalized so that they do\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533621179996_1063231524",
      "id": "20180807-055259_357420550",
      "dateCreated": "Aug 7, 2018 5:52:59 AM",
      "dateStarted": "Aug 7, 2018 5:58:56 AM",
      "dateFinished": "Aug 7, 2018 5:58:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val seed \u003d 5\n\nval dataFrames \u003d df.randomSplit(Array(0.25, 0.75), seed)\n\ndataFrames(0).count()\ndataFrames(1).count()\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:01:00 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "seed: Int \u003d 5\ndataFrames: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] \u003d Array([DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field], [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field])\nres235: Long \u003d 60\nres236: Long \u003d 196\nres238: Long \u003d 256\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533621536573_1539872470",
      "id": "20180807-055856_1126172620",
      "dateCreated": "Aug 7, 2018 5:58:56 AM",
      "dateStarted": "Aug 7, 2018 6:00:44 AM",
      "dateFinished": "Aug 7, 2018 6:00:46 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Concatenating and Appending Rows (Union)\n\n- DataFrames are immutable. This means users cannot append to DataFrames because that would be changing it. \n- To append to a DataFrame, you must union the original DataFrame along with the new DataFrame. \n- This just concatenates the two DataFramess.\n- To union two DataFrames, you must be sure that they have the same schema and number of columns; otherwise, the union will fail.",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:03:08 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eConcatenating and Appending Rows (Union)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDataFrames are immutable. This means users cannot append to DataFrames because that would be changing it.\u003c/li\u003e\n\u003cli\u003eTo append to a DataFrame, you must union the original DataFrame along with the new DataFrame.\u003c/li\u003e\n\u003cli\u003eThis just concatenates the two DataFramess.\u003c/li\u003e\n\u003cli\u003eTo union two DataFrames, you must be sure that they have the same schema and number of columns; otherwise, the union will fail.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533621631114_-1116944568",
      "id": "20180807-060031_877952790",
      "dateCreated": "Aug 7, 2018 6:00:31 AM",
      "dateStarted": "Aug 7, 2018 6:03:08 AM",
      "dateFinished": "Aug 7, 2018 6:03:08 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.Row\n\nval schema \u003d df.schema\n\nval newRows \u003d Seq(Row(\"New Country\", \"Other Country\", 5L),  Row(\"New Country 2\", \"Other Country 3\", 1L))\n\nval parallelizedRows \u003d spark.sparkContext.parallelize(newRows)\n\nval newDF \u003d spark.createDataFrame(parallelizedRows, schema)\n\nval unionDf \u003d df.union(newDF)\n\nunionDf.where(\"count \u003d 1\")\n       .where($\"ORIGIN_COUNTRY_NAME\" \u003d!\u003d \"United States\")\n       .show() // get all of them and we\u0027ll see our new rows at the end\n\n\ndf.count()\nunionDf.count()",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:07:05 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.Row\nschema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\nnewRows: Seq[org.apache.spark.sql.Row] \u003d List([New Country,Other Country,5], [New Country 2,Other Country 3,1])\nparallelizedRows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d ParallelCollectionRDD[432] at parallelize at \u003cconsole\u003e:97\nnewDF: org.apache.spark.sql.DataFrame \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nunionDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n|    United States|          Gibraltar|    1|\n|    United States|             Cyprus|    1|\n|    United States|            Estonia|    1|\n|    United States|          Lithuania|    1|\n|    United States|           Bulgaria|    1|\n|    United States|            Georgia|    1|\n|    United States|            Bahrain|    1|\n|    United States|   Papua New Guinea|    1|\n|    United States|         Montenegro|    1|\n|    United States|            Namibia|    1|\n|    New Country 2|    Other Country 3|    1|\n+-----------------+-------------------+-----+\n\nres272: Long \u003d 256\nres273: Long \u003d 258\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533621788798_-1209590787",
      "id": "20180807-060308_111414911",
      "dateCreated": "Aug 7, 2018 6:03:08 AM",
      "dateStarted": "Aug 7, 2018 6:07:05 AM",
      "dateFinished": "Aug 7, 2018 6:07:08 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Sorting Rows\n\n- When we sort the values in a DataFrame, we always want to sort with either the largest or smallest values at the top of a DataFrame. \n- There are two equivalent operations to do this sort and orderBy that work the exact same way. \n- They accept both column expressions and strings as well as multiple columns. The default is to sort in ascending order:",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:08:10 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eSorting Rows\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhen we sort the values in a DataFrame, we always want to sort with either the largest or smallest values at the top of a DataFrame.\u003c/li\u003e\n\u003cli\u003eThere are two equivalent operations to do this sort and orderBy that work the exact same way.\u003c/li\u003e\n\u003cli\u003eThey accept both column expressions and strings as well as multiple columns. The default is to sort in ascending order:\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533621863302_-492544720",
      "id": "20180807-060423_910178862",
      "dateCreated": "Aug 7, 2018 6:04:23 AM",
      "dateStarted": "Aug 7, 2018 6:08:10 AM",
      "dateFinished": "Aug 7, 2018 6:08:10 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.sort(\"count\").show(5)\ndf.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\ndf.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:08:31 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|     Burkina Faso|      United States|    1|\n|    Cote d\u0027Ivoire|      United States|    1|\n|           Cyprus|      United States|    1|\n|         Djibouti|      United States|    1|\n|        Indonesia|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|     Burkina Faso|      United States|    1|\n|    Cote d\u0027Ivoire|      United States|    1|\n|           Cyprus|      United States|    1|\n|         Djibouti|      United States|    1|\n|        Indonesia|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622090592_-744415687",
      "id": "20180807-060810_43823378",
      "dateCreated": "Aug 7, 2018 6:08:10 AM",
      "dateStarted": "Aug 7, 2018 6:08:27 AM",
      "dateFinished": "Aug 7, 2018 6:08:30 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{desc, asc}\n\ndf.orderBy(expr(\"count desc\")).show(2)\ndf.orderBy(desc(\"count\"), asc(\"DEST_COUNTRY_NAME\")).show(2)",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:08:59 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{desc, asc}\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|          Moldova|      United States|    1|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n+-----------------+-------------------+------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622106955_521883538",
      "id": "20180807-060826_1817875030",
      "dateCreated": "Aug 7, 2018 6:08:26 AM",
      "dateStarted": "Aug 7, 2018 6:08:59 AM",
      "dateFinished": "Aug 7, 2018 6:09:01 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- You can use asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last to specify where you would like your null values to appear in an ordered DataFrame",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:10:20 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cul\u003e\n\u003cli\u003eYou can use asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last to specify where you would like your null values to appear in an ordered DataFrame\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622139045_960158508",
      "id": "20180807-060859_1025521226",
      "dateCreated": "Aug 7, 2018 6:08:59 AM",
      "dateStarted": "Aug 7, 2018 6:10:20 AM",
      "dateFinished": "Aug 7, 2018 6:10:20 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Limit\n\n- Oftentimes, you might want to restrict what you extract from a DataFrame; for example, you might want just the top ten of some DataFrame. You can do this by using the limit method:\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:10:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1533622220704_1412946666",
      "id": "20180807-061020_249722948",
      "dateCreated": "Aug 7, 2018 6:10:20 AM",
      "dateStarted": "Aug 7, 2018 6:10:51 AM",
      "dateFinished": "Aug 7, 2018 6:10:51 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.limit(5).show()\ndf.orderBy(expr(\"count desc\")).limit(6).show()\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:11:38 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\n\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n|             Moldova|      United States|    1|\n+--------------------+-------------------+-----+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622251282_-237379284",
      "id": "20180807-061051_1964371400",
      "dateCreated": "Aug 7, 2018 6:10:51 AM",
      "dateStarted": "Aug 7, 2018 6:11:38 AM",
      "dateFinished": "Aug 7, 2018 6:11:40 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Repartition and Coalesce\n- One important optimization opportunity is to partition the data according to some frequently filtered columns, which control the physical layout of data across the cluster including the partitioning scheme and the number of partitions.\n- Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of partitions or when you are looking to partition by a set of columns",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:12:34 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eRepartition and Coalesce\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOne important optimization opportunity is to partition the data according to some frequently filtered columns, which control the physical layout of data across the cluster including the partitioning scheme and the number of partitions.\u003c/li\u003e\n\u003cli\u003eRepartition will incur a full shuffle of the data, regardless of whether one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of partitions or when you are looking to partition by a set of columns\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622297968_587799838",
      "id": "20180807-061137_1308702095",
      "dateCreated": "Aug 7, 2018 6:11:37 AM",
      "dateStarted": "Aug 7, 2018 6:12:34 AM",
      "dateFinished": "Aug 7, 2018 6:12:34 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.rdd.getNumPartitions \nval repartDf \u003d df.repartition(5)\nrepartDf.rdd.getNumPartitions ",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:13:39 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res286: Int \u003d 1\nrepartDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nres287: Int \u003d 5\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622354769_-757054099",
      "id": "20180807-061234_791789703",
      "dateCreated": "Aug 7, 2018 6:12:34 AM",
      "dateStarted": "Aug 7, 2018 6:13:39 AM",
      "dateFinished": "Aug 7, 2018 6:13:40 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val destColPartDf \u003d df.repartition(col(\"DEST_COUNTRY_NAME\"))\ndestColPartDf.rdd.getNumPartitions\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:14:31 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "destColPartDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nres290: Int \u003d 200\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622366074_765797372",
      "id": "20180807-061246_1148495743",
      "dateCreated": "Aug 7, 2018 6:12:46 AM",
      "dateStarted": "Aug 7, 2018 6:14:31 AM",
      "dateFinished": "Aug 7, 2018 6:14:32 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val destColPartDfWithPartCount \u003d df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\ndestColPartDfWithPartCount.rdd.getNumPartitions\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:15:36 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "destColPartDfWithPartCount: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nres291: Int \u003d 5\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622455050_-229994078",
      "id": "20180807-061415_404486826",
      "dateCreated": "Aug 7, 2018 6:14:15 AM",
      "dateStarted": "Aug 7, 2018 6:15:36 AM",
      "dateFinished": "Aug 7, 2018 6:15:37 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- Coalesce, will not incur a full shuffle and will try to combine partitions. \n- Below operation will shuffle your data into five partitions based on the destination country name, and then coalesce them (without a full shuffle):",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:16:13 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cul\u003e\n\u003cli\u003eCoalesce, will not incur a full shuffle and will try to combine partitions.\u003c/li\u003e\n\u003cli\u003eBelow operation will shuffle your data into five partitions based on the destination country name, and then coalesce them (without a full shuffle):\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622511500_1231959797",
      "id": "20180807-061511_352101292",
      "dateCreated": "Aug 7, 2018 6:15:11 AM",
      "dateStarted": "Aug 7, 2018 6:16:13 AM",
      "dateFinished": "Aug 7, 2018 6:16:13 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val coalescepart \u003d df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)\ncoalescepart.rdd.getNumPartitions\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:33:41 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "coalescepart: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nres292: Int \u003d 2\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622573781_-2117401972",
      "id": "20180807-061613_1486454009",
      "dateCreated": "Aug 7, 2018 6:16:13 AM",
      "dateStarted": "Aug 7, 2018 6:16:44 AM",
      "dateFinished": "Aug 7, 2018 6:16:45 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n\n### Coalesce example\n\nSo, it would go something like this:\n\nNode 1 \u003d 1,2,3\nNode 2 \u003d 4,5,6\nNode 3 \u003d 7,8,9\nNode 4 \u003d 10,11,12\n\nThen coalesce down to 2 partitions:\n\nNode 1 \u003d 1,2,3 + (10,11,12)\nNode 3 \u003d 7,8,9 + (4,5,6)\n\nNotice that Node 1 and Node 3 did not require its original data to move.\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:33:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eCoalesce example\u003c/h3\u003e\n\u003cp\u003eSo, it would go something like this:\u003c/p\u003e\n\u003cp\u003eNode 1 \u003d 1,2,3\n\u003cbr  /\u003eNode 2 \u003d 4,5,6\n\u003cbr  /\u003eNode 3 \u003d 7,8,9\n\u003cbr  /\u003eNode 4 \u003d 10,11,12\u003c/p\u003e\n\u003cp\u003eThen coalesce down to 2 partitions:\u003c/p\u003e\n\u003cp\u003eNode 1 \u003d 1,2,3 + (10,11,12)\n\u003cbr  /\u003eNode 3 \u003d 7,8,9 + (4,5,6)\u003c/p\u003e\n\u003cp\u003eNotice that Node 1 and Node 3 did not require its original data to move.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533623623630_1585535290",
      "id": "20180807-063343_1111193317",
      "dateCreated": "Aug 7, 2018 6:33:43 AM",
      "dateStarted": "Aug 7, 2018 6:33:48 AM",
      "dateFinished": "Aug 7, 2018 6:33:48 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Collecting Rows to the Driver\n- Spark maintains the state of the cluster in the driver.\n- There are times when you’ll want to collect some of your data to the driver in order to manipulate it on your local machine.\n- collect gets all data from the entire DataFrame, take selects the first N rows, and show prints out a number of rows nicely.",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:17:45 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eCollecting Rows to the Driver\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSpark maintains the state of the cluster in the driver.\u003c/li\u003e\n\u003cli\u003eThere are times when you’ll want to collect some of your data to the driver in order to manipulate it on your local machine.\u003c/li\u003e\n\u003cli\u003ecollect gets all data from the entire DataFrame, take selects the first N rows, and show prints out a number of rows nicely.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622604684_1781319930",
      "id": "20180807-061644_783923931",
      "dateCreated": "Aug 7, 2018 6:16:44 AM",
      "dateStarted": "Aug 7, 2018 6:17:45 AM",
      "dateFinished": "Aug 7, 2018 6:17:45 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val collectDF \u003d df.limit(10)\ncollectDF.take(5) // take works with an Integer count\ncollectDF.show() // this prints it out nicely\ncollectDF.show(5, false)\ncollectDF.collect()\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:20:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "collectDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nres293: Array[org.apache.spark.sql.Row] \u003d Array([United States,Romania,15], [United States,Croatia,1], [United States,Ireland,344], [Egypt,United States,15], [United States,India,62])\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n|    United States|          Singapore|    1|\n|    United States|            Grenada|   62|\n|       Costa Rica|      United States|  588|\n|          Senegal|      United States|   40|\n|          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|United States    |Romania            |15   |\n|United States    |Croatia            |1    |\n|United States    |Ireland            |344  |\n|Egypt            |United States      |15   |\n|United States    |India              |62   |\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\nres296: Array[org.apache.spark.sql.Row] \u003d Array([United States,Romania,15], [United States,Croatia,1], [United States,Ireland,344], [Egypt,United States,15], [United States,India,62], [United States,Singapore,1], [United States,Grenada,62], [Costa Rica,United States,588], [Senegal,United States,40], [Moldova,United States,1])\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622665701_-42531063",
      "id": "20180807-061745_130745049",
      "dateCreated": "Aug 7, 2018 6:17:45 AM",
      "dateStarted": "Aug 7, 2018 6:18:21 AM",
      "dateFinished": "Aug 7, 2018 6:18:24 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val itr \u003d collectDF.toLocalIterator()\n\nitr.next()",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:21:11 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "itr: java.util.Iterator[org.apache.spark.sql.Row] \u003d IteratorWrapper(non-empty iterator)\nres301: org.apache.spark.sql.Row \u003d [United States,Romania,15]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622701629_-797931072",
      "id": "20180807-061821_1892674100",
      "dateCreated": "Aug 7, 2018 6:18:21 AM",
      "dateStarted": "Aug 7, 2018 6:21:11 AM",
      "dateFinished": "Aug 7, 2018 6:21:12 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n##### Any collection of data to the driver can be a very expensive operation! If you have a large dataset and call collect, you can crash the driver. If you use toLocalIterator and have very large partitions, you can easily crash the driver node and lose the state of your application. This is also expensive because we can operate on a one-by-one basis, instead of running computation in parallel.",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:21:59 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch5\u003eAny collection of data to the driver can be a very expensive operation! If you have a large dataset and call collect, you can crash the driver. If you use toLocalIterator and have very large partitions, you can easily crash the driver node and lose the state of your application. This is also expensive because we can operate on a one-by-one basis, instead of running computation in parallel.\u003c/h5\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533622752902_1503529579",
      "id": "20180807-061912_1128944264",
      "dateCreated": "Aug 7, 2018 6:19:12 AM",
      "dateStarted": "Aug 7, 2018 6:21:59 AM",
      "dateFinished": "Aug 7, 2018 6:21:59 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\n",
      "user": "anonymous",
      "dateUpdated": "Aug 7, 2018 6:22:08 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1533622915128_1348673342",
      "id": "20180807-062155_51376423",
      "dateCreated": "Aug 7, 2018 6:21:55 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "DataFrame_fundamental_operations",
  "id": "2DNJMUSM1",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}