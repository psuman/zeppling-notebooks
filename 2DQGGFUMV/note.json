{
  "paragraphs": [
    {
      "text": "%md\n\n### Performance Tuning\n\n- There are a variety of different parts of Spark jobs that you might want to optimize, and it’s valuable to be specific. \n- Following are some of the areas\n            - Code-level design choices (e.g., RDDs versus DataFrames)\n            - Data at rest\n            - Joins\n            - Aggregations\n            - Data in flight\n            - Individual application properties\n            - Inside of the Java Virtual Machine (JVM) of an executor\n            - Worker nodes\n            - Cluster\n            - deployment properties\n\n\n#### Scala versus Java versus Python versus R\n\n- if you want to perform some single-node machine learning after performing a large ETL job, we might recommend running your Extract, Transform, and Load (ETL) code as SparkR code and then using R’s massive machine learning ecosystem to run your single-node machine learning algorithms. \n- This gives you the best of both worlds and takes advantage of the strength of R as well as the strength of Spark without sacrifices. \n- when you need to include custom transformations that cannot be created in the Structured APIs. These might manifest themselves as RDD transformations or user-defined functions (UDFs). \n- If you’re going to do this, R and Python are not necessarily the best choice simply because of how this is actually executed. It’s also more difficult to provide stricter guarantees of types and manipulations when you’re defining functions that jump across languages\n- We find that using Python for the majority of the application, and porting some of it to Scala or writing specific UDFs in Scala as your application evolves, is a powerful technique—it allows for a nice balance between overall usability, maintainability, and performance.\n \n\n#### DataFrames versus SQL versus Datasets versus RDDs\n\n- Across all languages, DataFrames, Datasets, and SQL are equivalent in speed. This means that if you’re using DataFrames in any of these languages, performance is equal.\n- However, if you’re going to be defining UDFs, you’ll take a performance hit writing those in Python or R, and to some extent a lesser performance hit in Java and Scala. \n- Although all DataFrame, SQL, and Dataset code compiles down to RDDs, Spark’s optimization engine will write “better” RDD code than you can manually and certainly do it with orders of magnitude less effort.\n- Additionally, you will lose out on new optimizations that are added to Spark’s SQL engine every release.Lastly, \n- if you want to use RDDs, we definitely recommend using Scala or Java. when Python runs RDD code, it’s serializes a lot of data to and from the Python process. This is very expensive to run over very big data and can also decrease stability.\n\n#### Object Serialization in RDDs\n\n- When you’re working with custom data types, you’re going to want to serialize them using Kryo because it’s both more compact and much more efficient than Java serialization. \n- However, this does come at the inconvenience of registering the classes that you will be using in your application.\n- You can use Kryo serialization by setting spark.serializer to org.apache.spark.serializer.KryoSerializer. \n- You will also need to explicitly register the classes that you would like to register with the Kryo serializer via the spark.kryo.classesToRegister configuration.\n\n#### Cluster/application sizing and sharing\n\n- This somewhat comes down to a resource sharing and scheduling problem; however, there are a lot of options for how you want to share resources at the cluster level or at the application level.\n- Dynamic allocationSpark provides a mechanism to dynamically adjust the resources your application occupies based on the workload. \n- This means that your application can give resources back to the cluster if they are no longer used, and request them again later when there is demand.\n- This feature is particularly useful if multiple applications share resources in your Spark cluster. \n- setting spark.scheduler.mode to FAIR to allow better sharing of resources across multiple users, or setting --max-executor-cores, which specifies the maximum number of executor cores that your application will need. \n- Specifying this value can ensure that your application does not take up all the resources on the cluster.\n\n#### Data at Rest\n\n- when you’re saving data it will be read many times as other folks in your organization access the same datasets in order to run different analyses. Making sure that you’re storing your data for effective reads later on is absolutely essential to successful big data projects. \n- This involves choosing your storage system, choosing your data format, and taking advantage of features such as data partitioning in some storage formats.\n\n##### File-based long-term data storage\n\n- Generally you should always favor structured, binary types to store your data, especially when you’ll be accessing it frequently.\n- Although files like “CSV” seem well-structured, they’re very slow to parse, and often also full of edge cases and pain points. For instance, improperly escaped new-line characters can often cause a lot of trouble when reading a large number of files. \n- The most efficient file format you can generally choose is Apache Parquet. \n- Parquet stores data in binary files with column-oriented storage, and also tracks some statistics about each file that make it possible to quickly skip data not needed for a query. \n- It is well integrated with Spark through the built-in Parquet data source.\n\n##### Splittable file types and compression\n\n- Whatever file format you choose, you should make sure it is “splittable”, which means that different tasks can read different parts of the file in parallel.\n- When we read in the file, all cores were able to do part of the work when the file was splittable.\n- If we didn’t use a splittable file type—say something like a malformed JSON file—we’re going to need to read in the entire file on a single machine, greatly reducing parallelism.\n- The main place splittability comes in is compression formats. A ZIP file or TAR archive cannot be split, which means that even if we have 10 files in a ZIP file and 10 cores, only one core can read in that data because we cannot parallelize access to the ZIP file. This is a poor use of resources.\n- In contrast, files compressed using gzip, bzip2, or lz4 are generally splittable if they were written by a parallel processing framework like Hadoop or Spark. \n- For your own input data, the simplest way to make it splittable is to upload it as separate files, ideally each no larger than a few hundred megabytes.\n\n\n##### Table partitioning\n\n-  Table partitioning refers to storing files in separate directories based on a key, such as the date field in the data. \n-  Storage managers like Apache Hive support this concept, as do many of Spark’s built-in data sources. \n-  Partitioning your data correctly allows Spark to skip many irrelevant files when it only requires data with a specific range of keys. \n-  For instance, if users frequently filter by “date” or “customerId” in their queries, partition your data by those columns. This will greatly reduce the amount of data that end users must read by most queries, and therefore dramatically increase speed.\n-  The one downside of partitioning, however, is that if you partition at too fine a granularity, it can result in many small files, and a great deal of overhead trying to list all the files in the storage system.\n\n\n##### Bucketing\n\n- The essense is that bucketing your data allows Spark to “pre-partition” data according to how joins or aggregations are likely to be performed by readers. \n- This can improve performance and stability because data can be consistently distributed across partitions as opposed to skewed into just one or two. \n- For instance, if joins are frequently performed on a column immediately after a read, you can use bucketing to ensure that the data is well partitioned according to those values. \n- This can help prevent a shuffle before a join and therefore help speed up data access. \n- Bucketing generally works hand-in-hand with partitioning as a second way of physically splitting up data\n\n##### The number of files\n\n- If there are lots of small files, you’re going to pay a price listing and fetching each of those individual files.\n- For instance, if you’re reading a data from Hadoop Distributed File System (HDFS), this data is managed in blocks that are up to 128 MB in size (by default). \n- This means if you have 30 files, of 5 MB each, you’re going to have to potentially request 30 blocks, even though the same data could have fit into 2 blocks (150 MB total).\n- Having lots of small files is going to make the scheduler work much harder to locate the data and launch all of the read tasks. This can increase the network and scheduling overhead of the job. \n- Having fewer large files eases the pain off the scheduler but it will also make tasks run longer. In this case, though, you can always launch more tasks than there are input files if you want more parallelism—\n- Spark will split each file across multiple tasks assuming you are using a splittable format. \n- In general, we recommend sizing your files so that they each contain at least a few tens of megatbytes of data.\n- To control how many records go into each file, you can specify the maxRecordsPerFile option to the write operation.\n\n##### Data locality\n\n- Data locality basically specifies a preference for certain nodes that hold certain data, rather than having to exchange these blocks of data over the network. \n- If you run your storage system on the same nodes as Spark, and the system supports locality hints, Spark will try to schedule tasks close to each input block of data.\n- For example HDFS storage provides this option. There are several configurations that affect locality, but it will generally be used by default if Spark detects that it is using a local storage system.",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 9:59:47 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003ePerformance Tuning\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThere are a variety of different parts of Spark jobs that you might want to optimize, and it’s valuable to be specific.\u003c/li\u003e\n\u003cli\u003eFollowing are some of the areas\u003cpre\u003e\u003ccode\u003e    - Code-level design choices (e.g., RDDs versus DataFrames)\n    - Data at rest\n    - Joins\n    - Aggregations\n    - Data in flight\n    - Individual application properties\n    - Inside of the Java Virtual Machine (JVM) of an executor\n    - Worker nodes\n    - Cluster\n    - deployment properties\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534064794725_-154108336",
      "id": "20180812-090634_1427838345",
      "dateCreated": "Aug 12, 2018 9:06:34 AM",
      "dateStarted": "Aug 12, 2018 9:37:38 AM",
      "dateFinished": "Aug 12, 2018 9:37:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 9:37:38 AM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534066658337_648940884",
      "id": "20180812-093738_642930717",
      "dateCreated": "Aug 12, 2018 9:37:38 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "spark_performance_tuning",
  "id": "2DQGGFUMV",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}