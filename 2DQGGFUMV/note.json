{
  "paragraphs": [
    {
      "text": "%md\n\n### Performance Tuning\n\n- There are a variety of different parts of Spark jobs that you might want to optimize, and it’s valuable to be specific. \n- Following are some of the areas\n            - Code-level design choices (e.g., RDDs versus DataFrames)\n            - Data at rest\n            - Joins\n            - Aggregations\n            - Data in flight\n            - Individual application properties\n            - Inside of the Java Virtual Machine (JVM) of an executor\n            - Worker nodes\n            - Cluster\n            - deployment properties\n\n\n### Indirect Performance Enhancements\n\n#### Scala versus Java versus Python versus R\n\n- if you want to perform some single-node machine learning after performing a large ETL job, we might recommend running your Extract, Transform, and Load (ETL) code as SparkR code and then using R’s massive machine learning ecosystem to run your single-node machine learning algorithms. \n- This gives you the best of both worlds and takes advantage of the strength of R as well as the strength of Spark without sacrifices. \n- when you need to include custom transformations that cannot be created in the Structured APIs. These might manifest themselves as RDD transformations or user-defined functions (UDFs). \n- If you’re going to do this, R and Python are not necessarily the best choice simply because of how this is actually executed. It’s also more difficult to provide stricter guarantees of types and manipulations when you’re defining functions that jump across languages\n- We find that using Python for the majority of the application, and porting some of it to Scala or writing specific UDFs in Scala as your application evolves, is a powerful technique—it allows for a nice balance between overall usability, maintainability, and performance.\n \n\n#### DataFrames versus SQL versus Datasets versus RDDs\n\n- Across all languages, DataFrames, Datasets, and SQL are equivalent in speed. This means that if you’re using DataFrames in any of these languages, performance is equal.\n- However, if you’re going to be defining UDFs, you’ll take a performance hit writing those in Python or R, and to some extent a lesser performance hit in Java and Scala. \n- Although all DataFrame, SQL, and Dataset code compiles down to RDDs, Spark’s optimization engine will write “better” RDD code than you can manually and certainly do it with orders of magnitude less effort.\n- Additionally, you will lose out on new optimizations that are added to Spark’s SQL engine every release.Lastly, \n- if you want to use RDDs, we definitely recommend using Scala or Java. when Python runs RDD code, it’s serializes a lot of data to and from the Python process. This is very expensive to run over very big data and can also decrease stability.\n\n#### Object Serialization in RDDs\n\n- When you’re working with custom data types, you’re going to want to serialize them using Kryo because it’s both more compact and much more efficient than Java serialization. \n- However, this does come at the inconvenience of registering the classes that you will be using in your application.\n- You can use Kryo serialization by setting spark.serializer to org.apache.spark.serializer.KryoSerializer. \n- You will also need to explicitly register the classes that you would like to register with the Kryo serializer via the spark.kryo.classesToRegister configuration.\n\n#### Cluster/application sizing and sharing\n\n- This somewhat comes down to a resource sharing and scheduling problem; however, there are a lot of options for how you want to share resources at the cluster level or at the application level.\n- Dynamic allocationSpark provides a mechanism to dynamically adjust the resources your application occupies based on the workload. \n- This means that your application can give resources back to the cluster if they are no longer used, and request them again later when there is demand.\n- This feature is particularly useful if multiple applications share resources in your Spark cluster. \n- setting spark.scheduler.mode to FAIR to allow better sharing of resources across multiple users, or setting --max-executor-cores, which specifies the maximum number of executor cores that your application will need. \n- Specifying this value can ensure that your application does not take up all the resources on the cluster.\n\n#### Data at Rest\n\n- when you’re saving data it will be read many times as other folks in your organization access the same datasets in order to run different analyses. Making sure that you’re storing your data for effective reads later on is absolutely essential to successful big data projects. \n- This involves choosing your storage system, choosing your data format, and taking advantage of features such as data partitioning in some storage formats.\n\n##### File-based long-term data storage\n\n- Generally you should always favor structured, binary types to store your data, especially when you’ll be accessing it frequently.\n- Although files like “CSV” seem well-structured, they’re very slow to parse, and often also full of edge cases and pain points. For instance, improperly escaped new-line characters can often cause a lot of trouble when reading a large number of files. \n- The most efficient file format you can generally choose is Apache Parquet. \n- Parquet stores data in binary files with column-oriented storage, and also tracks some statistics about each file that make it possible to quickly skip data not needed for a query. \n- It is well integrated with Spark through the built-in Parquet data source.\n\n##### Splittable file types and compression\n\n- Whatever file format you choose, you should make sure it is “splittable”, which means that different tasks can read different parts of the file in parallel.\n- When we read in the file, all cores were able to do part of the work when the file was splittable.\n- If we didn’t use a splittable file type—say something like a malformed JSON file—we’re going to need to read in the entire file on a single machine, greatly reducing parallelism.\n- The main place splittability comes in is compression formats. A ZIP file or TAR archive cannot be split, which means that even if we have 10 files in a ZIP file and 10 cores, only one core can read in that data because we cannot parallelize access to the ZIP file. This is a poor use of resources.\n- In contrast, files compressed using gzip, bzip2, or lz4 are generally splittable if they were written by a parallel processing framework like Hadoop or Spark. \n- For your own input data, the simplest way to make it splittable is to upload it as separate files, ideally each no larger than a few hundred megabytes.\n\n\n##### Table partitioning\n\n-  Table partitioning refers to storing files in separate directories based on a key, such as the date field in the data. \n-  Storage managers like Apache Hive support this concept, as do many of Spark’s built-in data sources. \n-  Partitioning your data correctly allows Spark to skip many irrelevant files when it only requires data with a specific range of keys. \n-  For instance, if users frequently filter by “date” or “customerId” in their queries, partition your data by those columns. This will greatly reduce the amount of data that end users must read by most queries, and therefore dramatically increase speed.\n-  The one downside of partitioning, however, is that if you partition at too fine a granularity, it can result in many small files, and a great deal of overhead trying to list all the files in the storage system.\n\n\n##### Bucketing\n\n- The essense is that bucketing your data allows Spark to “pre-partition” data according to how joins or aggregations are likely to be performed by readers. \n- This can improve performance and stability because data can be consistently distributed across partitions as opposed to skewed into just one or two. \n- For instance, if joins are frequently performed on a column immediately after a read, you can use bucketing to ensure that the data is well partitioned according to those values. \n- This can help prevent a shuffle before a join and therefore help speed up data access. \n- Bucketing generally works hand-in-hand with partitioning as a second way of physically splitting up data\n\n##### The number of files\n\n- If there are lots of small files, you’re going to pay a price listing and fetching each of those individual files.\n- For instance, if you’re reading a data from Hadoop Distributed File System (HDFS), this data is managed in blocks that are up to 128 MB in size (by default). \n- This means if you have 30 files, of 5 MB each, you’re going to have to potentially request 30 blocks, even though the same data could have fit into 2 blocks (150 MB total).\n- Having lots of small files is going to make the scheduler work much harder to locate the data and launch all of the read tasks. This can increase the network and scheduling overhead of the job. \n- Having fewer large files eases the pain off the scheduler but it will also make tasks run longer. In this case, though, you can always launch more tasks than there are input files if you want more parallelism—\n- Spark will split each file across multiple tasks assuming you are using a splittable format. \n- In general, we recommend sizing your files so that they each contain at least a few tens of megatbytes of data.\n- To control how many records go into each file, you can specify the maxRecordsPerFile option to the write operation.\n\n##### Data locality\n\n- Data locality basically specifies a preference for certain nodes that hold certain data, rather than having to exchange these blocks of data over the network. \n- If you run your storage system on the same nodes as Spark, and the system supports locality hints, Spark will try to schedule tasks close to each input block of data.\n- For example HDFS storage provides this option. There are several configurations that affect locality, but it will generally be used by default if Spark detects that it is using a local storage system.\n\n\n##### Statistics collection\n\n\n![sv-image](https://user-images.githubusercontent.com/1182329/44001058-5f63536e-9e48-11e8-9133-864edba6e37d.png)\n\n\n- Spark includes a cost-based query optimizer that plans queries based on the properties of the input data when using the structured APIs.\n- However, to allow the cost-based optimizer to make these sorts of decisions, you need to collect (and maintain) statistics about your tables that it can use. \n- There are two kinds of statistics: \n        - table-level \n        - column-level . \n\n- Statistics collection is available only on named tables, not on arbitrary DataFrames or RDDs.\n- To collect table-level statistics, you can run the following command:\n```\nANALYZE TABLE table_name COMPUTE STATISTICS\n\n```\n- To collect column-level statistics, you can name the specific columns:\n\n```\nANALYZE TABLE table_name COMPUTE STATISTICS FORCOLUMNS column_name1, column_name2, ...\n```\n\n- Column-level statistics are slower to collect, but provide more information for the cost-based optimizer to use about those data columns. \n- Both types of statistics can help with joins, aggregations, filters, and a number of other potential things (e.g., automatically choosing when to do a broadcast join). \n\n\n![sv-image](https://user-images.githubusercontent.com/1182329/44001059-64450864-9e48-11e8-9a2c-5476584f7646.png)\n\n- https://databricks.com/blog/2017/08/31/cost-based-optimizer-in-apache-spark-2-2.html\n\n\n##### Shuffle Configurations\n\n- Configuring Spark’s external shuffle service can often increase performance because it allows nodes to read shuffle data from remote machines even when the executors on those machines are busy (e.g., with garbage collection). \n- This does come at the cost of complexity and maintenance\n- For all jobs, the number of partitions of a shuffle matters. If you have too few partitions, then too few nodes will be doing work and there may be skew, but if you have too many partitions, there is an overhead to launching each one that may start to dominate. \n- Try to aim for at least a few tens of megabytes of data per output partition in your shuffle.\n\n##### Memory Pressure and Garbage Collection\n\n- During the course of running Spark jobs, the executor or driver machines may struggle to complete their tasks because of a lack of sufficient memory or “memory pressure.”\n- This may occur when an application takes up too much memory during execution or when garbage collection runs too frequently or is slow to run as large numbers of objects are created in the JVM and subsequently garbage collected as they are no longer used.\n- One strategy for easing this issue is to ensure that you’re using the Structured APIs as much as possible. It will greatly reduce memory pressure because JVM objects are never realized and Spark SQL simply performs the computation on its internal format.\n\n\n### Direct Performance Enhancements\n\n##### Parallelism\n\n- The first thing you should do whenever trying to speed up a specific stage is to increase the degree of parallelism. \n- In general, we recommend having at least two or three tasks per CPU core in your cluster if the stage processes a large amount of data.\n- You can set this via the spark.default.parallelism property as well as tuning the spark.sql.shuffle.partitions according to the number of cores in your cluster.\n\n##### Improved Filtering\n\n- Mobe filters to the earliest part of your Spark job. \n- Sometimes, these filters can be pushed into the data sources themselves and this means that you can avoid reading and working with data that is irrelevant to your end result. \n- Enabling partitioning and bucketing also helps achieve this. \n- Always look to be filtering as much data as you can early on, and you’ll find that your Spark jobs will almost always run faster.\n\n##### Repartitioning and Coalescing\n\n- Repartition calls can incur a shuffle. However, doing some can optimize the overall execution of a job by balancing data across the cluster, so they can be worth it. \n- In general, you should try to shuffle the least amount of data possible. For this reason, if you’re reducing the number of overall partitions in a DataFrame or RDD, first try coalesce method, which will not perform a shuffle but rather merge partitions on the same node into one partition.\n- The slower repartition method will also shuffle data across the network to achieve even load balancing. \n- Repartitions can be particularly helpful when performing joins or prior to a cache call.\n\n##### Custom partitioning\n\n- You might want to explore performing custom partitioning at the RDD level. \n- This allows you to define a custom partition function that will organize the data across the cluster to a finer level of precision than is available at the DataFrame level.\n\n##### User-Defined Functions (UDFs)\n\n- In general, avoiding UDFs is a good optimization opportunity. \n- UDFs are expensive because they force representing data as objects in the JVM and sometimes do this multiple times per record in a query. \n- You should try to use the Structured APIs as much as possible to perform your manipulations simply because they are going to perform the transformations in a much more efficient manner than you can do in a high-level language.\n\n##### Temporary Data Storage (Caching)\n\n- In applications that reuse the same datasets over and over, one of the most useful optimizations is caching. Caching will place a DataFrame, table, or RDD into temporary storage (either memory or disk) across the executors in your cluster, and make subsequent reads faster\n- Although caching might sound like something we should do all the time, it’s not always a good thing to do. \n- That’s because caching data incurs a serialization, deserialization, and storage cost. For example, if you are only going to process a dataset once (in a later transformation), caching it will only slow you down.\n- The use case for caching is simple: as you work with data in Spark, either within an interactive session or a standalone application, you will often want to reuse a certain dataset (e.g., a DataFrame or RDD). \n- For example, in an interactive data science session, you might load and clean your data and then reuse it to try multiple statistical models\n- Caching is a lazy operation, meaning that things will be cached only as they are accessed. The RDD API and the Structured API differ in how they actually perform caching\n- When we cache an RDD, we cache the actual, physical data (i.e., the bits). The bits. When this data is accessed again, Spark returns the proper data. This is done through the RDD reference. \n- However, in the Structured API, caching is done based on the physical plan. This means that we effectively store the physical plan as our key (as opposed to the object reference) and perform a lookup prior to the execution of a Structured job. \n- This can cause confusion because sometimes you might be expecting to access raw data but because someone else already cached the data, you’re actually accessing their cached version. Keep that in mind when using this feature.\n- There are different storage levels that you can use to cache your data\n\n![sv-image](https://user-images.githubusercontent.com/1182329/44001248-22d98ae0-9e4c-11e8-8720-c3815ff3f9b4.png)\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 10:55:52 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003ePerformance Tuning\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThere are a variety of different parts of Spark jobs that you might want to optimize, and it’s valuable to be specific.\u003c/li\u003e\n\u003cli\u003eFollowing are some of the areas\u003cpre\u003e\u003ccode\u003e    - Code-level design choices (e.g., RDDs versus DataFrames)\n    - Data at rest\n    - Joins\n    - Aggregations\n    - Data in flight\n    - Individual application properties\n    - Inside of the Java Virtual Machine (JVM) of an executor\n    - Worker nodes\n    - Cluster\n    - deployment properties\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534064794725_-154108336",
      "id": "20180812-090634_1427838345",
      "dateCreated": "Aug 12, 2018 9:06:34 AM",
      "dateStarted": "Aug 12, 2018 9:37:38 AM",
      "dateFinished": "Aug 12, 2018 9:37:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n\nif [ -e /tmp/2015-summary.csv ]\nthen\n    rm -f /tmp/2015-summary.csv\nfi\n\nwget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/csv/2015-summary.csv -O /tmp/2015-summary.csv\n\nhdfs dfs -rm -f /tmp/2015-summary.csv\n\n# Move downloaded JSON file from local storage to HDFS\nhdfs dfs -put /tmp/2015-summary.csv /tmp\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 10:59:02 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "--2018-08-12 10:59:02--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/csv/2015-summary.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7080 (6.9K) [text/plain]\nSaving to: ‘/tmp/2015-summary.csv’\n\n     0K ......                                                100% 26.6M\u003d0s\n\n2018-08-12 10:59:05 (26.6 MB/s) - ‘/tmp/2015-summary.csv’ saved [7080/7080]\n\n18/08/12 10:59:11 INFO fs.TrashPolicyDefault: Moved: \u0027hdfs://sandbox-hdp.hortonworks.com:8020/tmp/2015-summary.csv\u0027 to trash at: hdfs://sandbox-hdp.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/2015-summary.csv1534071551475\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534071464822_736297424",
      "id": "20180812-105744_521563291",
      "dateCreated": "Aug 12, 2018 10:57:44 AM",
      "dateStarted": "Aug 12, 2018 10:59:02 AM",
      "dateFinished": "Aug 12, 2018 10:59:21 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val DF1 \u003d spark.read.format(\"csv\")\n                .option(\"inferSchema\", \"true\")\n                .option(\"header\", \"true\")\n                .load(\"/tmp/2015-summary.csv\")\n\nval DF2 \u003d DF1.groupBy(\"DEST_COUNTRY_NAME\").count().collect()\nval DF3 \u003d DF1.groupBy(\"ORIGIN_COUNTRY_NAME\").count().collect()\nval DF4 \u003d DF1.groupBy(\"count\").count().collect()\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 11:05:06 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DF1: org.apache.spark.sql.DataFrame \u003d [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nDF2: Array[org.apache.spark.sql.Row] \u003d Array([Anguilla,1], [Russia,1], [Paraguay,1], [Senegal,1], [Sweden,1], [Kiribati,1], [Guyana,1], [Philippines,1], [Djibouti,1], [Malaysia,1], [Singapore,1], [Fiji,1], [Turkey,1], [Iraq,1], [Germany,1], [Jordan,1], [Palau,1], [Turks and Caicos Islands,1], [France,1], [Greece,1], [Kosovo,1], [Taiwan,1], [British Virgin Islands,1], [Dominica,1], [Algeria,1], [Argentina,1], [Belgium,1], [Angola,1], [Ecuador,1], [Qatar,1], [Finland,1], [New Caledonia,1], [Nicaragua,1], [Ghana,1], [Peru,1], [United States,125], [India,1], [China,1], [Curacao,1], [Malta,1], [Kuwait,1], [Marshall Islands,1], [Chile,1], [Martinique,1], [Cayman Islands,1], [Croatia,1], [Bolivia,1], [Nigeria,1], [Italy,1], [Suriname,1], [Norway,1], [Spain,1], [Cuba,1], [Guadeloupe,1], [Denmar...DF3: Array[org.apache.spark.sql.Row] \u003d Array([Paraguay,1], [Russia,1], [Anguilla,1], [Senegal,1], [Sweden,1], [Kiribati,1], [Guyana,1], [Philippines,1], [Singapore,1], [Malaysia,1], [Fiji,1], [Turkey,1], [Germany,1], [Jordan,1], [Palau,1], [Turks and Caicos Islands,1], [France,1], [Greece,1], [British Virgin Islands,1], [Taiwan,1], [Dominica,1], [Argentina,1], [Angola,1], [Belgium,1], [Ecuador,1], [Qatar,1], [Finland,1], [Nicaragua,1], [Ghana,1], [Peru,1], [United States,132], [India,1], [China,1], [Curacao,1], [Kuwait,1], [Malta,1], [Marshall Islands,1], [Chile,1], [Martinique,1], [Cayman Islands,1], [Croatia,1], [Nigeria,1], [Bolivia,1], [Italy,1], [Suriname,1], [Lithuania,1], [Norway,1], [Spain,1], [Cuba,1], [Guadeloupe,1], [Denmark,1], [Barbados,1], [Ireland,1], [Thailand,1], [Moroc...DF4: Array[org.apache.spark.sql.Row] \u003d Array([31,1], [2025,1], [588,1], [53,1], [853,1], [362,1], [1468,1], [155,1], [108,1], [211,1], [34,1], [193,2], [126,2], [115,1], [772,1], [28,2], [183,1], [1496,1], [300,1], [332,1], [26,2], [27,1], [44,3], [236,1], [329,1], [12,5], [230,1], [225,1], [246,1], [346,1], [111,1], [8399,1], [370002,1], [827,1], [152,1], [185,1], [305,1], [325,2], [955,1], [660,1], [935,1], [259,1], [1,25], [7140,1], [442,1], [13,8], [6,1], [3,7], [867,1], [40,3], [20,3], [1353,1], [139,1], [920,1], [776,1], [235,1], [266,1], [268,1], [5,1], [279,1], [258,1], [19,3], [1420,1], [397,1], [64,1], [117,1], [1048,1], [41,2], [154,1], [15,5], [43,3], [666,1], [179,1], [61,1], [127,2], [318,1], [107,1], [202,1], [619,1], [35,1], [310,1], [290,1], [986,1], [217,1], [4,3], [59..."
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534066658337_648940884",
      "id": "20180812-093738_642930717",
      "dateCreated": "Aug 12, 2018 9:37:38 AM",
      "dateStarted": "Aug 12, 2018 11:05:07 AM",
      "dateFinished": "Aug 12, 2018 11:05:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// We used the count above to eagerly cache the data (basically perform an action to force Spark to store it in memory), because caching itself is lazy—the data is cached only on the first time you run an action on the DataFrame\n\nDF1.cache()\nDF1.count()",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 11:00:26 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534071418078_-1276400693",
      "id": "20180812-105658_806164900",
      "dateCreated": "Aug 12, 2018 10:56:58 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Now that the data is cached, the previous commands will be faster, as we can see by running the following code\n\nDF2 \u003d DF1.groupBy(\"DEST_COUNTRY_NAME\").count().collect()\nDF3 \u003d DF1.groupBy(\"ORIGIN_COUNTRY_NAME\").count().collect()\nDF4 \u003d DF1.groupBy(\"count\").count().collect()",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 11:01:02 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534071626806_-255922372",
      "id": "20180812-110026_1684738596",
      "dateCreated": "Aug 12, 2018 11:00:26 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- The cache command in Spark always places data in memory by default, caching only part of the dataset if the cluster’s total memory is full. \n\n- For more control, there is also a persist method that takes a StorageLevel object to specify where to cache the data: in memory, on disk, or both.",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 11:02:09 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534071706792_872435593",
      "id": "20180812-110146_664898508",
      "dateCreated": "Aug 12, 2018 11:01:46 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Joins\n\n- Equi-joins are the easiest for Spark to optimize at this point and therefore should be preferred wherever possible. \n- Simple things like trying to use the filtering ability of inner joins by changing join ordering can yield large speedups. \n- Using broadcast join hints can help Spark make intelligent planning decisions when it comes to creating query plans\n- Collecting statistics on tables prior to a join will help Spark make intelligent join decisions. \n- Bucketing your data appropriately can also help Spark avoid large shuffles when joins are performed.\n\n### Aggregations\n\n- There are not too many ways that you can optimize specific aggregations beyond filtering data before the aggregation having a sufficiently high number of partitions. \n- However, if you’re using RDDs, controlling exactly how these aggregations are performed (e.g., using reduceByKey when possible over groupByKey) can be very helpful and improve the speed and stability of your code.\n- If some large piece of data will be used across multiple UDF calls in your program, you can broadcast it to save just a single read-only copy on each node and avoid re-sending this data with each job. \n- For example, broadcast variables may be useful to save a lookup table or a machine learning model.",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 11:07:26 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "editOnDblClick": true,
          "language": "markdown"
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534071729700_-1025884831",
      "id": "20180812-110209_1533245407",
      "dateCreated": "Aug 12, 2018 11:02:09 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "spark_performance_tuning",
  "id": "2DQGGFUMV",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}