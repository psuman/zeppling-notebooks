{
  "paragraphs": [
    {
      "text": "%md\n\n### Spark SQL\n\n- with Spark SQL you can run SQL queries against views or tables organized into databases. \n- You also can use system functions or define user functions and analyze query plans in order to optimize their workloads. \n- This integrates directly into the DataFrame and Dataset API\n- You can choose to express some of your data manipulations in SQL and others in DataFrames and they will compile to the same underlying code.\n- Spark implements a subset of ANSI SQL:2003. \n- This SQL standard is one that is available in the majority of SQL databases and this support means that Spark successfully runs the popular benchmark TPC-DS\n- With the release of Spark 2.0, its authors created a superset of Hive’s support, writing a native SQL parser that supports both ANSI-SQL as well as HiveQL queries. This, along with its unique interoperability with DataFrames, makes it a powerful tool.\n- This unifying API allows for data to be extracted with SQL, manipulated as a DataFrame, passed into one of Spark MLlibs’ large-scale machine learning algorithms, written out to another data source, and everything in between.\n- Spark SQL is intended to operate as an online analytic processing (OLAP) database, not an online transaction processing (OLTP) database. This means that it is not intended to perform extremely low-latency queries. \n- Spark SQL has a great relationship with Hive because it can connect to Hive metastores. \n- The Hive metastore is the way in which Hive maintains table information for use across sessions. \n- With Spark SQL, you can connect to your Hive metastore (if you already have one) and access table metadata to reduce file listing when accessing information",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 6:45:11 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534315039709_-1338161785",
      "id": "20180815-063719_1400808385",
      "dateCreated": "Aug 15, 2018 6:37:19 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### How to Run Spark SQL Queries\n\n##### Spark SQL CLI\n\n- The Spark SQL CLI is a convenient tool with which you can make basic Spark SQL queries in local mode from the command line\n\n```\n./bin/spark-sql\n\n```\n\n##### Spark’s Programmatic SQL Interface\n\n- you can also execute SQL in an ad hoc manner via any of Spark’s language APIs. You can do this via the method sql on the SparkSession object. This returns a DataFrame\n\n```\nspark.sql(\"SELECT 1 + 1\").show()\n\n```\n\n- The command spark.sql(\"SELECT 1 + 1\") returns a DataFrame that we can then evaluate programmatically. Just like other transformations, this will not be executed eagerly but lazily.\n- This is an immensely powerful interface because there are some transformations that are much simpler to express in SQL code than in DataFrames.\n\n\n- You can express multiline queries quite simply by passing a multiline string into the function\n\n```\nspark.sql(\"\"\"SELECT user_id, department, first_name FROM professors  WHERE department IN    (SELECT name FROM department WHERE created_date \u003e\u003d \u00272016-01-01\u0027)\"\"\")\n\n```\n\n- Even more powerful, you can completely interoperate between SQL and DataFrames, as you see fit. For instance, you can create a DataFrame, manipulate it with SQL, and then manipulate it again as a DataFrame\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 6:51:14 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534315546672_304463730",
      "id": "20180815-064546_759521013",
      "dateCreated": "Aug 15, 2018 6:45:46 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n\nif [ -e /tmp/2015-summary.json ]\nthen\n    rm -f /tmp/2015-summary.json\nfi\n\nwget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/json/2015-summary.json -O /tmp/2015-summary.json\n\nhdfs dfs -rm -f /tmp/2015-summary.json\n\n# Move downloaded JSON file from local storage to HDFS\nhdfs dfs -put /tmp/2015-summary.json /tmp\n",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 6:51:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "--2018-08-15 06:51:52--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/json/2015-summary.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 21368 (21K) [text/plain]\nSaving to: ‘/tmp/2015-summary.json’\n\n     0K .......... ..........                                 100% 26.2M\u003d0.001s\n\n2018-08-15 06:51:53 (26.2 MB/s) - ‘/tmp/2015-summary.json’ saved [21368/21368]\n\n18/08/15 06:51:58 INFO fs.TrashPolicyDefault: Moved: \u0027hdfs://sandbox-hdp.hortonworks.com:8020/tmp/2015-summary.json\u0027 to trash at: hdfs://sandbox-hdp.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/2015-summary.json1534315918597\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534315880091_-1281980223",
      "id": "20180815-065120_1803207107",
      "dateCreated": "Aug 15, 2018 6:51:20 AM",
      "dateStarted": "Aug 15, 2018 6:51:51 AM",
      "dateFinished": "Aug 15, 2018 6:52:02 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.read\n     .json(\"/tmp/2015-summary.json\")  \n     .createOrReplaceTempView(\"flight_data_sql_view\") // DF \u003d\u003e SQL\n\nspark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count) FROM flight_data_sql_view GROUP BY DEST_COUNTRY_NAME\"\"\")\n     .where(\"DEST_COUNTRY_NAME like \u0027S%\u0027\")\n     .where(\"`sum(count)` \u003e 10\")\n     .count() // SQL \u003d\u003e DF\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 6:55:56 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res6: Long \u003d 12\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534315911173_-468190326",
      "id": "20180815-065151_17016029",
      "dateCreated": "Aug 15, 2018 6:51:51 AM",
      "dateStarted": "Aug 15, 2018 6:56:02 AM",
      "dateFinished": "Aug 15, 2018 6:56:41 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Spark SQL concepts\n\n#### Catalog\n\n- The highest level abstraction in Spark SQL is the Catalog. \n- The Catalog is an abstraction for the storage of metadata about the data stored in your tables as well as other helpful things like databases, tables, functions, and views. \n- The catalog is available in the org.apache.spark.sql.catalog.Catalog package and contains a number of helpful functions for doing things like listing tables, databases, and functions. \n\n#### Tables\n\n- To do anything useful with Spark SQL, you first need to define tables. \n- Tables are logically equivalent to a DataFrame in that they are a structure of data against which you run commands. \n- We can join tables, filter them, aggregate them, and perform different manipulations. \n- The core difference between tables and DataFrames is this: you define DataFrames in the scope of a programming language, whereas you define tables within a database. \n- This means that when you create a table (assuming you never changed the database), it will belong to the default database\n- An important thing to note is that in Spark 2.X, tables always contain data. There is no notion of a temporary table, only a view, which does not contain data. This is important because if you go to drop a table, you can risk losing the data when doing so.\n\n#### Spark-Managed vs Un Managed Tables\n\n- One important note is the concept of managed versus unmanaged tables. \n- Tables store two important pieces of information. The data within the tables as well as the data about the tables; that is, the metadata. \n- You can have Spark manage the metadata for a set of files as well as for the data.\n- When you define a table from files on disk, you are defining an unmanaged table.\n- When you use saveAsTable on a DataFrame, you are creating a managed table for which Spark will track of all of the relevant information.\n\n\n#### Creating Tables\n\n- You can create tables from a variety of sources. \n- Something fairly unique to Spark is the capability of reusing the entire Data Source API within SQL. \n- This means that you do not need to define a table and then load data into it; Spark lets you create one on the fly.",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 7:21:06 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534316005964_783518794",
      "id": "20180815-065325_248807490",
      "dateCreated": "Aug 15, 2018 6:53:25 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.sql(\"DROP TABLE IF EXISTS flights\")\n\nspark.sql(\"\"\"CREATE TABLE flights (DEST_COUNTRY_NAME STRING, \n                      ORIGIN_COUNTRY_NAME STRING, \n                      count LONG)\n             USING JSON OPTIONS (path \u0027/tmp/2015-summary.json\u0027)\"\"\")\n             \n\nspark.sql(\"select count(*) from flights\").show()\n",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 7:28:32 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res17: org.apache.spark.sql.DataFrame \u003d []\nres19: org.apache.spark.sql.DataFrame \u003d []\n+--------+\n|count(1)|\n+--------+\n|     256|\n+--------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534317666668_-709131270",
      "id": "20180815-072106_1147125089",
      "dateCreated": "Aug 15, 2018 7:21:06 AM",
      "dateStarted": "Aug 15, 2018 7:28:32 AM",
      "dateFinished": "Aug 15, 2018 7:28:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n\nif [ -e /tmp/2015-summary.csv ]\nthen\n    rm -f /tmp/2015-summary.csv\nfi\n\nwget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/csv/2015-summary.csv -O /tmp/2015-summary.csv\n\nhdfs dfs -rm -f /tmp/2015-summary.csv\n\n# Move downloaded JSON file from local storage to HDFS\nhdfs dfs -put /tmp/2015-summary.csv /tmp\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 7:30:17 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "--2018-08-15 07:30:17--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/csv/2015-summary.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7080 (6.9K) [text/plain]\nSaving to: ‘/tmp/2015-summary.csv’\n\n     0K ......                                                100% 26.3M\u003d0s\n\n2018-08-15 07:30:18 (26.3 MB/s) - ‘/tmp/2015-summary.csv’ saved [7080/7080]\n\n18/08/15 07:30:22 INFO fs.TrashPolicyDefault: Moved: \u0027hdfs://sandbox-hdp.hortonworks.com:8020/tmp/2015-summary.csv\u0027 to trash at: hdfs://sandbox-hdp.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/2015-summary.csv\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534317663844_-962696120",
      "id": "20180815-072103_1883810985",
      "dateCreated": "Aug 15, 2018 7:21:03 AM",
      "dateStarted": "Aug 15, 2018 7:30:17 AM",
      "dateFinished": "Aug 15, 2018 7:30:28 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.sql(\"\"\"CREATE TABLE flights_csv (  DEST_COUNTRY_NAME STRING,  \n                            ORIGIN_COUNTRY_NAME STRING COMMENT \"remember, the US will be most prevalent\",  \n                            count LONG)\n             USING csv OPTIONS (header true, path \u0027/tmp/2015-summary.csv\u0027)\"\"\")\n\n\nspark.sql(\"DESCRIBE TABLE flights_csv\").show()            \n\nspark.sql(\"select count(*) from flights_csv\").show()\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 7:30:27 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res23: org.apache.spark.sql.DataFrame \u003d []\n+-------------------+---------+--------------------+\n|           col_name|data_type|             comment|\n+-------------------+---------+--------------------+\n|  DEST_COUNTRY_NAME|   string|                null|\n|ORIGIN_COUNTRY_NAME|   string|remember, the US ...|\n|              count|   bigint|                null|\n+-------------------+---------+--------------------+\n\n+--------+\n|count(1)|\n+--------+\n|     256|\n+--------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534317894886_2014427928",
      "id": "20180815-072454_503973519",
      "dateCreated": "Aug 15, 2018 7:24:54 AM",
      "dateStarted": "Aug 15, 2018 7:30:27 AM",
      "dateFinished": "Aug 15, 2018 7:30:31 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// You can create table from select statement\n\nspark.sql(\"DROP TABLE IF EXISTS flights_from_select\")\nspark.sql(\"CREATE TABLE flights_from_select USING parquet AS SELECT * FROM flights\")\nspark.sql(\"DESCRIBE TABLE flights_from_select\").show()",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 7:33:54 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res37: org.apache.spark.sql.DataFrame \u003d []\nres38: org.apache.spark.sql.DataFrame \u003d []\n+-------------------+---------+-------+\n|           col_name|data_type|comment|\n+-------------------+---------+-------+\n|  DEST_COUNTRY_NAME|   string|   null|\n|ORIGIN_COUNTRY_NAME|   string|   null|\n|              count|   bigint|   null|\n+-------------------+---------+-------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534318155033_-759259186",
      "id": "20180815-072915_305711403",
      "dateCreated": "Aug 15, 2018 7:29:15 AM",
      "dateStarted": "Aug 15, 2018 7:33:54 AM",
      "dateFinished": "Aug 15, 2018 7:34:00 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// In addition, you can specify to create a table only if it does not currently exist:\n\nspark.sql(\"CREATE TABLE IF NOT EXISTS flights_from_select  AS SELECT * FROM flights\")",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 7:34:17 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res42: org.apache.spark.sql.DataFrame \u003d []\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534318333324_-51428479",
      "id": "20180815-073213_1097197132",
      "dateCreated": "Aug 15, 2018 7:32:13 AM",
      "dateStarted": "Aug 15, 2018 7:34:17 AM",
      "dateFinished": "Aug 15, 2018 7:34:18 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// you can control the layout of the data by writing out a partitioned dataset\n\nspark.sql(\"CREATE TABLE partitioned_flights USING parquet PARTITIONED BY (DEST_COUNTRY_NAME)AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 5\")\n\n// These tables will be available in Spark even through sessions; temporary tables do not currently exist in Spark. You must create a temporary view.",
      "user": "anonymous",
      "dateUpdated": "Aug 15, 2018 7:35:47 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res45: org.apache.spark.sql.DataFrame \u003d []\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534318457541_-718643845",
      "id": "20180815-073417_1653606136",
      "dateCreated": "Aug 15, 2018 7:34:17 AM",
      "dateStarted": "Aug 15, 2018 7:35:47 AM",
      "dateFinished": "Aug 15, 2018 7:35:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534318547126_1262583128",
      "id": "20180815-073547_1813769626",
      "dateCreated": "Aug 15, 2018 7:35:47 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "spark_sql",
  "id": "2DM6MKPNJ",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}