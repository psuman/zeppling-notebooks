{
  "paragraphs": [
    {
      "text": "%md\n\n### Stream Processing\n- Stream processing is the act of continuously incorporating new data to compute a result. \n- In stream processing, the input data is unbounded and has no predetermined beginning or end. \n- It simply forms a series of events that arrive at the stream processing system (e.g., credit card transactions, clicks on a website, or sensor read)\n- User applications can then compute various queries over this stream of events (e.g., tracking a running count of each type of event or aggregating them into hourly windows).\n- Batch processing also takes a query to compute, similar to stream processing, but only computes the result once.\n- Although streaming and batch processing sound different, in practice, they often need to work together.\n- For example, streaming applications often need to join input data against a dataset written periodically by a batch job, and the output of streaming jobs is often files or tables that are queried in batch jobs\n- Structured Streaming was designed from the beginning to interoperate easily with the rest of Spark, including batch applications. \n- Indeed, the Structured Streaming developers coined the term continuous applications to capture end-to-end applications that consist of streaming, batch, and interactive jobs all working on the same data to deliver an end product\n\n### Stream processing usecases\n- NOTIFICATIONS AND ALERTING\n- Real-time reporting\n\n- Incremental ETL\n- Update data to serve in real time\n- Real-time decision making\n- Online machine learning\n\n### Advantages of Stream Processing\n- stream processing enables lower latency: when your application needs to respond quickly (on a timescale of minutes, seconds, or milliseconds), you will need a streaming system that can keep state in memory to get acceptable performance.\n- stream processing can also be more efficient in updating a result than repeated batch jobs, because it automatically incrementalizes the computation\n\n### Challenges of Stream Processing\n- Processing out-of-order data based on application timestamp\n- Maintaining large amounts of state\n- Supporting high-data throughputWhen using event-time, several issues become common concerns across applications, including tracking state in a manner that allows the system to incorporate late events, and determining when it is safe to output a result for a given time window in event time \n- Processing each event exactly once despite machine failures\n- Handling load imbalance and stragglers\n- Joining with external data in other storage systems\n- Writing data transactionally to output systems\n\n### Stream Processing Design Points\n\n#### Record-at-a-Time Versus Declarative APIs\n- just pass each event to the application and let it react using custom code.\n- Apache Storm, implemented record at a time, and it has an important place when applications need full control over the processing of data\n- downside of these systems is that most of the complicating factors we described earlier, such as maintaining state, are solely governed by the application.\n- For example, with a record-at-a-time API, you are responsible for tracking state over longer time periods, dropping it after some time to clear up space, and responding differently to duplicate events after a failure.\n- In declarative APIs,  your application specifies what to compute but not how to compute it in response to each new event and how to recover from failure. \n- Spark Streaming tracks how much data each operator had processed, saved any relevant state reliably, and recovered the computation from failure when needed.\n- Structured streaming allow SQL like operations apart from functional operations over stream\n\n####  Event Time vs Processing Time\n- Event time is the idea of processing data based on timestamps inserted into each record at the source, as opposed to the time when the record is received at the streaming application \n- In particular, when using event time, records may arrive to the system out of order \n- If your application collects data from remote sources that may be delayed, such as mobile phones or IoT devices, event-time processing is crucial: without it, you will miss important patterns when some data is late. \n- In contrast, if your application only processes local events (e.g., ones generated in the same datacenter), you may not need sophisticated event-time processing.\n- When using event-time, several issues become common concerns across applications, \n             - including tracking state in a manner that allows the system to incorporate late events\n             - determining when it is safe to output a result for a given time window in event time \n             - \n- many declarative systems, including Structured Streaming, have “native” support for event time integrated into all their APIs, so that these concerns can be handled automatically across your whole program\n\n#### Continuous Versus Micro-Batch Execution\n- In a continuous processing system, each of the nodes implementing map would read records one by one from an input source, compute its function on them, and send them to the appropriate reducer. The reducer would then update its state whenever it gets a new record. The key idea is that this happens on each individual record\n- Continuous processing has the advantage of offering the lowest possible latency when the total input rate is relatively low, because each node responds immediately to a new message. \n- However, continuous processing systems generally have lower maximum throughput, because they incur a significant amount of overhead per-record (e.g., calling the operating system to send a packet to a downstream node).\n\n![sv-image](https://user-images.githubusercontent.com/1182329/44578961-8f009f00-a7b2-11e8-8ce1-1046eb9b01e1.png)\n\n\n- Micro-batch systems wait to accumulate small batches of input data (say, 500 ms’ worth), then process each batch in parallel using a distributed collection of tasks, similar to the execution of a batch job in Spark. - - Micro-batch systems can often achieve high throughput per node because they leverage the same optimizations as batch systems (e.g., vectorized processing), and do not incur any extra per-record overhead.\n- Hwever, Micro-batch systems has higher base latency due to waiting to accumulate a micro-batch. \n- In practice, the streaming applications that are large-scale enough to need to distribute their computation tend to prioritize throughput, so Spark has traditionally implemented micro-batch processing.\n- Micro-batch systems can comfortably deliver latencies from 100 ms to a second, depending on the application. Within this regime, they will generally require fewer nodes to achieve the same throughput, and hence lower operational cost (including lower maintenance cost due to less frequent node failures). \n- For much lower latencies, you should consider a continuous processing system, or using a micro-batch system in conjunction with a fast serving layer to provide low-latency queries\n\n![sv-image](https://user-images.githubusercontent.com/1182329/44578861-4d6ff400-a7b2-11e8-9b7b-640e17baa387.png)\n\n\n### Spark’s Streaming APIs\n\n\n#### DStreams API\n\n- Spark Streaming and its DStreams API, one of the first APIs to enable stream processing using high-level functional operators like map and reduce.\n- Interactions with RDD code, such as joins with static data, are also natively supported in Spark Streaming.\n- Many companies use and operate Spark Streaming at scale in production today due to its high-level API interface.\n- Much like the Resilient Distributed Dataset (RDD) API, however, the DStreams API is based on relatively low-level operations on Java/Python objects that limit opportunities for higher-level optimization\n- API is purely based on processing time—to handle event-time operations, applications need to implement them on their own. \n- Finally, DStreams can only operate in a micro-batch fashion, and exposes the duration of micro-batches in some parts of its API, making it difficult to support alternative execution modes.\n\n#### Structured Streaming API\n\n- in 2016, the Spark project added Structured Streaming, a new streaming API built directly on DataFrames that supports both rich optimizations and significantly simpler integration with other DataFrame and Dataset code\n- stream processing framework built on the Spark SQL engine\n- Structured Streaming offers a superset of the majority of the functionality of DStreams, and will often perform better due to code generation and the Catalyst optimizer.\n- Structured Streaming ensures end-to-end, exactly-once processing as well as fault-tolerance through checkpointing and write-ahead logs.\n- Structured Streaming has native support for event time data (all of its the windowing operators automatically support it). \n- Structured Streaming does not use a separate API from # Structured Streaming Core Concepts: you simply write a normal DataFrame (or SQL) computation and launch it on a stream. \n- Structured Streaming will automatically update the result of this computation in an incremental fashion as data arrives\n- Structured Streaming can output data to standard sinks usable by Spark SQL, such as Parquet tables, making it easy to query your stream state from another Spark applications\n- In general, Structured Streaming is meant to be an easier-to-use and higher-performance evolution of Spark Streaming’s DStream API\n- Main idea behind Structured Streaming is to treat a stream of data as a table to which data is continuously appended. \n- The job then periodically checks for new input data, process it, updates some internal state located in a state store if needed, and updates its result.\n- A cornerstone of the API is that you should not have to change your query’s code when doing batch or stream processing.\n- You should have to specify only whether to run that query in a batch or streaming fashion. Internally, Structured Streaming will automatically figure out how to “incrementalize” your query\n\n![sv-image](https://user-images.githubusercontent.com/1182329/44588974-d9931300-a7d4-11e8-8f56-ece5d9935477.png)\n\n\n#### Structured Streaming Core Concepts\n\n##### Transformations and Actions\n\n- Structured Streaming maintains the same concept of transformations and actions\n- The transformations available in Structured Streaming are, with a few restrictions,triggers define when data is output—that is, when Structured Streaming should check for new input data and update its result. By default, Structured Streaming will look for new input records as soon as it has finished processing the last group of input data, giving the lowest latency possible for new results\n- The restrictions usually involve some types of queries that the engine cannot incrementalize yet, although some of the limitations are being lifted in new versions of Spark. \n- There is generally only one action available in Structured Streaming: that of starting a stream, which will then run continuously and output results.\n\n##### Input Sources\n- Structured Streaming supports several input sources for reading in a streaming fashion. As of Spark 2.2, the supported input sources are as follows:\n           - Kafka Source\n           - DFS like HDFS or s3\n           - Socket source for testing\n\n##### Sinks\n- Sinks specify the destination for the result set of that stream. \n- Sinks and the execution engine are also responsible for reliably tracking the exact progress of data processing. \n- The supported output sinks as of Spark 2.2: \n         - kafka\n         - any file format\n         - console\n         - memory\n         - foreach\n\n##### Output Modes\n- Need to define how we want Spark to write data to that sink. \n         - For instance, do we only want to append new information? \n         - Do we want to update rows as we receive more information about them over time (e.g., updating the click count for a given web page)? \n         - Do we want to completely overwrite the result set every single time \n         - \n- The supported output modes are as follows:\n              - Append (only add new records to the output sink)\n              - Update (update changed records in place)\n              - Complete (rewrite the full output)\n- One important detail is that certain queries, and certain sinks, only support certain output modes\n- For example, suppose that your job is just performing a map on a stream. The output data will grow indefinitely as new records arrive, so it would not make sense to use Complete mode, which requires writing all the data to a new file at once. \n- In contrast, if you are doing an aggregation into a limited number of keys, Complete and Update modes would make sense, but Append would not, because the values of some keys’ need to be updated over time.\n\n##### Triggers\n\n- Triggers define when data is output — that is, when Structured Streaming should check for new input data and update its result. \n- By default, Structured Streaming will look for new input records as soon as it has finished processing the last group of input data, giving the lowest latency possible for new results\n- Spark also supports triggers based on processing time (only look for new data at a fixed interval)\n\n##### Event-Time Processing\n- Structured Streaming also has support for event-time processing\n- Event-time means time fields that are embedded in your data. This means that rather than processing data according to the time it reaches your system, you process it according to the time that it was generated, even if records arrive out of order at the streaming application due to slow uploads or network delays.\n- Structured Streaming can take some special actions when it knows that one of your columns is an event-time field, including optimizing query execution or determining when it is safe to forget state about a time window\n\n##### Watermarks\n- Watermarks are a feature of streaming systems that allow you to specify how late they expect to see data in event time\n- For example, in an application that processes logs from mobile devices, one might expect logs to be up to 30 minutes late due to upload delays\n- Structured Streaming, usually allow setting watermarks to limit how long they need to remember old data.\n- Streaming DataFrames are largely the same as static DataFrames.\n- We create them within Spark applications and then perform transformations on them to get our data into the correct format. \n- Basically, all of the transformations that are available in the static Structured APIs apply to Streaming DataFrames.\n- However, one small difference is that Structured Streaming does not let you perform schema inference without explicitly enabling it. \n- You can enable schema inference for this by setting the configuration spark.sql.streaming.schemaInference to true\n",
      "user": "anonymous",
      "dateUpdated": "Aug 24, 2018 5:24:57 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eStream Processing\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eStream processing is the act of continuously incorporating new data to compute a result.\u003c/li\u003e\n\u003cli\u003eIn stream processing, the input data is unbounded and has no predetermined beginning or end.\u003c/li\u003e\n\u003cli\u003eIt simply forms a series of events that arrive at the stream processing system (e.g., credit card transactions, clicks on a website, or sensor read)\u003c/li\u003e\n\u003cli\u003eUser applications can then compute various queries over this stream of events (e.g., tracking a running count of each type of event or aggregating them into hourly windows).\u003c/li\u003e\n\u003cli\u003eBatch processing also takes a query to compute, similar to stream processing, but only computes the result once.\u003c/li\u003e\n\u003cli\u003eAlthough streaming and batch processing sound different, in practice, they often need to work together.\u003c/li\u003e\n\u003cli\u003eFor example, streaming applications often need to join input data against a dataset written periodically by a batch job, and the output of streaming jobs is often files or tables that are queried in batch jobs\u003c/li\u003e\n\u003cli\u003eStructured Streaming was designed from the beginning to interoperate easily with the rest of Spark, including batch applications.\u003c/li\u003e\n\u003cli\u003eIndeed, the Structured Streaming developers coined the term continuous applications to capture end-to-end applications that consist of streaming, batch, and interactive jobs all working on the same data to deliver an end product\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eStream processing usecases\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eNOTIFICATIONS AND ALERTING\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eReal-time reporting\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIncremental ETL\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eUpdate data to serve in real time\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eReal-time decision making\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eOnline machine learning\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAdvantages of Stream Processing\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003estream processing enables lower latency: when your application needs to respond quickly (on a timescale of minutes, seconds, or milliseconds), you will need a streaming system that can keep state in memory to get acceptable performance.\u003c/li\u003e\n\u003cli\u003estream processing can also be more efficient in updating a result than repeated batch jobs, because it automatically incrementalizes the computation\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eChallenges of Stream Processing\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eProcessing out-of-order data based on application timestamp\u003c/li\u003e\n\u003cli\u003eMaintaining large amounts of state\u003c/li\u003e\n\u003cli\u003eSupporting high-data throughputWhen using event-time, several issues become common concerns across applications, including tracking state in a manner that allows the system to incorporate late events, and determining when it is safe to output a result for a given time window in event time\u003c/li\u003e\n\u003cli\u003eProcessing each event exactly once despite machine failures\u003c/li\u003e\n\u003cli\u003eHandling load imbalance and stragglers\u003c/li\u003e\n\u003cli\u003eJoining with external data in other storage systems\u003c/li\u003e\n\u003cli\u003eWriting data transactionally to output systems\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eStream Processing Design Points\u003c/h3\u003e\n\u003ch4\u003eRecord-at-a-Time Versus Declarative APIs\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003ejust pass each event to the application and let it react using custom code.\u003c/li\u003e\n\u003cli\u003eApache Storm, implemented record at a time, and it has an important place when applications need full control over the processing of data\u003c/li\u003e\n\u003cli\u003edownside of these systems is that most of the complicating factors we described earlier, such as maintaining state, are solely governed by the application.\u003c/li\u003e\n\u003cli\u003eFor example, with a record-at-a-time API, you are responsible for tracking state over longer time periods, dropping it after some time to clear up space, and responding differently to duplicate events after a failure.\u003c/li\u003e\n\u003cli\u003eIn declarative APIs,  your application specifies what to compute but not how to compute it in response to each new event and how to recover from failure.\u003c/li\u003e\n\u003cli\u003eSpark Streaming tracks how much data each operator had processed, saved any relevant state reliably, and recovered the computation from failure when needed.\u003c/li\u003e\n\u003cli\u003eStructured streaming allow SQL like operations apart from functional operations over stream\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eEvent Time vs Processing Time\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eEvent time is the idea of processing data based on timestamps inserted into each record at the source, as opposed to the time when the record is received at the streaming application\u003c/li\u003e\n\u003cli\u003eIn particular, when using event time, records may arrive to the system out of order\u003c/li\u003e\n\u003cli\u003eIf your application collects data from remote sources that may be delayed, such as mobile phones or IoT devices, event-time processing is crucial: without it, you will miss important patterns when some data is late.\u003c/li\u003e\n\u003cli\u003eIn contrast, if your application only processes local events (e.g., ones generated in the same datacenter), you may not need sophisticated event-time processing.\u003c/li\u003e\n\u003cli\u003eWhen using event-time, several issues become common concerns across applications,\u003cpre\u003e\u003ccode\u003e     - including tracking state in a manner that allows the system to incorporate late events\n     - determining when it is safe to output a result for a given time window in event time \n     - \n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003emany declarative systems, including Structured Streaming, have “native” support for event time integrated into all their APIs, so that these concerns can be handled automatically across your whole program\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eContinuous Versus Micro-Batch Execution\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eIn a continuous processing system, each of the nodes implementing map would read records one by one from an input source, compute its function on them, and send them to the appropriate reducer. The reducer would then update its state whenever it gets a new record. The key idea is that this happens on each individual record\u003c/li\u003e\n\u003cli\u003eContinuous processing has the advantage of offering the lowest possible latency when the total input rate is relatively low, because each node responds immediately to a new message.\u003c/li\u003e\n\u003cli\u003eHowever, continuous processing systems generally have lower maximum throughput, because they incur a significant amount of overhead per-record (e.g., calling the operating system to send a packet to a downstream node).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/44578961-8f009f00-a7b2-11e8-8ce1-1046eb9b01e1.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMicro-batch systems wait to accumulate small batches of input data (say, 500 ms’ worth), then process each batch in parallel using a distributed collection of tasks, similar to the execution of a batch job in Spark. - - Micro-batch systems can often achieve high throughput per node because they leverage the same optimizations as batch systems (e.g., vectorized processing), and do not incur any extra per-record overhead.\u003c/li\u003e\n\u003cli\u003eHwever, Micro-batch systems has higher base latency due to waiting to accumulate a micro-batch.\u003c/li\u003e\n\u003cli\u003eIn practice, the streaming applications that are large-scale enough to need to distribute their computation tend to prioritize throughput, so Spark has traditionally implemented micro-batch processing.\u003c/li\u003e\n\u003cli\u003eMicro-batch systems can comfortably deliver latencies from 100 ms to a second, depending on the application. Within this regime, they will generally require fewer nodes to achieve the same throughput, and hence lower operational cost (including lower maintenance cost due to less frequent node failures).\u003c/li\u003e\n\u003cli\u003eFor much lower latencies, you should consider a continuous processing system, or using a micro-batch system in conjunction with a fast serving layer to provide low-latency queries\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/44578861-4d6ff400-a7b2-11e8-9b7b-640e17baa387.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch3\u003eSpark’s Streaming APIs\u003c/h3\u003e\n\u003ch4\u003eDStreams API\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eSpark Streaming and its DStreams API, one of the first APIs to enable stream processing using high-level functional operators like map and reduce.\u003c/li\u003e\n\u003cli\u003eInteractions with RDD code, such as joins with static data, are also natively supported in Spark Streaming.\u003c/li\u003e\n\u003cli\u003eMany companies use and operate Spark Streaming at scale in production today due to its high-level API interface.\u003c/li\u003e\n\u003cli\u003eMuch like the Resilient Distributed Dataset (RDD) API, however, the DStreams API is based on relatively low-level operations on Java/Python objects that limit opportunities for higher-level optimization\u003c/li\u003e\n\u003cli\u003eAPI is purely based on processing time—to handle event-time operations, applications need to implement them on their own.\u003c/li\u003e\n\u003cli\u003eFinally, DStreams can only operate in a micro-batch fashion, and exposes the duration of micro-batches in some parts of its API, making it difficult to support alternative execution modes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eStructured Streaming API\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003ein 2016, the Spark project added Structured Streaming, a new streaming API built directly on DataFrames that supports both rich optimizations and significantly simpler integration with other DataFrame and Dataset code\u003c/li\u003e\n\u003cli\u003estream processing framework built on the Spark SQL engine\u003c/li\u003e\n\u003cli\u003eStructured Streaming offers a superset of the majority of the functionality of DStreams, and will often perform better due to code generation and the Catalyst optimizer.\u003c/li\u003e\n\u003cli\u003eStructured Streaming ensures end-to-end, exactly-once processing as well as fault-tolerance through checkpointing and write-ahead logs.\u003c/li\u003e\n\u003cli\u003eStructured Streaming has native support for event time data (all of its the windowing operators automatically support it).\u003c/li\u003e\n\u003cli\u003eStructured Streaming does not use a separate API from # Structured Streaming Core Concepts: you simply write a normal DataFrame (or SQL) computation and launch it on a stream.\u003c/li\u003e\n\u003cli\u003eStructured Streaming will automatically update the result of this computation in an incremental fashion as data arrives\u003c/li\u003e\n\u003cli\u003eStructured Streaming can output data to standard sinks usable by Spark SQL, such as Parquet tables, making it easy to query your stream state from another Spark applications\u003c/li\u003e\n\u003cli\u003eIn general, Structured Streaming is meant to be an easier-to-use and higher-performance evolution of Spark Streaming’s DStream API\u003c/li\u003e\n\u003cli\u003eMain idea behind Structured Streaming is to treat a stream of data as a table to which data is continuously appended.\u003c/li\u003e\n\u003cli\u003eThe job then periodically checks for new input data, process it, updates some internal state located in a state store if needed, and updates its result.\u003c/li\u003e\n\u003cli\u003eA cornerstone of the API is that you should not have to change your query’s code when doing batch or stream processing.\u003c/li\u003e\n\u003cli\u003eYou should have to specify only whether to run that query in a batch or streaming fashion. Internally, Structured Streaming will automatically figure out how to “incrementalize” your query\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/44588974-d9931300-a7d4-11e8-8f56-ece5d9935477.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch4\u003eStructured Streaming Core Concepts\u003c/h4\u003e\n\u003ch5\u003eTransformations and Actions\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eStructured Streaming maintains the same concept of transformations and actions\u003c/li\u003e\n\u003cli\u003eThe transformations available in Structured Streaming are, with a few restrictions,triggers define when data is output—that is, when Structured Streaming should check for new input data and update its result. By default, Structured Streaming will look for new input records as soon as it has finished processing the last group of input data, giving the lowest latency possible for new results\u003c/li\u003e\n\u003cli\u003eThe restrictions usually involve some types of queries that the engine cannot incrementalize yet, although some of the limitations are being lifted in new versions of Spark.\u003c/li\u003e\n\u003cli\u003eThere is generally only one action available in Structured Streaming: that of starting a stream, which will then run continuously and output results.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eInput Sources\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eStructured Streaming supports several input sources for reading in a streaming fashion. As of Spark 2.2, the supported input sources are as follows:\u003cpre\u003e\u003ccode\u003e   - Kafka Source\n   - DFS like HDFS or s3\n   - Socket source for testing\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eSinks\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eSinks specify the destination for the result set of that stream.\u003c/li\u003e\n\u003cli\u003eSinks and the execution engine are also responsible for reliably tracking the exact progress of data processing.\u003c/li\u003e\n\u003cli\u003eThe supported output sinks as of Spark 2.2:\u003cpre\u003e\u003ccode\u003e - kafka\n - any file format\n - console\n - memory\n - foreach\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eOutput Modes\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eNeed to define how we want Spark to write data to that sink.\u003cpre\u003e\u003ccode\u003e - For instance, do we only want to append new information? \n - Do we want to update rows as we receive more information about them over time (e.g., updating the click count for a given web page)? \n - Do we want to completely overwrite the result set every single time \n - \n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003eThe supported output modes are as follows:\u003cpre\u003e\u003ccode\u003e      - Append (only add new records to the output sink)\n      - Update (update changed records in place)\n      - Complete (rewrite the full output)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003eOne important detail is that certain queries, and certain sinks, only support certain output modes\u003c/li\u003e\n\u003cli\u003eFor example, suppose that your job is just performing a map on a stream. The output data will grow indefinitely as new records arrive, so it would not make sense to use Complete mode, which requires writing all the data to a new file at once.\u003c/li\u003e\n\u003cli\u003eIn contrast, if you are doing an aggregation into a limited number of keys, Complete and Update modes would make sense, but Append would not, because the values of some keys’ need to be updated over time.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eTriggers\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eTriggers define when data is output — that is, when Structured Streaming should check for new input data and update its result.\u003c/li\u003e\n\u003cli\u003eBy default, Structured Streaming will look for new input records as soon as it has finished processing the last group of input data, giving the lowest latency possible for new results\u003c/li\u003e\n\u003cli\u003eSpark also supports triggers based on processing time (only look for new data at a fixed interval)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eEvent-Time Processing\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eStructured Streaming also has support for event-time processing\u003c/li\u003e\n\u003cli\u003eEvent-time means time fields that are embedded in your data. This means that rather than processing data according to the time it reaches your system, you process it according to the time that it was generated, even if records arrive out of order at the streaming application due to slow uploads or network delays.\u003c/li\u003e\n\u003cli\u003eStructured Streaming can take some special actions when it knows that one of your columns is an event-time field, including optimizing query execution or determining when it is safe to forget state about a time window\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eWatermarks\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eWatermarks are a feature of streaming systems that allow you to specify how late they expect to see data in event time\u003c/li\u003e\n\u003cli\u003eFor example, in an application that processes logs from mobile devices, one might expect logs to be up to 30 minutes late due to upload delays\u003c/li\u003e\n\u003cli\u003eStructured Streaming, usually allow setting watermarks to limit how long they need to remember old data.\u003c/li\u003e\n\u003cli\u003eStreaming DataFrames are largely the same as static DataFrames.\u003c/li\u003e\n\u003cli\u003eWe create them within Spark applications and then perform transformations on them to get our data into the correct format.\u003c/li\u003e\n\u003cli\u003eBasically, all of the transformations that are available in the static Structured APIs apply to Streaming DataFrames.\u003c/li\u003e\n\u003cli\u003eHowever, one small difference is that Structured Streaming does not let you perform schema inference without explicitly enabling it.\u003c/li\u003e\n\u003cli\u003eYou can enable schema inference for this by setting the configuration spark.sql.streaming.schemaInference to true\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1535102804528_343963730",
      "id": "20180824-092644_37209853",
      "dateCreated": "Aug 24, 2018 9:26:44 AM",
      "dateStarted": "Aug 24, 2018 5:24:57 PM",
      "dateFinished": "Aug 24, 2018 5:24:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n\nif [ -d /tmp/activity-data ]\nthen\n  rm -fr /tmp/activity-data\nelse\n  mkdir /tmp/activity-data\nfi\n\nhdfs dfs -rm -r /tmp/activity-data\n\nhdfs dfs -mkdir /tmp/activity-data\n\n\nwget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json -O /tmp/activity-data_1.json\n\nwget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/activity-data/part-00001-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json -O /tmp/activity-data_2.json\n\n\n# Move downloaded JSON file from local storage to HDFS\nhdfs dfs -put /tmp/activity-data_1.json /tmp/activity-data/activity-data_1.json\n\nhdfs dfs -put /tmp/activity-data_2.json /tmp/activity-data/activity-data_2.json\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 26, 2018 5:18:12 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "18/08/26 17:18:15 INFO fs.TrashPolicyDefault: Moved: \u0027hdfs://sandbox-hdp.hortonworks.com:8020/tmp/activity-data\u0027 to trash at: hdfs://sandbox-hdp.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/activity-data1535303895246\n--2018-08-26 17:18:17--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.128.133, 151.101.64.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14831354 (14M) [text/plain]\nSaving to: ‘/tmp/activity-data_1.json’\n\n     0K .......... .......... .......... .......... ..........  0%  134K 1m48s\n    50K .......... .......... .......... .......... ..........  0%  364K 74s\n   100K .......... .......... .......... .......... ..........  1%  889K 54s\n   150K .......... .......... .......... .......... ..........  1%  589K 47s\n   200K .......... .......... .......... .......... ..........  1% 1.31M 39s\n   250K .......... .......... .......... .......... ..........  2% 1.74M 34s\n   300K .......... .......... .......... .......... ..........  2% 1.92M 30s\n   350K .......... .......... .......... .......... ..........  2%  727K 29s\n   400K .......... .......... .......... .......... ..........  3% 2.59M 26s\n   450K .......... .......... .......... .......... ..........  3% 2.96M 24s\n   500K .......... .......... .......... .......... ..........  3% 3.08M 22s\n   550K .......... .......... .......... .......... ..........  4% 1.97M 21s\n   600K .......... .......... .......... .......... ..........  4% 9.40M 19s\n   650K .......... .......... .......... .......... ..........  4% 3.90M 18s\n   700K .......... .......... .......... .......... ..........  5%  791K 18s\n   750K .......... .......... .......... .......... ..........  5% 4.09M 17s\n   800K .......... .......... .......... .......... ..........  5% 4.71M 16s\n   850K .......... .......... .......... .......... ..........  6% 11.2M 15s\n   900K .......... .......... .......... .......... ..........  6%  135K 19s\n   950K .......... .......... .......... .......... ..........  6%  300M 18s\n  1000K .......... .......... .......... .......... ..........  7%  448M 17s\n  1050K .......... .......... .......... .......... ..........  7%  333M 17s\n  1100K .......... .......... .......... .......... ..........  7%  309M 16s\n  1150K .......... .......... .......... .......... ..........  8%  242M 15s\n  1200K .......... .......... .......... .......... ..........  8%  299M 14s\n  1250K .......... .......... .......... .......... ..........  8%  288M 14s\n  1300K .......... .......... .......... .......... ..........  9%  274M 13s\n  1350K .......... .......... .......... .......... ..........  9% 3.15M 13s\n  1400K .......... .......... .......... .......... .......... 10%  429M 12s\n  1450K .......... .......... .......... .......... .......... 10% 73.1M 12s\n  1500K .......... .......... .......... .......... .......... 10% 1.02M 12s\n  1550K .......... .......... .......... .......... .......... 11%  842K 12s\n  1600K .......... .......... .......... .......... .......... 11% 6.51M 12s\n  1650K .......... .......... .......... .......... .......... 11% 17.8M 11s\n  1700K .......... .......... .......... .......... .......... 12% 2.79M 11s\n  1750K .......... .......... .......... .......... .......... 12% 6.71M 11s\n  1800K .......... .......... .......... .......... .......... 12% 4.47M 10s\n  1850K .......... .......... .......... .......... .......... 13% 3.33M 10s\n  1900K .......... .......... .......... .......... .......... 13%  523K 11s\n  1950K .......... .......... .......... .......... .......... 13% 2.71M 10s\n  2000K .......... .......... .......... .......... .......... 14% 3.82M 10s\n  2050K .......... .......... .......... .......... .......... 14% 3.69M 10s\n  2100K .......... .......... .......... .......... .......... 14% 4.70M 10s\n  2150K .......... .......... .......... .......... .......... 15% 4.61M 10s\n  2200K .......... .......... .......... .......... .......... 15% 4.46M 9s\n  2250K .......... .......... .......... .......... .......... 15%  269K 10s\n  2300K .......... .......... .......... .......... .......... 16% 3.41M 10s\n  2350K .......... .......... .......... .......... .......... 16% 1.40M 10s\n  2400K .......... .......... .......... .......... .......... 16% 4.08M 10s\n  2450K .......... .......... .......... .......... .......... 17% 3.17M 10s\n  2500K .......... .......... .......... .......... .......... 17% 2.94M 9s\n  2550K .......... .......... .......... .......... .......... 17% 12.3M 9s\n  2600K .......... .......... .......... .......... .......... 18% 3.29M 9s\n  2650K .......... .......... .......... .......... .......... 18% 7.53M 9s\n  2700K .......... .......... .......... .......... .......... 18% 4.03M 9s\n  2750K .......... .......... .......... .......... .......... 19% 2.44M 9s\n  2800K .......... .......... .......... .......... .......... 19% 5.36M 8s\n  2850K .......... .......... .......... .......... .......... 20%  246K 9s\n  2900K .......... .......... .......... .......... .......... 20% 5.13M 9s\n  2950K .......... .......... .......... .......... .......... 20% 97.0M 9s\n  3000K .......... .......... .......... .......... .......... 21% 2.80M 9s\n  3050K .......... .......... .......... .......... .......... 21% 4.11M 8s\n  3100K .......... .......... .......... .......... .......... 21% 2.57M 8s\n  3150K .......... .......... .......... .......... .......... 22% 2.05M 8s\n  3200K .......... .......... .......... .......... .......... 22% 5.57M 8s\n  3250K .......... .......... .......... .......... .......... 22% 2.88M 8s\n  3300K .......... .......... .......... .......... .......... 23% 2.35M 8s\n  3350K .......... .......... .......... .......... .......... 23% 5.72M 8s\n  3400K .......... .......... .......... .......... .......... 23%  106M 8s\n  3450K .......... .......... .......... .......... .......... 24% 74.7M 8s\n  3500K .......... .......... .......... .......... .......... 24% 3.14M 7s\n  3550K .......... .......... .......... .......... .......... 24% 1.61M 7s\n  3600K .......... .......... .......... .......... .......... 25%  269K 8s\n  3650K .......... .......... .......... .......... .......... 25% 2.89M 8s\n  3700K .......... .......... .......... .......... .......... 25%  158M 8s\n  3750K .......... .......... .......... .......... .......... 26%  138M 7s\n  3800K .......... .......... .......... .......... .......... 26%  156M 7s\n  3850K .......... .......... .......... .......... .......... 26%  142M 7s\n  3900K .......... .......... .......... .......... .......... 27%  150M 7s\n  3950K .......... .......... .......... .......... .......... 27% 3.25M 7s\n  4000K .......... .......... .......... .......... .......... 27% 1.30M 7s\n  4050K .......... .......... .......... .......... .......... 28% 1.02M 7s\n  4100K .......... .......... .......... .......... .......... 28% 1.88M 7s\n  4150K .......... .......... .......... .......... .......... 28% 1.47M 7s\n  4200K .......... .......... .......... .......... .......... 29% 1.52M 7s\n  4250K .......... .......... .......... .......... .......... 29% 1.31M 7s\n  4300K .......... .......... .......... .......... .......... 30% 1.17M 7s\n  4350K .......... .......... .......... .......... .......... 30%  282K 7s\n  4400K .......... .......... .......... .......... .......... 30%  833K 7s\n  4450K .......... .......... .......... .......... .......... 31% 1.76M 7s\n  4500K .......... .......... .......... .......... .......... 31% 3.48M 7s\n  4550K .......... .......... .......... .......... .......... 31% 2.77M 7s\n  4600K .......... .......... .......... .......... .......... 32%  763K 7s\n  4650K .......... .......... .......... .......... .......... 32%  846K 7s\n  4700K .......... .......... .......... .......... .......... 32%  992K 7s\n  4750K .......... .......... .......... .......... .......... 33%  933K 7s\n  4800K .......... .......... .......... .......... .......... 33% 1.66M 7s\n  4850K .......... .......... .......... .......... .......... 33% 1.66M 7s\n  4900K .......... .......... .......... .......... .......... 34% 1.47M 7s\n  4950K .......... .......... .......... .......... .......... 34% 1.48M 7s\n  5000K .......... .......... .......... .......... .......... 34% 1.41M 7s\n  5050K .......... .......... .......... .......... .......... 35% 1.64M 7s\n  5100K .......... .......... .......... .......... .......... 35% 1.72M 7s\n  5150K .......... .......... .......... .......... .......... 35% 1.09M 7s\n  5200K .......... .......... .......... .......... .......... 36% 1.84M 7s\n  5250K .......... .......... .......... .......... .......... 36% 1.44M 7s\n  5300K .......... .......... .......... .......... .......... 36% 1.53M 7s\n  5350K .......... .......... .......... .......... .......... 37% 1.63M 6s\n  5400K .......... .......... .......... .......... .......... 37% 1.52M 6s\n  5450K .......... .......... .......... .......... .......... 37% 1.44M 6s\n  5500K .......... .......... .......... .......... .......... 38% 1.64M 6s\n  5550K .......... .......... .......... .......... .......... 38% 1.28M 6s\n  5600K .......... .......... .......... .......... .......... 39% 1.63M 6s\n  5650K .......... .......... .......... .......... .......... 39% 1.49M 6s\n  5700K .......... .......... .......... .......... .......... 39% 1.59M 6s\n  5750K .......... .......... .......... .......... .......... 40% 1.71M 6s\n  5800K .......... .......... .......... .......... .......... 40% 1.59M 6s\n  5850K .......... .......... .......... .......... .......... 40% 1.82M 6s\n  5900K .......... .......... .......... .......... .......... 41% 1.42M 6s\n  5950K .......... .......... .......... .......... .......... 41% 1.31M 6s\n  6000K .......... .......... .......... .......... .......... 41% 1.52M 6s\n  6050K .......... .......... .......... .......... .......... 42% 1.65M 6s\n  6100K .......... .......... .......... .......... .......... 42% 1.80M 6s\n  6150K .......... .......... .......... .......... .......... 42% 1.52M 6s\n  6200K .......... .......... .......... .......... .......... 43% 1.56M 6s\n  6250K .......... .......... .......... .......... .......... 43% 1.86M 6s\n  6300K .......... .......... .......... .......... .......... 43% 1.59M 6s\n  6350K .......... .......... .......... .......... .......... 44% 1.20M 6s\n  6400K .......... .......... .......... .......... .......... 44% 1.66M 6s\n  6450K .......... .......... .......... .......... .......... 44% 1.52M 6s\n  6500K .......... .......... .......... .......... .......... 45% 2.01M 6s\n  6550K .......... .......... .......... .......... .......... 45% 1.64M 5s\n  6600K .......... .......... .......... .......... .......... 45% 1.58M 5s\n  6650K .......... .......... .......... .......... .......... 46% 1.71M 5s\n  6700K .......... .......... .......... .......... .......... 46% 1.55M 5s\n  6750K .......... .......... .......... .......... .......... 46% 1.35M 5s\n  6800K .......... .......... .......... .......... .......... 47% 1.42M 5s\n  6850K .......... .......... .......... .......... .......... 47% 2.10M 5s\n  6900K .......... .......... .......... .......... .......... 47% 1.58M 5s\n  6950K .......... .......... .......... .......... .......... 48% 1.43M 5s\n  7000K .......... .......... .......... .......... .......... 48% 1.94M 5s\n  7050K .......... .......... .......... .......... .......... 49% 1.63M 5s\n  7100K .......... .......... .......... .......... .......... 49% 1.80M 5s\n  7150K .......... .......... .......... .......... .......... 49% 1.21M 5s\n  7200K .......... .......... .......... .......... .......... 50%  311K 5s\n  7250K .......... .......... .......... .......... .......... 50% 4.29M 5s\n  7300K .......... .......... .......... .......... .......... 50% 4.09M 5s\n  7350K .......... .......... .......... .......... .......... 51% 6.11M 5s\n  7400K .......... .......... .......... .......... .......... 51% 4.59M 5s\n  7450K .......... .......... .......... .......... .......... 51% 2.46M 5s\n  7500K .......... .......... .......... .......... .......... 52%  194K 5s\n  7550K .......... .......... .......... .......... .......... 52% 50.3K 6s\n  7600K .......... .......... .......... .......... .......... 52% 2.73M 6s\n  7650K .......... .......... .......... .......... .......... 53% 3.49M 6s\n  7700K .......... .......... .......... .......... .......... 53% 1.15M 6s\n  7750K .......... .......... .......... .......... .......... 53% 2.70M 6s\n  7800K .......... .......... .......... .......... .......... 54%  202K 6s\n  7850K .......... .......... .......... .......... .......... 54%  466K 6s\n  7900K .......... .......... .......... .......... .......... 54% 1.06M 6s\n  7950K .......... .......... .......... .......... .......... 55%  935K 6s\n  8000K .......... .......... .......... .......... .......... 55% 1.58M 6s\n  8050K .......... .......... .......... .......... .......... 55% 1.79M 6s\n  8100K .......... .......... .......... .......... .......... 56% 1.43M 6s\n  8150K .......... .......... .......... .......... .......... 56% 3.01M 5s\n  8200K .......... .......... .......... .......... .......... 56%  511K 5s\n  8250K .......... .......... .......... .......... .......... 57% 3.97M 5s\n  8300K .......... .......... .......... .......... .......... 57% 2.79M 5s\n  8350K .......... .......... .......... .......... .......... 57% 2.82M 5s\n  8400K .......... .......... .......... .......... .......... 58% 3.76M 5s\n  8450K .......... .......... .......... .......... .......... 58% 2.17M 5s\n  8500K .......... .......... .......... .......... .......... 59%  327K 5s\n  8550K .......... .......... .......... .......... .......... 59% 5.38M 5s\n  8600K .......... .......... .......... .......... .......... 59% 2.04M 5s\n  8650K .......... .......... .......... .......... .......... 60% 2.47M 5s\n  8700K .......... .......... .......... .......... .......... 60% 7.55M 5s\n  8750K .......... .......... .......... .......... .......... 60% 4.88M 5s\n  8800K .......... .......... .......... .......... .......... 61%  513K 5s\n  8850K .......... .......... .......... .......... .......... 61% 1.36M 5s\n  8900K .......... .......... .......... .......... .......... 61% 2.33M 5s\n  8950K .......... .......... .......... .......... .......... 62% 2.15M 5s\n  9000K .......... .......... .......... .......... .......... 62% 2.33M 5s\n  9050K .......... .......... .......... .......... .......... 62% 2.60M 5s\n  9100K .......... .......... .......... .......... .......... 63% 2.22M 5s\n  9150K .......... .......... .......... .......... .......... 63%  935K 4s\n  9200K .......... .......... .......... .......... .......... 63% 2.49M 4s\n  9250K .......... .......... .......... .......... .......... 64% 2.19M 4s\n  9300K .......... .......... .......... .......... .......... 64% 2.40M 4s\n  9350K .......... .......... .......... .......... .......... 64% 1.78M 4s\n  9400K .......... .......... .......... .......... .......... 65% 2.81M 4s\n  9450K .......... .......... .......... .......... .......... 65% 2.85M 4s\n  9500K .......... .......... .......... .......... .......... 65% 1.32M 4s\n  9550K .......... .......... .......... .......... .......... 66% 1.89M 4s\n  9600K .......... .......... .......... .......... .......... 66% 2.36M 4s\n  9650K .......... .......... .......... .......... .......... 66% 2.55M 4s\n  9700K .......... .......... .......... .......... .......... 67% 2.03M 4s\n  9750K .......... .......... .......... .......... .......... 67% 2.35M 4s\n  9800K .......... .......... .......... .......... .......... 68% 2.62M 4s\n  9850K .......... .......... .......... .......... .......... 68% 1.77M 4s\n  9900K .......... .......... .......... .......... .......... 68% 2.28M 4s\n  9950K .......... .......... .......... .......... .......... 69% 1.98M 4s\n 10000K .......... .......... .......... .......... .......... 69% 2.53M 4s\n 10050K .......... .......... .......... .......... .......... 69% 2.10M 4s\n 10100K .......... .......... .......... .......... .......... 70% 2.51M 4s\n 10150K .......... .......... .......... .......... .......... 70% 2.28M 3s\n 10200K .......... .......... .......... .......... .......... 70% 2.20M 3s\n 10250K .......... .......... .......... .......... .......... 71% 2.33M 3s\n 10300K .......... .......... .......... .......... .......... 71% 1.91M 3s\n 10350K .......... .......... .......... .......... .......... 71%  265K 3s\n 10400K .......... .......... .......... .......... .......... 72% 3.46M 3s\n 10450K .......... .......... .......... .......... .......... 72% 2.45M 3s\n 10500K .......... .......... .......... .......... .......... 72% 2.50M 3s\n 10550K .......... .......... .......... .......... .......... 73% 2.56M 3s\n 10600K .......... .......... .......... .......... .......... 73% 2.15M 3s\n 10650K .......... .......... .......... .......... .......... 73% 3.25M 3s\n 10700K .......... .......... .......... .......... .......... 74% 1.36M 3s\n 10750K .......... .......... .......... .......... .......... 74% 75.7M 3s\n 10800K .......... .......... .......... .......... .......... 74% 77.1M 3s\n 10850K .......... .......... .......... .......... .......... 75% 71.2M 3s\n 10900K .......... .......... .......... .......... .......... 75% 1.22M 3s\n 10950K .......... .......... .......... .......... .......... 75% 3.59M 3s\n 11000K .......... .......... .......... .......... .......... 76% 2.42M 3s\n 11050K .......... .......... .......... .......... .......... 76% 1.72M 3s\n 11100K .......... .......... .......... .......... .......... 76% 1.74M 3s\n 11150K .......... .......... .......... .......... .......... 77%  780K 3s\n 11200K .......... .......... .......... .......... .......... 77% 1.39M 3s\n 11250K .......... .......... .......... .......... .......... 78% 3.47M 3s\n 11300K .......... .......... .......... .......... .......... 78% 2.11M 2s\n 11350K .......... .......... .......... .......... .......... 78% 2.08M 2s\n 11400K .......... .......... .......... .......... .......... 79% 2.09M 2s\n 11450K .......... .......... .......... .......... .......... 79% 1.13M 2s\n 11500K .......... .......... .......... .......... .......... 79% 1.52M 2s\n 11550K .......... .......... .......... .......... .......... 80% 1.63M 2s\n 11600K .......... .......... .......... .......... .......... 80% 2.10M 2s\n 11650K .......... .......... .......... .......... .......... 80% 2.00M 2s\n 11700K .......... .......... .......... .......... .......... 81% 2.10M 2s\n 11750K .......... .......... .......... .......... .......... 81% 1.40M 2s\n 11800K .......... .......... .......... .......... .......... 81% 1.73M 2s\n 11850K .......... .......... .......... .......... .......... 82% 2.03M 2s\n 11900K .......... .......... .......... .......... .......... 82%  882K 2s\n 11950K .......... .......... .......... .......... .......... 82%  520K 2s\n 12000K .......... .......... .......... .......... .......... 83% 1.74M 2s\n 12050K .......... .......... .......... .......... .......... 83%  700K 2s\n 12100K .......... .......... .......... .......... .......... 83% 26.7M 2s\n 12150K .......... .......... .......... .......... .......... 84% 5.03M 2s\n 12200K .......... .......... .......... .......... .......... 84% 8.65M 2s\n 12250K .......... .......... .......... .......... .......... 84%  669K 2s\n 12300K .......... .......... .......... .......... .......... 85% 1.31M 2s\n 12350K .......... .......... .......... .......... .......... 85%  919K 2s\n 12400K .......... .......... .......... .......... .......... 85% 2.21M 2s\n 12450K .......... .......... .......... .......... .......... 86% 1.86M 2s\n 12500K .......... .......... .......... .......... .......... 86% 2.20M 1s\n 12550K .......... .......... .......... .......... .......... 86% 2.30M 1s\n 12600K .......... .......... .......... .......... .......... 87% 2.38M 1s\n 12650K .......... .......... .......... .......... .......... 87% 1013K 1s\n 12700K .......... .......... .......... .......... .......... 88% 1.16M 1s\n 12750K .......... .......... .......... .......... .......... 88%  880K 1s\n 12800K .......... .......... .......... .......... .......... 88% 1.29M 1s\n 12850K .......... .......... .......... .......... .......... 89% 2.76M 1s\n 12900K .......... .......... .......... .......... .......... 89% 3.40M 1s\n 12950K .......... .......... .......... .......... .......... 89%  274K 1s\n 13000K .......... .......... .......... .......... .......... 90%  213M 1s\n 13050K .......... .......... .......... .......... .......... 90%  223M 1s\n 13100K .......... .......... .......... .......... .......... 90%  221M 1s\n 13150K .......... .......... .......... .......... .......... 91%  158M 1s\n 13200K .......... .......... .......... .......... .......... 91%  172M 1s\n 13250K .......... .......... .......... .......... .......... 91% 1.16M 1s\n 13300K .......... .......... .......... .......... .......... 92%  689K 1s\n 13350K .......... .......... .......... .......... .......... 92% 2.11M 1s\n 13400K .......... .......... .......... .......... .......... 92% 2.10M 1s\n 13450K .......... .......... .......... .......... .......... 93% 1.50M 1s\n 13500K .......... .......... .......... .......... .......... 93% 1.69M 1s\n 13550K .......... .......... .......... .......... .......... 93%  794K 1s\n 13600K .......... .......... .......... .......... .......... 94% 1.78M 1s\n 13650K .......... .......... .......... .......... .......... 94% 1.77M 1s\n 13700K .......... .......... .......... .......... .......... 94% 1.61M 1s\n 13750K .......... .......... .......... .......... .......... 95% 1.89M 1s\n 13800K .......... .......... .......... .......... .......... 95% 1.03M 0s\n 13850K .......... .......... .......... .......... .......... 95% 1.74M 0s\n 13900K .......... .......... .......... .......... .......... 96% 1.67M 0s\n 13950K .......... .......... .......... .......... .......... 96%  773K 0s\n 14000K .......... .......... .......... .......... .......... 97% 1.77M 0s\n 14050K .......... .......... .......... .......... .......... 97% 1.85M 0s\n 14100K .......... .......... .......... .......... .......... 97% 1.64M 0s\n 14150K .......... .......... .......... .......... .......... 98% 2.02M 0s\n 14200K .......... .......... .......... .......... .......... 98% 1.05M 0s\n 14250K .......... .......... .......... .......... .......... 98% 1.82M 0s\n 14300K .......... .......... .......... .......... .......... 99% 1.82M 0s\n 14350K .......... .......... .......... .......... .......... 99% 1.37M 0s\n 14400K .......... .......... .......... .......... .......... 99% 1.85M 0s\n 14450K .......... .......... .......... ...                  100%  335K\u003d11s\n\n2018-08-26 17:18:30 (1.27 MB/s) - ‘/tmp/activity-data_1.json’ saved [14831354/14831354]\n\n--2018-08-26 17:18:30--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/activity-data/part-00001-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.128.133, 151.101.64.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14831559 (14M) [text/plain]\nSaving to: ‘/tmp/activity-data_2.json’\n\n     0K .......... .......... .......... .......... ..........  0%  129K 1m52s\n    50K .......... .......... .......... .......... ..........  0%  326K 78s\n   100K .......... .......... .......... .......... ..........  1%  586K 60s\n   150K .......... .......... .......... .......... ..........  1%  637K 50s\n   200K .......... .......... .......... .......... ..........  1%  602K 45s\n   250K .......... .......... .......... .......... ..........  2%  229K 48s\n   300K .......... .......... .......... .......... ..........  2%  340K 47s\n   350K .......... .......... .......... .......... ..........  2%  549K 44s\n   400K .......... .......... .......... .......... ..........  3% 1.71M 40s\n   450K .......... .......... .......... .......... ..........  3% 2.64M 36s\n   500K .......... .......... .......... .......... ..........  3% 2.30M 33s\n   550K .......... .......... .......... .......... ..........  4%  363K 34s\n   600K .......... .......... .......... .......... ..........  4% 7.72M 31s\n   650K .......... .......... .......... .......... ..........  4% 2.19M 29s\n   700K .......... .......... .......... .......... ..........  5% 1.61M 28s\n   750K .......... .......... .......... .......... ..........  5% 1.10M 27s\n   800K .......... .......... .......... .......... ..........  5%  595K 26s\n   850K .......... .......... .......... .......... ..........  6% 1.66M 25s\n   900K .......... .......... .......... .......... ..........  6% 1.54M 24s\n   950K .......... .......... .......... .......... ..........  6% 1.72M 23s\n  1000K .......... .......... .......... .......... ..........  7% 1.08M 23s\n  1050K .......... .......... .......... .......... ..........  7% 1.70M 22s\n  1100K .......... .......... .......... .......... ..........  7% 1.51M 21s\n  1150K .......... .......... .......... .......... ..........  8%  354K 22s\n  1200K .......... .......... .......... .......... ..........  8% 1000K 21s\n  1250K .......... .......... .......... .......... ..........  8% 3.41M 21s\n  1300K .......... .......... .......... .......... ..........  9% 3.19M 20s\n  1350K .......... .......... .......... .......... ..........  9% 1.16M 20s\n  1400K .......... .......... .......... .......... .......... 10%  924K 19s\n  1450K .......... .......... .......... .......... .......... 10%  885K 19s\n  1500K .......... .......... .......... .......... .......... 10% 1.11M 19s\n  1550K .......... .......... .......... .......... .......... 11% 1.10M 18s\n  1600K .......... .......... .......... .......... .......... 11% 1.58M 18s\n  1650K .......... .......... .......... .......... .......... 11% 1.68M 18s\n  1700K .......... .......... .......... .......... .......... 12% 1.39M 17s\n  1750K .......... .......... .......... .......... .......... 12% 1.24M 17s\n  1800K .......... .......... .......... .......... .......... 12% 1.58M 17s\n  1850K .......... .......... .......... .......... .......... 13% 1.50M 17s\n  1900K .......... .......... .......... .......... .......... 13% 1.67M 16s\n  1950K .......... .......... .......... .......... .......... 13% 1013K 16s\n  2000K .......... .......... .......... .......... .......... 14% 1.44M 16s\n  2050K .......... .......... .......... .......... .......... 14% 1.58M 16s\n  2100K .......... .......... .......... .......... .......... 14% 1.67M 15s\n  2150K .......... .......... .......... .......... .......... 15% 1.49M 15s\n  2200K .......... .......... .......... .......... .......... 15%  470K 15s\n  2250K .......... .......... .......... .......... .......... 15% 2.11M 15s\n  2300K .......... .......... .......... .......... .......... 16% 2.82M 15s\n  2350K .......... .......... .......... .......... .......... 16% 2.65M 14s\n  2400K .......... .......... .......... .......... .......... 16%  542K 15s\n  2450K .......... .......... .......... .......... .......... 17% 1.36M 14s\n  2500K .......... .......... .......... .......... .......... 17% 1.14M 14s\n  2550K .......... .......... .......... .......... .......... 17% 1.84M 14s\n  2600K .......... .......... .......... .......... .......... 18% 1.61M 14s\n  2650K .......... .......... .......... .......... .......... 18% 1.52M 14s\n  2700K .......... .......... .......... .......... .......... 18% 1.32M 13s\n  2750K .......... .......... .......... .......... .......... 19% 1.14M 13s\n  2800K .......... .......... .......... .......... .......... 19% 1.70M 13s\n  2850K .......... .......... .......... .......... .......... 20% 1.42M 13s\n  2900K .......... .......... .......... .......... .......... 20% 1.57M 13s\n  2950K .......... .......... .......... .......... .......... 20% 1.41M 13s\n  3000K .......... .......... .......... .......... .......... 21% 1.43M 13s\n  3050K .......... .......... .......... .......... .......... 21% 1.69M 12s\n  3100K .......... .......... .......... .......... .......... 21% 1.54M 12s\n  3150K .......... .......... .......... .......... .......... 22% 1.13M 12s\n  3200K .......... .......... .......... .......... .......... 22% 1.29M 12s\n  3250K .......... .......... .......... .......... .......... 22% 1.76M 12s\n  3300K .......... .......... .......... .......... .......... 23% 1.54M 12s\n  3350K .......... .......... .......... .......... .......... 23% 1.65M 12s\n  3400K .......... .......... .......... .......... .......... 23% 1.34M 12s\n  3450K .......... .......... .......... .......... .......... 24% 1.48M 12s\n  3500K .......... .......... .......... .......... .......... 24% 1.62M 11s\n  3550K .......... .......... .......... .......... .......... 24%  584K 11s\n  3600K .......... .......... .......... .......... .......... 25% 2.93M 11s\n  3650K .......... .......... .......... .......... .......... 25% 3.85M 11s\n  3700K .......... .......... .......... .......... .......... 25% 2.11M 11s\n  3750K .......... .......... .......... .......... .......... 26% 1.74M 11s\n  3800K .......... .......... .......... .......... .......... 26%  105K 12s\n  3850K .......... .......... .......... .......... .......... 26%  109M 12s\n  3900K .......... .......... .......... .......... .......... 27%  162M 12s\n  3950K .......... .......... .......... .......... .......... 27% 97.8M 11s\n  4000K .......... .......... .......... .......... .......... 27%  120M 11s\n  4050K .......... .......... .......... .......... .......... 28%  805K 11s\n  4100K .......... .......... .......... .......... .......... 28%  678K 11s\n  4150K .......... .......... .......... .......... .......... 28%  952K 11s\n  4200K .......... .......... .......... .......... .......... 29%  582K 11s\n  4250K .......... .......... .......... .......... .......... 29%  821K 11s\n  4300K .......... .......... .......... .......... .......... 30% 1.11M 11s\n  4350K .......... .......... .......... .......... .......... 30%  904K 11s\n  4400K .......... .......... .......... .......... .......... 30%  512K 11s\n  4450K .......... .......... .......... .......... .......... 31% 2.22M 11s\n  4500K .......... .......... .......... .......... .......... 31% 1.97M 11s\n  4550K .......... .......... .......... .......... .......... 31% 1.49M 11s\n  4600K .......... .......... .......... .......... .......... 32%  494K 11s\n  4650K .......... .......... .......... .......... .......... 32% 1.26M 11s\n  4700K .......... .......... .......... .......... .......... 32% 1.30M 11s\n  4750K .......... .......... .......... .......... .......... 33%  900K 10s\n  4800K .......... .......... .......... .......... .......... 33% 1.15M 10s\n  4850K .......... .......... .......... .......... .......... 33% 1.27M 10s\n  4900K .......... .......... .......... .......... .......... 34% 1.32M 10s\n  4950K .......... .......... .......... .......... .......... 34% 1.19M 10s\n  5000K .......... .......... .......... .......... .......... 34% 1.22M 10s\n  5050K .......... .......... .......... .......... .......... 35% 1.25M 10s\n  5100K .......... .......... .......... .......... .......... 35% 1.19M 10s\n  5150K .......... .......... .......... .......... .......... 35% 1.02M 10s\n  5200K .......... .......... .......... .......... .......... 36% 1.23M 10s\n  5250K .......... .......... .......... .......... .......... 36% 1.35M 10s\n  5300K .......... .......... .......... .......... .......... 36% 1.30M 10s\n  5350K .......... .......... .......... .......... .......... 37% 1.34M 10s\n  5400K .......... .......... .......... .......... .......... 37% 1.27M 9s\n  5450K .......... .......... .......... .......... .......... 37% 1.10M 9s\n  5500K .......... .......... .......... .......... .......... 38% 1.54M 9s\n  5550K .......... .......... .......... .......... .......... 38% 1.11M 9s\n  5600K .......... .......... .......... .......... .......... 39% 1.17M 9s\n  5650K .......... .......... .......... .......... .......... 39% 1.12M 9s\n  5700K .......... .......... .......... .......... .......... 39% 1.13M 9s\n  5750K .......... .......... .......... .......... .......... 40% 2.32M 9s\n  5800K .......... .......... .......... .......... .......... 40% 1.42M 9s\n  5850K .......... .......... .......... .......... .......... 40% 1.03M 9s\n  5900K .......... .......... .......... .......... .......... 41% 1.20M 9s\n  5950K .......... .......... .......... .......... .......... 41% 1.06M 9s\n  6000K .......... .......... .......... .......... .......... 41% 1.46M 9s\n  6050K .......... .......... .......... .......... .......... 42% 1.68M 9s\n  6100K .......... .......... .......... .......... .......... 42% 1.35M 8s\n  6150K .......... .......... .......... .......... .......... 42% 1.03M 8s\n  6200K .......... .......... .......... .......... .......... 43% 1.77M 8s\n  6250K .......... .......... .......... .......... .......... 43% 1.44M 8s\n  6300K .......... .......... .......... .......... .......... 43% 1.67M 8s\n  6350K .......... .......... .......... .......... .......... 44%  909K 8s\n  6400K .......... .......... .......... .......... .......... 44% 1.51M 8s\n  6450K .......... .......... .......... .......... .......... 44% 1.55M 8s\n  6500K .......... .......... .......... .......... .......... 45% 1.53M 8s\n  6550K .......... .......... .......... .......... .......... 45% 1.19M 8s\n  6600K .......... .......... .......... .......... .......... 45% 1.35M 8s\n  6650K .......... .......... .......... .......... .......... 46% 1.62M 8s\n  6700K .......... .......... .......... .......... .......... 46% 1.57M 8s\n  6750K .......... .......... .......... .......... .......... 46% 1.17M 8s\n  6800K .......... .......... .......... .......... .......... 47%  363K 8s\n  6850K .......... .......... .......... .......... .......... 47% 1.78M 8s\n  6900K .......... .......... .......... .......... .......... 47% 1.95M 7s\n  6950K .......... .......... .......... .......... .......... 48% 4.74M 7s\n  7000K .......... .......... .......... .......... .......... 48% 2.08M 7s\n  7050K .......... .......... .......... .......... .......... 49%  600K 7s\n  7100K .......... .......... .......... .......... .......... 49% 1.07M 7s\n  7150K .......... .......... .......... .......... .......... 49%  920K 7s\n  7200K .......... .......... .......... .......... .......... 50% 1.39M 7s\n  7250K .......... .......... .......... .......... .......... 50% 1.63M 7s\n  7300K .......... .......... .......... .......... .......... 50% 1.68M 7s\n  7350K .......... .......... .......... .......... .......... 51% 1.54M 7s\n  7400K .......... .......... .......... .......... .......... 51% 1.28M 7s\n  7450K .......... .......... .......... .......... .......... 51% 1.50M 7s\n  7500K .......... .......... .......... .......... .......... 52% 1.55M 7s\n  7550K .......... .......... .......... .......... .......... 52% 1.26M 7s\n  7600K .......... .......... .......... .......... .......... 52% 1.35M 7s\n  7650K .......... .......... .......... .......... .......... 53% 1.48M 7s\n  7700K .......... .......... .......... .......... .......... 53% 1.40M 7s\n  7750K .......... .......... .......... .......... .......... 53% 1.71M 6s\n  7800K .......... .......... .......... .......... .......... 54% 1.73M 6s\n  7850K .......... .......... .......... .......... .......... 54% 1.40M 6s\n  7900K .......... .......... .......... .......... .......... 54% 1.27M 6s\n  7950K .......... .......... .......... .......... .......... 55% 1.26M 6s\n  8000K .......... .......... .......... .......... .......... 55% 1.52M 6s\n  8050K .......... .......... .......... .......... .......... 55% 1.70M 6s\n  8100K .......... .......... .......... .......... .......... 56% 1.58M 6s\n  8150K .......... .......... .......... .......... .......... 56% 1.24M 6s\n  8200K .......... .......... .......... .......... .......... 56% 1.73M 6s\n  8250K .......... .......... .......... .......... .......... 57% 1.51M 6s\n  8300K .......... .......... .......... .......... .......... 57% 1.76M 6s\n  8350K .......... .......... .......... .......... .......... 57% 1.04M 6s\n  8400K .......... .......... .......... .......... .......... 58% 1.59M 6s\n  8450K .......... .......... .......... .......... .......... 58% 1.53M 6s\n  8500K .......... .......... .......... .......... .......... 59% 1.69M 6s\n  8550K .......... .......... .......... .......... .......... 59% 1.70M 6s\n  8600K .......... .......... .......... .......... .......... 59% 1.41M 5s\n  8650K .......... .......... .......... .......... .......... 60% 1.50M 5s\n  8700K .......... .......... .......... .......... .......... 60% 1.46M 5s\n  8750K .......... .......... .......... .......... .......... 60% 1.31M 5s\n  8800K .......... .......... .......... .......... .......... 61% 1.54M 5s\n  8850K .......... .......... .......... .......... .......... 61% 1.46M 5s\n  8900K .......... .......... .......... .......... .......... 61% 1.51M 5s\n  8950K .......... .......... .......... .......... .......... 62% 1.43M 5s\n  9000K .......... .......... .......... .......... .......... 62% 1.84M 5s\n  9050K .......... .......... .......... .......... .......... 62% 1.53M 5s\n  9100K .......... .......... .......... .......... .......... 63% 1.59M 5s\n  9150K .......... .......... .......... .......... .......... 63% 1.11M 5s\n  9200K .......... .......... .......... .......... .......... 63% 1.50M 5s\n  9250K .......... .......... .......... .......... .......... 64% 1.89M 5s\n  9300K .......... .......... .......... .......... .......... 64% 1.36M 5s\n  9350K .......... .......... .......... .......... .......... 64% 1.82M 5s\n  9400K .......... .......... .......... .......... .......... 65% 1.26M 5s\n  9450K .......... .......... .......... .......... .......... 65% 1.74M 5s\n  9500K .......... .......... .......... .......... .......... 65% 1.73M 4s\n  9550K .......... .......... .......... .......... .......... 66% 1.15M 4s\n  9600K .......... .......... .......... .......... .......... 66%  652K 4s\n  9650K .......... .......... .......... .......... .......... 66% 5.87M 4s\n  9700K .......... .......... .......... .......... .......... 67% 2.59M 4s\n  9750K .......... .......... .......... .......... .......... 67% 1.79M 4s\n  9800K .......... .......... .......... .......... .......... 68% 1.82M 4s\n  9850K .......... .......... .......... .......... .......... 68%  544K 4s\n  9900K .......... .......... .......... .......... .......... 68% 1.77M 4s\n  9950K .......... .......... .......... .......... .......... 69% 1.27M 4s\n 10000K .......... .......... .......... .......... .......... 69% 1.69M 4s\n 10050K .......... .......... .......... .......... .......... 69% 1.10M 4s\n 10100K .......... .......... .......... .......... .......... 70% 1.73M 4s\n 10150K .......... .......... .......... .......... .......... 70% 1.64M 4s\n 10200K .......... .......... .......... .......... .......... 70% 1.91M 4s\n 10250K .......... .......... .......... .......... .......... 71% 1.69M 4s\n 10300K .......... .......... .......... .......... .......... 71% 1.77M 4s\n 10350K .......... .......... .......... .......... .......... 71%  949K 4s\n 10400K .......... .......... .......... .......... .......... 72% 1.62M 4s\n 10450K .......... .......... .......... .......... .......... 72% 1.80M 4s\n 10500K .......... .......... .......... .......... .......... 72% 1.70M 3s\n 10550K .......... .......... .......... .......... .......... 73% 1.15M 3s\n 10600K .......... .......... .......... .......... .......... 73% 1.54M 3s\n 10650K .......... .......... .......... .......... .......... 73% 1.79M 3s\n 10700K .......... .......... .......... .......... .......... 74% 1.80M 3s\n 10750K .......... .......... .......... .......... .......... 74% 1.32M 3s\n 10800K .......... .......... .......... .......... .......... 74% 1.09M 3s\n 10850K .......... .......... .......... .......... .......... 75% 1.68M 3s\n 10900K .......... .......... .......... .......... .......... 75% 1.73M 3s\n 10950K .......... .......... .......... .......... .......... 75% 1.68M 3s\n 11000K .......... .......... .......... .......... .......... 76% 1.60M 3s\n 11050K .......... .......... .......... .......... .......... 76%  264K 3s\n 11100K .......... .......... .......... .......... .......... 76%  648K 3s\n 11150K .......... .......... .......... .......... .......... 77%  716K 3s\n 11200K .......... .......... .......... .......... .......... 77% 1.54M 3s\n 11250K .......... .......... .......... .......... .......... 78% 2.42M 3s\n 11300K .......... .......... .......... .......... .......... 78% 1.96M 3s\n 11350K .......... .......... .......... .......... .......... 78% 1.33M 3s\n 11400K .......... .......... .......... .......... .......... 79%  778K 3s\n 11450K .......... .......... .......... .......... .......... 79%  824K 3s\n 11500K .......... .......... .......... .......... .......... 79% 1.63M 3s\n 11550K .......... .......... .......... .......... .......... 80%  599K 3s\n 11600K .......... .......... .......... .......... .......... 80% 2.85M 3s\n 11650K .......... .......... .......... .......... .......... 80% 3.27M 2s\n 11700K .......... .......... .......... .......... .......... 81% 2.36M 2s\n 11750K .......... .......... .......... .......... .......... 81% 1.55M 2s\n 11800K .......... .......... .......... .......... .......... 81%  585K 2s\n 11850K .......... .......... .......... .......... .......... 82% 1.53M 2s\n 11900K .......... .......... .......... .......... .......... 82% 1.67M 2s\n 11950K .......... .......... .......... .......... .......... 82% 1.41M 2s\n 12000K .......... .......... .......... .......... .......... 83% 1.80M 2s\n 12050K .......... .......... .......... .......... .......... 83%  391K 2s\n 12100K .......... .......... .......... .......... .......... 83% 1.18M 2s\n 12150K .......... .......... .......... .......... .......... 84% 1.67M 2s\n 12200K .......... .......... .......... .......... .......... 84% 1.82M 2s\n 12250K .......... .......... .......... .......... .......... 84% 4.70M 2s\n 12300K .......... .......... .......... .......... .......... 85% 1004K 2s\n 12350K .......... .......... .......... .......... .......... 85%  762K 2s\n 12400K .......... .......... .......... .......... .......... 85% 1.21M 2s\n 12450K .......... .......... .......... .......... .......... 86% 1.29M 2s\n 12500K .......... .......... .......... .......... .......... 86% 1.73M 2s\n 12550K .......... .......... .......... .......... .......... 86% 1.64M 2s\n 12600K .......... .......... .......... .......... .......... 87% 1.75M 2s\n 12650K .......... .......... .......... .......... .......... 87% 1.40M 2s\n 12700K .......... .......... .......... .......... .......... 88% 1.47M 2s\n 12750K .......... .......... .......... .......... .......... 88% 1.24M 1s\n 12800K .......... .......... .......... .......... .......... 88% 1.72M 1s\n 12850K .......... .......... .......... .......... .......... 89% 1.58M 1s\n 12900K .......... .......... .......... .......... .......... 89% 1.42M 1s\n 12950K .......... .......... .......... .......... .......... 89% 1.51M 1s\n 13000K .......... .......... .......... .......... .......... 90% 1.51M 1s\n 13050K .......... .......... .......... .......... .......... 90% 1.81M 1s\n 13100K .......... .......... .......... .......... .......... 90% 1.63M 1s\n 13150K .......... .......... .......... .......... .......... 91% 1.12M 1s\n 13200K .......... .......... .......... .......... .......... 91% 1.40M 1s\n 13250K .......... .......... .......... .......... .......... 91% 1.73M 1s\n 13300K .......... .......... .......... .......... .......... 92% 1.80M 1s\n 13350K .......... .......... .......... .......... .......... 92% 1.65M 1s\n 13400K .......... .......... .......... .......... .......... 92%  145K 1s\n 13450K .......... .......... .......... .......... .......... 93%  234M 1s\n 13500K .......... .......... .......... .......... .......... 93%  189M 1s\n 13550K .......... .......... .......... .......... .......... 93%  154M 1s\n 13600K .......... .......... .......... .......... .......... 94%  216M 1s\n 13650K .......... .......... .......... .......... .......... 94% 5.31M 1s\n 13700K .......... .......... .......... .......... .......... 94%  746K 1s\n 13750K .......... .......... .......... .......... .......... 95% 1.19M 1s\n 13800K .......... .......... .......... .......... .......... 95%  808K 1s\n 13850K .......... .......... .......... .......... .......... 95% 1.18M 1s\n 13900K .......... .......... .......... .......... .......... 96% 1.27M 0s\n 13950K .......... .......... .......... .......... .......... 96%  717K 0s\n 14000K .......... .......... .......... .......... .......... 97% 1.30M 0s\n 14050K .......... .......... .......... .......... .......... 97% 1.13M 0s\n 14100K .......... .......... .......... .......... .......... 97% 1.49M 0s\n 14150K .......... .......... .......... .......... .......... 98%  919K 0s\n 14200K .......... .......... .......... .......... .......... 98% 1.33M 0s\n 14250K .......... .......... .......... .......... .......... 98%  964K 0s\n 14300K .......... .......... .......... .......... .......... 99% 1.29M 0s\n 14350K .......... .......... .......... .......... .......... 99% 1.01M 0s\n 14400K .......... .......... .......... .......... .......... 99% 1.34M 0s\n 14450K .......... .......... .......... ...                  100% 1.11M\u003d13s\n\n2018-08-26 17:18:44 (1.11 MB/s) - ‘/tmp/activity-data_2.json’ saved [14831559/14831559]\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1535104822790_-411049362",
      "id": "20180824-100022_761080285",
      "dateCreated": "Aug 24, 2018 10:00:22 AM",
      "dateStarted": "Aug 26, 2018 5:18:12 PM",
      "dateFinished": "Aug 26, 2018 5:18:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.read.format(\"json\")\n                   .load(\"/tmp/activity-data\")\n                   \ndf.schema\ndf.count()\n",
      "user": "anonymous",
      "dateUpdated": "Aug 26, 2018 5:21:57 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df: org.apache.spark.sql.DataFrame \u003d [Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]\nres5: org.apache.spark.sql.types.StructType \u003d StructType(StructField(Arrival_Time,LongType,true), StructField(Creation_Time,LongType,true), StructField(Device,StringType,true), StructField(Index,LongType,true), StructField(Model,StringType,true), StructField(User,StringType,true), StructField(gt,StringType,true), StructField(x,DoubleType,true), StructField(y,DoubleType,true), StructField(z,DoubleType,true))\nres6: Long \u003d 156023\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1535126969504_973393802",
      "id": "20180824-160929_1416638510",
      "dateCreated": "Aug 24, 2018 4:09:29 PM",
      "dateStarted": "Aug 26, 2018 5:21:57 PM",
      "dateFinished": "Aug 26, 2018 5:22:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// By specifying maxFilesPerTrigger value lower, we’re artificially limiting the flow of the stream to one file per trigger.\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n\nval streamingDf \u003d spark.readStream.schema(df.schema)\n                                  .option(\"maxFilesPerTrigger\", 1)\n                                  .json(\"/tmp/activity-data\")\n\nval activityCounts \u003d streamingDf.groupBy(\"gt\")\n                                .count()\n\nval activityQuery \u003d activityCounts.writeStream.queryName(\"activity_counts\")\n                                              .format(\"memory\")\n                                              .outputMode(\"complete\")\n                                              .start()\n                                              \n\nspark.streams.active\n\nfor( i \u003c- 1 to 5 ) {    \n    spark.sql(\"SELECT * FROM activity_counts\")\n         .show()\n    Thread.sleep(1000)\n}\n\nactivityQuery.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "Aug 26, 2018 5:22:18 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535127111003_-29464335",
      "id": "20180824-161151_898673649",
      "dateCreated": "Aug 24, 2018 4:11:51 PM",
      "dateStarted": "Aug 26, 2018 5:22:18 PM",
      "dateFinished": "Aug 26, 2018 5:21:40 PM",
      "status": "RUNNING",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "Aug 24, 2018 5:25:44 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535130160215_-2057888493",
      "id": "20180824-170240_747411825",
      "dateCreated": "Aug 24, 2018 5:02:40 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n#### Transformations on Streams\n\n- Streaming transformations, as we mentioned, include almost all static DataFrame transformations. \n- All select, filter, and simple transformations are supported, as are all DataFrame functions and individual column manipulations. \n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 26, 2018 5:27:10 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535130988508_1673786156",
      "id": "20180824-171628_1991746325",
      "dateCreated": "Aug 24, 2018 5:16:28 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.expr\n\nval simpleTransform \u003d streamingDf.withColumn(\"stairs\", expr(\"gt like \u0027%stairs%\u0027\"))\n                                  .where(\"stairs\")\n                                  .where(\"gt is not null\")\n                                  .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\n                                  .writeStream\n                                  .queryName(\"simple_transform\")\n                                  .format(\"memory\")\n                                  .outputMode(\"append\")\n                                  .start()\n                                  \nsimpleTransform.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "Aug 26, 2018 5:24:53 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535131086355_-264276132",
      "id": "20180824-171806_714435006",
      "dateCreated": "Aug 24, 2018 5:18:06 PM",
      "dateStarted": "Aug 26, 2018 5:24:53 PM",
      "dateFinished": "Aug 26, 2018 5:30:38 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### Aggregations\n\n- Structured Streaming has excellent support for aggregations. You can specify arbitrary aggregations.\n-  you can use a more exotic aggregation, like a cube as shown below",
      "user": "anonymous",
      "dateUpdated": "Aug 26, 2018 5:28:40 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535304293655_-770148929",
      "id": "20180826-172453_2057495367",
      "dateCreated": "Aug 26, 2018 5:24:53 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val deviceModelStats \u003d streamingDf.cube(\"gt\", \"model\").avg()\n                                  .drop(\"avg(Arrival_time)\")\n                                  .drop(\"avg(Creation_Time)\")\n                                  .drop(\"avg(Index)\")\n                                  .writeStream.queryName(\"device_counts\")\n                                              .format(\"memory\")\n                                              .outputMode(\"complete\")\n                                              .start()\n                                              \n                                              \n for( i \u003c- 1 to 5 ) {    \n    spark.sql(\"SELECT * FROM device_counts\")\n         .show()\n    Thread.sleep(1000)\n}\n\ndeviceModelStats.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "Aug 26, 2018 5:30:38 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535304520017_1719731414",
      "id": "20180826-172840_1694758932",
      "dateCreated": "Aug 26, 2018 5:28:40 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n#### Joins\n- As of Apache Spark 2.2, Structured Streaming supports joining streaming DataFrames to static DataFrames. \n- Spark 2.3 will add the ability to join multiple streams together. \n- You can do multiple column joins and supplement streaming data with that from static data sources:",
      "user": "anonymous",
      "dateUpdated": "Aug 26, 2018 5:32:12 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535304648611_-1356569194",
      "id": "20180826-173048_823944315",
      "dateCreated": "Aug 26, 2018 5:30:48 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val historicalAgg \u003d df.groupBy(\"gt\", \"model\").avg()\n\nval deviceModelStatsJoin \u003d streamingDf.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\n                                .cube(\"gt\", \"model\").avg()\n                                .join(historicalAgg, Seq(\"gt\", \"model\"))\n                                .writeStream.queryName(\"device_counts\")\n                                .format(\"memory\")\n                                .outputMode(\"complete\")\n                                .start()\n                                \ndeviceModelStatsJoin.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "Aug 26, 2018 5:33:55 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535304732042_259515748",
      "id": "20180826-173212_951735541",
      "dateCreated": "Aug 26, 2018 5:32:12 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535304835463_876573074",
      "id": "20180826-173355_745700447",
      "dateCreated": "Aug 26, 2018 5:33:55 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "spark_streaming",
  "id": "2DQBK3QB5",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}