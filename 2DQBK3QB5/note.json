{
  "paragraphs": [
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "Aug 24, 2018 10:05:09 AM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535105109015_35858650",
      "id": "20180824-100509_1708310699",
      "dateCreated": "Aug 24, 2018 10:05:09 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Stream Processing\n- Stream processing is the act of continuously incorporating new data to compute a result. \n- In stream processing, the input data is unbounded and has no predetermined beginning or end. \n- It simply forms a series of events that arrive at the stream processing system (e.g., credit card transactions, clicks on a website, or sensor read)\n- User applications can then compute various queries over this stream of events (e.g., tracking a running count of each type of event or aggregating them into hourly windows).\n- Batch processing also takes a query to compute, similar to stream processing, but only computes the result once.\n- Although streaming and batch processing sound different, in practice, they often need to work together.\n- For example, streaming applications often need to join input data against a dataset written periodically by a batch job, and the output of streaming jobs is often files or tables that are queried in batch jobs\n- Structured Streaming was designed from the beginning to interoperate easily with the rest of Spark, including batch applications. \n- Indeed, the Structured Streaming developers coined the term continuous applications to capture end-to-end applications that consist of streaming, batch, and interactive jobs all working on the same data to deliver an end product\n\n### Stream processing usecases\n- NOTIFICATIONS AND ALERTING\n- Real-time reporting\n- Incremental ETL\n- Update data to serve in real time\n- Real-time decision making\n- Online machine learning\n\n### Advantages of Stream Processing\n- stream processing enables lower latency: when your application needs to respond quickly (on a timescale of minutes, seconds, or milliseconds), you will need a streaming system that can keep state in memory to get acceptable performance.\n- stream processing can also be more efficient in updating a result than repeated batch jobs, because it automatically incrementalizes the computation\n\n### Challenges of Stream Processing\n- Processing out-of-order data based on application timestamp\n- Maintaining large amounts of state\n- Supporting high-data throughputWhen using event-time, several issues become common concerns across applications, including tracking state in a manner that allows the system to incorporate late events, and determining when it is safe to output a result for a given time window in event time \n- Processing each event exactly once despite machine failures\n- Handling load imbalance and stragglers\n- Joining with external data in other storage systems\n- Writing data transactionally to output systems\n\n### Stream Processing Design Points\n\n#### Record-at-a-Time Versus Declarative APIs\n- just pass each event to the application and let it react using custom code.\n- Apache Storm, implemented record at a time, and it has an important place when applications need full control over the processing of data\n- downside of these systems is that most of the complicating factors we described earlier, such as maintaining state, are solely governed by the application.\n- For example, with a record-at-a-time API, you are responsible for tracking state over longer time periods, dropping it after some time to clear up space, and responding differently to duplicate events after a failure.\n- In declarative APIs,  your application specifies what to compute but not how to compute it in response to each new event and how to recover from failure. \n- Spark Streaming tracks how much data each operator had processed, saved any relevant state reliably, and recovered the computation from failure when needed.\n- Structured streaming allow SQL like operations apart from functional operations over stream\n\n####  Event Time vs Processing Time\n- Event time is the idea of processing data based on timestamps inserted into each record at the source, as opposed to the time when the record is received at the streaming application \n- In particular, when using event time, records may arrive to the system out of order \n- If your application collects data from remote sources that may be delayed, such as mobile phones or IoT devices, event-time processing is crucial: without it, you will miss important patterns when some data is late. \n- In contrast, if your application only processes local events (e.g., ones generated in the same datacenter), you may not need sophisticated event-time processing.\n- When using event-time, several issues become common concerns across applications, \n             - including tracking state in a manner that allows the system to incorporate late events\n             - determining when it is safe to output a result for a given time window in event time \n             - \n- many declarative systems, including Structured Streaming, have “native” support for event time integrated into all their APIs, so that these concerns can be handled automatically across your whole program\n\n#### Continuous Versus Micro-Batch Execution\n- In a continuous processing system, each of the nodes implementing map would read records one by one from an input source, compute its function on them, and send them to the appropriate reducer. The reducer would then update its state whenever it gets a new record. The key idea is that this happens on each individual record\n- Continuous processing has the advantage of offering the lowest possible latency when the total input rate is relatively low, because each node responds immediately to a new message. \n- However, continuous processing systems generally have lower maximum throughput, because they incur a significant amount of overhead per-record (e.g., calling the operating system to send a packet to a downstream node).\n\n![sv-image](https://user-images.githubusercontent.com/1182329/44578961-8f009f00-a7b2-11e8-8ce1-1046eb9b01e1.png)\n\n\n- Micro-batch systems wait to accumulate small batches of input data (say, 500 ms’ worth), then process each batch in parallel using a distributed collection of tasks, similar to the execution of a batch job in Spark. - - Micro-batch systems can often achieve high throughput per node because they leverage the same optimizations as batch systems (e.g., vectorized processing), and do not incur any extra per-record overhead.\n- Hwever, Micro-batch systems has higher base latency due to waiting to accumulate a micro-batch. \n- In practice, the streaming applications that are large-scale enough to need to distribute their computation tend to prioritize throughput, so Spark has traditionally implemented micro-batch processing.\n- Micro-batch systems can comfortably deliver latencies from 100 ms to a second, depending on the application. Within this regime, they will generally require fewer nodes to achieve the same throughput, and hence lower operational cost (including lower maintenance cost due to less frequent node failures). \n- For much lower latencies, you should consider a continuous processing system, or using a micro-batch system in conjunction with a fast serving layer to provide low-latency queries\n\n![sv-image](https://user-images.githubusercontent.com/1182329/44578861-4d6ff400-a7b2-11e8-9b7b-640e17baa387.png)\n\n\n\n### Sprk Streaming and DStreams\n\n- Spark Streaming and its DStreams API, one of the first APIs to enable stream processing using high-level functional operators like map and reduce.\n- Much like the Resilient Distributed Dataset (RDD) API, however, the DStreams API is based on relatively low-level operations on Java/Python objects that limit opportunities for higher-level optimization\n\n### Spark Streaming and Structured Streaming\n\n- in 2016, the Spark project added Structured Streaming, a new streaming API built directly on DataFrames that supports both rich optimizations and significantly simpler integration with other DataFrame and Dataset code\n- Structured Streaming offers a superset of the majority of the functionality of DStreams, and will often perform better due to code generation and the Catalyst optimizer.\n- Structured Streaming was designed from the beginning to interoperate easily with the rest of Spark, including batch applications. \n- Indeed, the Structured Streaming developers coined the term continuous applications to capture end-to-end applications that consist of streaming, batch, and interactive jobs all working on the same data to deliver an end product.\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 24, 2018 10:05:36 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eStream Processing\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eStream processing is the act of continuously incorporating new data to compute a result.\u003c/li\u003e\n\u003cli\u003eIn stream processing, the input data is unbounded and has no predetermined beginning or end.\u003c/li\u003e\n\u003cli\u003eIt simply forms a series of events that arrive at the stream processing system (e.g., credit card transactions, clicks on a website, or sensor read)\u003c/li\u003e\n\u003cli\u003eUser applications can then compute various queries over this stream of events (e.g., tracking a running count of each type of event or aggregating them into hourly windows).\u003c/li\u003e\n\u003cli\u003eBatch processing also takes a query to compute, similar to stream processing, but only computes the result once.\u003c/li\u003e\n\u003cli\u003eAlthough streaming and batch processing sound different, in practice, they often need to work together.\u003c/li\u003e\n\u003cli\u003eFor example, streaming applications often need to join input data against a dataset written periodically by a batch job, and the output of streaming jobs is often files or tables that are queried in batch jobs\u003c/li\u003e\n\u003cli\u003eStructured Streaming was designed from the beginning to interoperate easily with the rest of Spark, including batch applications.\u003c/li\u003e\n\u003cli\u003eIndeed, the Structured Streaming developers coined the term continuous applications to capture end-to-end applications that consist of streaming, batch, and interactive jobs all working on the same data to deliver an end product\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eStream processing usecases\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNOTIFICATIONS AND ALERTING\u003c/li\u003e\n\u003cli\u003eReal-time reporting\u003c/li\u003e\n\u003cli\u003eIncremental ETL\u003c/li\u003e\n\u003cli\u003eUpdate data to serve in real time\u003c/li\u003e\n\u003cli\u003eReal-time decision making\u003c/li\u003e\n\u003cli\u003eOnline machine learning\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAdvantages of Stream Processing\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003estream processing enables lower latency: when your application needs to respond quickly (on a timescale of minutes, seconds, or milliseconds), you will need a streaming system that can keep state in memory to get acceptable performance.\u003c/li\u003e\n\u003cli\u003estream processing can also be more efficient in updating a result than repeated batch jobs, because it automatically incrementalizes the computation\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eChallenges of Stream Processing\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eProcessing out-of-order data based on application timestamp\u003c/li\u003e\n\u003cli\u003eMaintaining large amounts of state\u003c/li\u003e\n\u003cli\u003eSupporting high-data throughputWhen using event-time, several issues become common concerns across applications, including tracking state in a manner that allows the system to incorporate late events, and determining when it is safe to output a result for a given time window in event time\u003c/li\u003e\n\u003cli\u003eProcessing each event exactly once despite machine failures\u003c/li\u003e\n\u003cli\u003eHandling load imbalance and stragglers\u003c/li\u003e\n\u003cli\u003eJoining with external data in other storage systems\u003c/li\u003e\n\u003cli\u003eWriting data transactionally to output systems\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eStream Processing Design Points\u003c/h3\u003e\n\u003ch5\u003eRecord-at-a-Time Versus Declarative APIs\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003ejust pass each event to the application and let it react using custom code.\u003c/li\u003e\n\u003cli\u003eApache Storm, implemented record at a time, and it has an important place when applications need full control over the processing of data\u003c/li\u003e\n\u003cli\u003edownside of these systems is that most of the complicating factors we described earlier, such as maintaining state, are solely governed by the application.\u003c/li\u003e\n\u003cli\u003eFor example, with a record-at-a-time API, you are responsible for tracking state over longer time periods, dropping it after some time to clear up space, and responding differently to duplicate events after a failure.\u003c/li\u003e\n\u003cli\u003eIn declarative APIs,  your application specifies what to compute but not how to compute it in response to each new event and how to recover from failure.\u003c/li\u003e\n\u003cli\u003eSpark Streaming tracks how much data each operator had processed, saved any relevant state reliably, and recovered the computation from failure when needed.\u003c/li\u003e\n\u003cli\u003eStructured streaming allow SQL like operations apart from functional operations over stream\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eEvent Time vs Processing Time\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEvent time is the idea of processing data based on timestamps inserted into each record at the source, as opposed to the time when the record is received at the streaming application\u003c/li\u003e\n\u003cli\u003eIn particular, when using event time, records may arrive to the system out of order\u003c/li\u003e\n\u003cli\u003eIf your application collects data from remote sources that may be delayed, such as mobile phones or IoT devices, event-time processing is crucial: without it, you will miss important patterns when some data is late.\u003c/li\u003e\n\u003cli\u003eIn contrast, if your application only processes local events (e.g., ones generated in the same datacenter), you may not need sophisticated event-time processing.\u003c/li\u003e\n\u003cli\u003eWhen using event-time, several issues become common concerns across applications,\u003cpre\u003e\u003ccode\u003e     - including tracking state in a manner that allows the system to incorporate late events\n     - determining when it is safe to output a result for a given time window in event time \n     - \n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003emany declarative systems, including Structured Streaming, have “native” support for event time integrated into all their APIs, so that these concerns can be handled automatically across your whole program\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eContinuous Versus Micro-Batch Execution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIn a continuous processing system, each of the nodes implementing map would read records one by one from an input source, compute its function on them, and send them to the appropriate reducer. The reducer would then update its state whenever it gets a new record. The key idea is that this happens on each individual record\u003c/li\u003e\n\u003cli\u003eContinuous processing has the advantage of offering the lowest possible latency when the total input rate is relatively low, because each node responds immediately to a new message.\u003c/li\u003e\n\u003cli\u003eHowever, continuous processing systems generally have lower maximum throughput, because they incur a significant amount of overhead per-record (e.g., calling the operating system to send a packet to a downstream node).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/44578961-8f009f00-a7b2-11e8-8ce1-1046eb9b01e1.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMicro-batch systems wait to accumulate small batches of input data (say, 500 ms’ worth), then process each batch in parallel using a distributed collection of tasks, similar to the execution of a batch job in Spark. - - Micro-batch systems can often achieve high throughput per node because they leverage the same optimizations as batch systems (e.g., vectorized processing), and do not incur any extra per-record overhead.\u003c/li\u003e\n\u003cli\u003eHwever, Micro-batch systems has higher base latency due to waiting to accumulate a micro-batch.\u003c/li\u003e\n\u003cli\u003eIn practice, the streaming applications that are large-scale enough to need to distribute their computation tend to prioritize throughput, so Spark has traditionally implemented micro-batch processing.\u003c/li\u003e\n\u003cli\u003eMicro-batch systems can comfortably deliver latencies from 100 ms to a second, depending on the application. Within this regime, they will generally require fewer nodes to achieve the same throughput, and hence lower operational cost (including lower maintenance cost due to less frequent node failures).\u003c/li\u003e\n\u003cli\u003eFor much lower latencies, you should consider a continuous processing system, or using a micro-batch system in conjunction with a fast serving layer to provide low-latency queries\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/44578861-4d6ff400-a7b2-11e8-9b7b-640e17baa387.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch3\u003eSprk Streaming and DStreams\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSpark Streaming and its DStreams API, one of the first APIs to enable stream processing using high-level functional operators like map and reduce.\u003c/li\u003e\n\u003cli\u003eMuch like the Resilient Distributed Dataset (RDD) API, however, the DStreams API is based on relatively low-level operations on Java/Python objects that limit opportunities for higher-level optimization\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eSpark Streaming and Structured Streaming\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein 2016, the Spark project added Structured Streaming, a new streaming API built directly on DataFrames that supports both rich optimizations and significantly simpler integration with other DataFrame and Dataset code\u003c/li\u003e\n\u003cli\u003eStructured Streaming offers a superset of the majority of the functionality of DStreams, and will often perform better due to code generation and the Catalyst optimizer.\u003c/li\u003e\n\u003cli\u003eStructured Streaming was designed from the beginning to interoperate easily with the rest of Spark, including batch applications.\u003c/li\u003e\n\u003cli\u003eIndeed, the Structured Streaming developers coined the term continuous applications to capture end-to-end applications that consist of streaming, batch, and interactive jobs all working on the same data to deliver an end product.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1535102804528_343963730",
      "id": "20180824-092644_37209853",
      "dateCreated": "Aug 24, 2018 9:26:44 AM",
      "dateStarted": "Aug 24, 2018 10:04:59 AM",
      "dateFinished": "Aug 24, 2018 10:04:59 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "Aug 24, 2018 10:00:22 AM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1535104822790_-411049362",
      "id": "20180824-100022_761080285",
      "dateCreated": "Aug 24, 2018 10:00:22 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "spark_streaming",
  "id": "2DQBK3QB5",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}