{
  "paragraphs": [
    {
      "text": "%md\n\n### Building expressions and working with different types of data\n\n- which are the bread and butter of Spark’s structured operations. \n- We also review working with a variety of different kinds of data\n           - Booleans\n           - Numbers\n           - Strings\n           - Dates\n           - timestamps\n           - Handling null\n           - Complex types\n           - User-defined functions\n\n#### API Links\n\n- http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spa\n- http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\n\n#### SQL functions\n\n- org.apache.spark.sql.functions contains a variety of functions for a range of different data types.\n- http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:02:52 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003ebuilding expressions and working with different types of data\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewhich are the bread and butter of Spark’s structured operations.\u003c/li\u003e\n\u003cli\u003eWe also review working with a variety of different kinds of data\u003cpre\u003e\u003ccode\u003e   - Booleans\n   - Numbers\n   - Strings\n   - Dates\n   - timestamps\n   - Handling null\n   - Complex types\n   - User-defined functions\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eAPI Links\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spa\u003c/li\u003e\n\u003cli\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eSQL functions\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eorg.apache.spark.sql.functions contains a variety of functions for a range of different data types.\u003c/li\u003e\n\u003cli\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533699503335_291883990",
      "id": "20180808-033823_1744307158",
      "dateCreated": "Aug 8, 2018 3:38:23 AM",
      "dateStarted": "Aug 8, 2018 5:02:44 AM",
      "dateFinished": "Aug 8, 2018 5:02:44 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n\nif [ -e /tmp/2010-12-01.csv ]\nthen\n    rm -f /tmp/2010-12-01.csv\nfi\n\nwget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-01.csv -O /tmp/2010-12-01.csv\n\nhdfs dfs -rm -f /tmp/2010-12-01.csv\n\n# Move downloaded JSON file from local storage to HDFS\nhdfs dfs -put /tmp/2010-12-01.csv /tmp",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 3:46:03 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "--2018-08-08 03:46:04--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-01.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 275001 (269K) [text/plain]\nSaving to: ‘/tmp/2010-12-01.csv’\n\n     0K .......... .......... .......... .......... .......... 18% 1.57M 0s\n    50K .......... .......... .......... .......... .......... 37% 3.52M 0s\n   100K .......... .......... .......... .......... .......... 55% 11.4M 0s\n   150K .......... .......... .......... .......... .......... 74% 4.35M 0s\n   200K .......... .......... .......... .......... .......... 93% 13.2M 0s\n   250K .......... ........                                   100% 14.3M\u003d0.07s\n\n2018-08-08 03:46:05 (4.00 MB/s) - ‘/tmp/2010-12-01.csv’ saved [275001/275001]\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533699601496_-18269596",
      "id": "20180808-034001_1314893295",
      "dateCreated": "Aug 8, 2018 3:40:01 AM",
      "dateStarted": "Aug 8, 2018 3:46:03 AM",
      "dateFinished": "Aug 8, 2018 3:46:16 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.read.format(\"csv\")\n              .option(\"inferSchema\", true)\n              .option(\"header\", true)\n              .load(\"/tmp/2010-12-01.csv\")\n\ndf.printSchema()\ndf.createOrReplaceTempView(\"dfTable\")",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:19:05 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df: org.apache.spark.sql.DataFrame \u003d [InvoiceNo: string, StockCode: string ... 6 more fields]\nroot\n |-- InvoiceNo: string (nullable \u003d true)\n |-- StockCode: string (nullable \u003d true)\n |-- Description: string (nullable \u003d true)\n |-- Quantity: integer (nullable \u003d true)\n |-- InvoiceDate: timestamp (nullable \u003d true)\n |-- UnitPrice: double (nullable \u003d true)\n |-- CustomerID: double (nullable \u003d true)\n |-- Country: string (nullable \u003d true)\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533699963705_-1664349105",
      "id": "20180808-034603_1257291904",
      "dateCreated": "Aug 8, 2018 3:46:03 AM",
      "dateStarted": "Aug 8, 2018 5:19:05 PM",
      "dateFinished": "Aug 8, 2018 5:19:34 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Working with Booleans\n\n- Booleans are essential when it comes to data analysis because they are the foundation for all filtering. \n- Boolean statements consist of four elements\n               - and\n               - or\n               - true\n               - false ",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 3:52:10 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eWorking with Booleans\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eBooleans are essential when it comes to data analysis because they are the foundation for all filtering.\u003c/li\u003e\n\u003cli\u003eBoolean statements consist of four elements\u003cpre\u003e\u003ccode\u003e       - and\n       - or\n       - true\n       - false\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533700089664_-1252066850",
      "id": "20180808-034809_96701658",
      "dateCreated": "Aug 8, 2018 3:48:09 AM",
      "dateStarted": "Aug 8, 2018 3:52:10 AM",
      "dateFinished": "Aug 8, 2018 3:52:10 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.where(col(\"InvoiceNo\").equalTo(536365))\n  .select(\"InvoiceNo\", \"Description\")\n  .show(5)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 3:54:52 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---------+--------------------+\n|InvoiceNo|         Description|\n+---------+--------------------+\n|   536365|WHITE HANGING HEA...|\n|   536365| WHITE METAL LANTERN|\n|   536365|CREAM CUPID HEART...|\n|   536365|KNITTED UNION FLA...|\n|   536365|RED WOOLLY HOTTIE...|\n+---------+--------------------+\nonly showing top 5 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533700329708_748748380",
      "id": "20180808-035209_683229902",
      "dateCreated": "Aug 8, 2018 3:52:09 AM",
      "dateStarted": "Aug 8, 2018 3:53:53 AM",
      "dateFinished": "Aug 8, 2018 3:53:55 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.where(col(\"InvoiceNo\") \u003d\u003d\u003d 536365)\n  .select(\"InvoiceNo\", \"Description\")\n  .show(5)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 3:55:28 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---------+--------------------+\n|InvoiceNo|         Description|\n+---------+--------------------+\n|   536365|WHITE HANGING HEA...|\n|   536365| WHITE METAL LANTERN|\n|   536365|CREAM CUPID HEART...|\n|   536365|KNITTED UNION FLA...|\n|   536365|RED WOOLLY HOTTIE...|\n+---------+--------------------+\nonly showing top 5 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533700432083_-1047858773",
      "id": "20180808-035352_177115352",
      "dateCreated": "Aug 8, 2018 3:53:52 AM",
      "dateStarted": "Aug 8, 2018 3:55:28 AM",
      "dateFinished": "Aug 8, 2018 3:55:29 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.where(\"InvoiceNo \u003d 536365\")\n  .select(\"InvoiceNo\", \"Description\")\n  .show(5)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 3:56:25 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---------+--------------------+\n|InvoiceNo|         Description|\n+---------+--------------------+\n|   536365|WHITE HANGING HEA...|\n|   536365| WHITE METAL LANTERN|\n|   536365|CREAM CUPID HEART...|\n|   536365|KNITTED UNION FLA...|\n|   536365|RED WOOLLY HOTTIE...|\n+---------+--------------------+\nonly showing top 5 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533700510524_1120075396",
      "id": "20180808-035510_1965763827",
      "dateCreated": "Aug 8, 2018 3:55:10 AM",
      "dateStarted": "Aug 8, 2018 3:56:26 AM",
      "dateFinished": "Aug 8, 2018 3:56:27 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.where(\"InvoiceNo \u003c\u003e 536365\")\n  .select(\"InvoiceNo\", \"Description\")\n  .show(5)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 3:57:07 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---------+--------------------+\n|InvoiceNo|         Description|\n+---------+--------------------+\n|   536366|HAND WARMER UNION...|\n|   536366|HAND WARMER RED P...|\n|   536367|ASSORTED COLOUR B...|\n|   536367|POPPY\u0027S PLAYHOUSE...|\n|   536367|POPPY\u0027S PLAYHOUSE...|\n+---------+--------------------+\nonly showing top 5 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533700585920_-1566333369",
      "id": "20180808-035625_160960952",
      "dateCreated": "Aug 8, 2018 3:56:25 AM",
      "dateStarted": "Aug 8, 2018 3:57:07 AM",
      "dateFinished": "Aug 8, 2018 3:57:08 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val priceFilter \u003d col(\"UnitPrice\") \u003e 600\n\nval descFilter \u003d col(\"Description\").contains(\"POSTAGE\")\n\ndf.where(col(\"StockCode\").isin(\"DOT\"))\n  .where(priceFilter.or(descFilter))\n  .show(5)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:00:42 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "priceFilter: org.apache.spark.sql.Column \u003d (UnitPrice \u003e 600)\ndescFilter: org.apache.spark.sql.Column \u003d contains(Description, POSTAGE)\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533700596412_-1925658194",
      "id": "20180808-035636_985458280",
      "dateCreated": "Aug 8, 2018 3:56:36 AM",
      "dateStarted": "Aug 8, 2018 4:00:42 AM",
      "dateFinished": "Aug 8, 2018 4:00:44 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val dotCodeFilter \u003d col(\"StockCode\") \u003d\u003d\u003d \"DOT\"\n\nval priceFilter \u003d col(\"UnitPrice\") \u003e 600\n\nval descFilter \u003d col(\"Description\").contains(\"POSTAGE\")\n\ndf.withColumn(\"isExpensive\", dotCodeFilter.and(priceFilter.or(descFilter)))\n  .where(\"isExpensive\")\n  .select(\"unitPrice\", \"isExpensive\")\n  .show(5)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:03:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "dotCodeFilter: org.apache.spark.sql.Column \u003d (StockCode \u003d DOT)\npriceFilter: org.apache.spark.sql.Column \u003d (UnitPrice \u003e 600)\ndescFilter: org.apache.spark.sql.Column \u003d contains(Description, POSTAGE)\n+---------+-----------+\n|unitPrice|isExpensive|\n+---------+-----------+\n|   569.77|       true|\n|   607.49|       true|\n+---------+-----------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533700825687_-618532075",
      "id": "20180808-040025_1110802486",
      "dateCreated": "Aug 8, 2018 4:00:25 AM",
      "dateStarted": "Aug 8, 2018 4:03:51 AM",
      "dateFinished": "Aug 8, 2018 4:03:52 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{expr, not, col}\n\ndf.withColumn(\"isExpensive\", not(col(\"UnitPrice\").leq(250)))\n  .filter(\"isExpensive\")\n  .select(\"Description\", \"UnitPrice\")\n  .show(5)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:05:25 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{expr, not, col}\n+--------------+---------+\n|   Description|UnitPrice|\n+--------------+---------+\n|DOTCOM POSTAGE|   569.77|\n|DOTCOM POSTAGE|   607.49|\n+--------------+---------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533701030904_-862387901",
      "id": "20180808-040350_2087761205",
      "dateCreated": "Aug 8, 2018 4:03:50 AM",
      "dateStarted": "Aug 8, 2018 4:05:25 AM",
      "dateFinished": "Aug 8, 2018 4:05:26 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Working with Numbers\n\n- When working with big data, the second most common task you will do after filtering things is counting things\n\n\n#### let’s imagine that we found out that we mis-recorded the quantity in our retail dataset and the true quantity is equal to (the current quantity * the unit price)2 + 5.",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:33:42 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eWorking with Numbers\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhen working with big data, the second most common task you will do after filtering things is counting things\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533701108903_1026652577",
      "id": "20180808-040508_2089107573",
      "dateCreated": "Aug 8, 2018 4:05:08 AM",
      "dateStarted": "Aug 8, 2018 4:32:40 AM",
      "dateFinished": "Aug 8, 2018 4:32:40 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{expr, pow}\n\nval fabricatedQuantity \u003d pow((col(\"Quantity\") * col(\"UnitPrice\")), 2) + 5\n\ndf.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\"))\n  .show(5)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:39:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{expr, pow}\nfabricatedQuantity: org.apache.spark.sql.Column \u003d (POWER((Quantity * UnitPrice), 2.0) + 5)\n+----------+------------------+\n|CustomerId|      realQuantity|\n+----------+------------------+\n|   17850.0|239.08999999999997|\n|   17850.0|          418.7156|\n|   17850.0|             489.0|\n|   17850.0|          418.7156|\n|   17850.0|          418.7156|\n+----------+------------------+\nonly showing top 5 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533702760408_2085075423",
      "id": "20180808-043240_848820343",
      "dateCreated": "Aug 8, 2018 4:32:40 AM",
      "dateStarted": "Aug 8, 2018 4:39:50 AM",
      "dateFinished": "Aug 8, 2018 4:39:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.selectExpr(  \"CustomerId\",  \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\")\n  .show(2)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:41:59 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----------+------------------+\n|CustomerId|      realQuantity|\n+----------+------------------+\n|   17850.0|239.08999999999997|\n|   17850.0|          418.7156|\n+----------+------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533703122740_-1057667964",
      "id": "20180808-043842_288160318",
      "dateCreated": "Aug 8, 2018 4:38:42 AM",
      "dateStarted": "Aug 8, 2018 4:41:43 AM",
      "dateFinished": "Aug 8, 2018 4:41:50 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{round, bround}\n\ndf.select(round(col(\"UnitPrice\"), 1).alias(\"rounded\"), col(\"UnitPrice\"))\n  .show(5)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:44:14 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{round, bround}\n+-------+---------+\n|rounded|UnitPrice|\n+-------+---------+\n|    2.6|     2.55|\n|    3.4|     3.39|\n|    2.8|     2.75|\n|    3.4|     3.39|\n|    3.4|     3.39|\n+-------+---------+\nonly showing top 5 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533703303341_606464618",
      "id": "20180808-044143_213506087",
      "dateCreated": "Aug 8, 2018 4:41:43 AM",
      "dateStarted": "Aug 8, 2018 4:44:14 AM",
      "dateFinished": "Aug 8, 2018 4:44:17 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### correlation of two columns\n\n- we can see the Pearson correlation coefficient for two columns to see if cheaper things are typically bought in greater quantities. \n- We can do this through a function as well as through the DataFrame statistic methods:",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:45:28 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003ecorrelation of two columns\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe can see the Pearson correlation coefficient for two columns to see if cheaper things are typically bought in greater quantities.\u003c/li\u003e\n\u003cli\u003eWe can do this through a function as well as through the DataFrame statistic methods:\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533703454817_1425023099",
      "id": "20180808-044414_1097207121",
      "dateCreated": "Aug 8, 2018 4:44:14 AM",
      "dateStarted": "Aug 8, 2018 4:45:28 AM",
      "dateFinished": "Aug 8, 2018 4:45:28 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{corr}\n\ndf.stat.corr(\"Quantity\", \"UnitPrice\")\ndf.select(corr(\"Quantity\", \"UnitPrice\"))\n  .show(5)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:47:48 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.corr\nres52: Double \u003d -0.04112314436835551\n+-------------------------+\n|corr(Quantity, UnitPrice)|\n+-------------------------+\n|     -0.04112314436835551|\n+-------------------------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533703530343_1519979456",
      "id": "20180808-044530_411900469",
      "dateCreated": "Aug 8, 2018 4:45:30 AM",
      "dateStarted": "Aug 8, 2018 4:47:41 AM",
      "dateFinished": "Aug 8, 2018 4:47:43 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### compute summary statistics \n\n- We can use the describe method to calculate summary statistics. \n- This will take all numeric columns and calculate the count, mean, standard deviation, min, and max",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:49:31 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003ecompute summary statistics\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWe can use the describe method to calculate summary statistics.\u003c/li\u003e\n\u003cli\u003eThis will take all numeric columns and calculate the count, mean, standard deviation, min, and max\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533703528472_2053626181",
      "id": "20180808-044528_80462706",
      "dateCreated": "Aug 8, 2018 4:45:28 AM",
      "dateStarted": "Aug 8, 2018 4:49:31 AM",
      "dateFinished": "Aug 8, 2018 4:49:31 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.describe().show()\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:49:45 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533703771818_1860419292",
      "id": "20180808-044931_1417871983",
      "dateCreated": "Aug 8, 2018 4:49:31 AM",
      "dateStarted": "Aug 8, 2018 4:49:45 AM",
      "dateFinished": "Aug 8, 2018 4:49:49 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- There are a number of statistical functions available in the StatFunctions Package (accessible using stat as we see in the code block below). \n- These are DataFrame methods that you can use to calculate a variety of different things. For instance, you can calculate either exact or approximate quantiles, cross-tabs, frequent item pairs",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:55:21 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cul\u003e\n\u003cli\u003eThere are a number of statistical functions available in the StatFunctions Package (accessible using stat as we see in the code block below).\u003c/li\u003e\n\u003cli\u003eThese are DataFrame methods that you can use to calculate a variety of different things. For instance, you can calculate either exact or approximate quantiles of your data using the approxQuantile method\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533703785618_-717767764",
      "id": "20180808-044945_813492401",
      "dateCreated": "Aug 8, 2018 4:49:45 AM",
      "dateStarted": "Aug 8, 2018 4:52:12 AM",
      "dateFinished": "Aug 8, 2018 4:52:12 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val quantileProbs \u003d Array(0.5)\nval relError \u003d 0.05\n\ndf.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:54:52 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "quantileProbs: Array[Double] \u003d Array(0.5)\nrelError: Double \u003d 0.05\nres62: Array[Double] \u003d Array(2.51)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533703932763_1397951637",
      "id": "20180808-045212_2046651678",
      "dateCreated": "Aug 8, 2018 4:52:12 AM",
      "dateStarted": "Aug 8, 2018 4:54:52 AM",
      "dateFinished": "Aug 8, 2018 4:54:52 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.stat.crosstab(\"StockCode\", \"Quantity\").show()",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:55:24 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n|             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n|             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n|             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n|             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533704010156_-482408183",
      "id": "20180808-045330_2079500363",
      "dateCreated": "Aug 8, 2018 4:53:30 AM",
      "dateStarted": "Aug 8, 2018 4:55:24 AM",
      "dateFinished": "Aug 8, 2018 4:55:32 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.stat.freqItems(Seq(\"StockCode\", \"Quantity\")).show()",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:59:01 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+--------------------+\n| StockCode_freqItems|  Quantity_freqItems|\n+--------------------+--------------------+\n|[90214E, 20728, 2...|[200, 128, 23, 32...|\n+--------------------+--------------------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533704124613_308436342",
      "id": "20180808-045524_341382899",
      "dateCreated": "Aug 8, 2018 4:55:24 AM",
      "dateStarted": "Aug 8, 2018 4:59:01 AM",
      "dateFinished": "Aug 8, 2018 4:59:02 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n#### we can also add a unique ID to each row by using the function monotonically_increasing_id. This function generates a unique value for each row, starting with 0",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:59:57 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1533704383271_817442168",
      "id": "20180808-045943_868444833",
      "dateCreated": "Aug 8, 2018 4:59:43 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.monotonically_increasing_id\ndf.select(monotonically_increasing_id()).show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 4:59:20 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.monotonically_increasing_id\n+-----------------------------+\n|monotonically_increasing_id()|\n+-----------------------------+\n|                            0|\n|                            1|\n+-----------------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533704150476_-1820966575",
      "id": "20180808-045550_395538635",
      "dateCreated": "Aug 8, 2018 4:55:50 AM",
      "dateStarted": "Aug 8, 2018 4:59:20 AM",
      "dateFinished": "Aug 8, 2018 4:59:21 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Working with Strings\n- String manipulation shows up in nearly every data flow.\n- You might be manipulating log files performing regular expression extraction or substitution, or checking for simple string existence, or making all strings uppercase or lowercase.\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:16:39 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eWorking with Strings\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eString manipulation shows up in nearly every data flow.\u003c/li\u003e\n\u003cli\u003eYou might be manipulating log files performing regular expression extraction or substitution, or checking for simple string existence, or making all strings uppercase or lowercase.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533704360903_-1786969109",
      "id": "20180808-045920_1697200325",
      "dateCreated": "Aug 8, 2018 4:59:20 AM",
      "dateStarted": "Aug 8, 2018 5:16:39 PM",
      "dateFinished": "Aug 8, 2018 5:16:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{initcap}\n\ndf.select(initcap(col(\"Description\")))\n  .show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:44:41 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.initcap\n+--------------------+\n|initcap(Description)|\n+--------------------+\n|White Hanging Hea...|\n| White Metal Lantern|\n+--------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533748599079_1806988243",
      "id": "20180808-171639_1766752602",
      "dateCreated": "Aug 8, 2018 5:16:39 PM",
      "dateStarted": "Aug 8, 2018 5:44:41 PM",
      "dateFinished": "Aug 8, 2018 5:44:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{lower, upper}\n\ndf.select(col(\"Description\"), \n         lower(col(\"Description\")), \n         upper(col(\"Description\")))\n  .show(5)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:28:39 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{lower, upper}\n+--------------------+--------------------+--------------------+\n|         Description|  lower(Description)|  upper(Description)|\n+--------------------+--------------------+--------------------+\n|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|\n| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|\n|CREAM CUPID HEART...|cream cupid heart...|CREAM CUPID HEART...|\n|KNITTED UNION FLA...|knitted union fla...|KNITTED UNION FLA...|\n|RED WOOLLY HOTTIE...|red woolly hottie...|RED WOOLLY HOTTIE...|\n+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533748655725_-1396923117",
      "id": "20180808-171735_1516067376",
      "dateCreated": "Aug 8, 2018 5:17:35 PM",
      "dateStarted": "Aug 8, 2018 5:28:39 PM",
      "dateFinished": "Aug 8, 2018 5:28:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Regular Expressions\n- Spark takes advantage of the complete power of Java regular expressions. \n- There are two key functions in Spark that you’ll need in order to perform regular expression tasks: regexp_extract and regexp_replace. \n- These functions extract values and replace values, respectively.",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:23:44 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eRegular Expressions\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSpark takes advantage of the complete power of Java regular expressions.\u003c/li\u003e\n\u003cli\u003eThere are two key functions in Spark that you’ll need in order to perform regular expression tasks: regexp_extract and regexp_replace.\u003c/li\u003e\n\u003cli\u003eThese functions extract values and replace values, respectively.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533748894375_-237168983",
      "id": "20180808-172134_644103233",
      "dateCreated": "Aug 8, 2018 5:21:34 PM",
      "dateStarted": "Aug 8, 2018 5:23:44 PM",
      "dateFinished": "Aug 8, 2018 5:23:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.regexp_replace\n\nval simpleColors \u003d Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n\nval regexString \u003d simpleColors.map(_.toUpperCase).mkString(\"|\")\n\n\ndf.select(regexp_replace(col(\"Description\"), regexString, \"COLOR\").alias(\"color_clean\"), col(\"Description\"))\n  .show(5)\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:29:09 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.regexp_replace\nsimpleColors: Seq[String] \u003d List(black, white, red, green, blue)\nregexString: String \u003d BLACK|WHITE|RED|GREEN|BLUE\n+--------------------+--------------------+\n|         color_clean|         Description|\n+--------------------+--------------------+\n|COLOR HANGING HEA...|WHITE HANGING HEA...|\n| COLOR METAL LANTERN| WHITE METAL LANTERN|\n|CREAM CUPID HEART...|CREAM CUPID HEART...|\n|KNITTED UNION FLA...|KNITTED UNION FLA...|\n|COLOR WOOLLY HOTT...|RED WOOLLY HOTTIE...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533749024469_1326429766",
      "id": "20180808-172344_1943299747",
      "dateCreated": "Aug 8, 2018 5:23:44 PM",
      "dateStarted": "Aug 8, 2018 5:29:09 PM",
      "dateFinished": "Aug 8, 2018 5:29:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.regexp_extract\n\nval colors \u003d Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n\nval regexString \u003d colors.map(_.toUpperCase).mkString(\"(\", \"|\", \")\")\n\ndf.select(regexp_extract(col(\"Description\"), regexString, 1).alias(\"color_clean\"), col(\"Description\"))\n  .show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:38:38 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.regexp_extract\ncolors: Seq[String] \u003d List(black, white, red, green, blue)\nregexString: String \u003d (BLACK|WHITE|RED|GREEN|BLUE)\n+-----------+--------------------+\n|color_clean|         Description|\n+-----------+--------------------+\n|      WHITE|WHITE HANGING HEA...|\n|      WHITE| WHITE METAL LANTERN|\n+-----------+--------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533749767504_1729903835",
      "id": "20180808-173607_303721850",
      "dateCreated": "Aug 8, 2018 5:36:07 PM",
      "dateStarted": "Aug 8, 2018 5:38:38 PM",
      "dateFinished": "Aug 8, 2018 5:38:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Translate\n- Replace given characters with other characters. Building this as a regular expression could be tedious, so Spark also provides the translate function to replace these values. \n- This is done at the character level and will replace all instances of a character with the indexed character in the replacement string:",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:32:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eTranslate\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eReplace given characters with other characters. Building this as a regular expression could be tedious, so Spark also provides the translate function to replace these values.\u003c/li\u003e\n\u003cli\u003eThis is done at the character level and will replace all instances of a character with the indexed character in the replacement string:\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533749302989_418236827",
      "id": "20180808-172822_1310241818",
      "dateCreated": "Aug 8, 2018 5:28:22 PM",
      "dateStarted": "Aug 8, 2018 5:32:15 PM",
      "dateFinished": "Aug 8, 2018 5:32:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.translate\n\ndf.select(translate(col(\"Description\"), \"LEET\", \"1337\"), col(\"Description\"))\n  .show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:35:17 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.translate\n+----------------------------------+--------------------+\n|translate(Description, LEET, 1337)|         Description|\n+----------------------------------+--------------------+\n|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n+----------------------------------+--------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533749535033_-597379321",
      "id": "20180808-173215_1718544066",
      "dateCreated": "Aug 8, 2018 5:32:15 PM",
      "dateStarted": "Aug 8, 2018 5:35:17 PM",
      "dateFinished": "Aug 8, 2018 5:35:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Sometimes, rather than extracting values, we simply want to check for their existence. We can do this with the contains method on each column. This will return a Boolean declaring whether the value you specify is in the column’s string\n\nval containsBlack \u003d col(\"Description\").contains(\"BLACK\")\nval containsWhite \u003d col(\"Description\").contains(\"WHITE\")\n\ndf.withColumn(\"hasSimpleColor\", containsBlack.or(containsWhite))\n  .where(\"hasSimpleColor\")\n  .select(\"Description\")\n  .show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:43:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "containsBlack: org.apache.spark.sql.Column \u003d contains(Description, BLACK)\ncontainsWhite: org.apache.spark.sql.Column \u003d contains(Description, WHITE)\n+--------------------+\n|         Description|\n+--------------------+\n|WHITE HANGING HEA...|\n| WHITE METAL LANTERN|\n+--------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533749971702_1342970679",
      "id": "20180808-173931_678801730",
      "dateCreated": "Aug 8, 2018 5:39:31 PM",
      "dateStarted": "Aug 8, 2018 5:42:32 PM",
      "dateFinished": "Aug 8, 2018 5:42:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Working with Dates and Timestamps\n\n- Dates and times are a constant challenge in programming languages and databases. It’s always necessary to keep track of timezones and ensure that formats are correct and valid\n- Another common “gotcha” is that Spark’s TimestampType class supports only second-level precision, which means that if you’re going to be working with milliseconds or microseconds, you’ll need to work around this problem by potentially operating on them as longs.\n- Any more precision when coercing to a TimestampType will be removed.",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:48:33 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eWorking with Dates and Timestamps\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDates and times are a constant challenge in programming languages and databases. It’s always necessary to keep track of timezones and ensure that formats are correct and valid\u003c/li\u003e\n\u003cli\u003eAnother common “gotcha” is that Spark’s TimestampType class supports only second-level precision, which means that if you’re going to be working with milliseconds or microseconds, you’ll need to work around this problem by potentially operating on them as longs.\u003c/li\u003e\n\u003cli\u003eAny more precision when coercing to a TimestampType will be removed.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533750126134_-1117898086",
      "id": "20180808-174206_594577046",
      "dateCreated": "Aug 8, 2018 5:42:06 PM",
      "dateStarted": "Aug 8, 2018 5:48:33 PM",
      "dateFinished": "Aug 8, 2018 5:48:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{current_date, current_timestamp}\n\nval dateDF \u003d spark.range(10)\n                  .withColumn(\"today\", current_date())\n                  .withColumn(\"now\", current_timestamp())\n\ndateDF.show()\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:58:07 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{current_date, current_timestamp}\ndateDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, today: date ... 1 more field]\n+---+----------+--------------------+\n| id|     today|                 now|\n+---+----------+--------------------+\n|  0|2018-08-08|2018-08-08 17:58:...|\n|  1|2018-08-08|2018-08-08 17:58:...|\n|  2|2018-08-08|2018-08-08 17:58:...|\n|  3|2018-08-08|2018-08-08 17:58:...|\n|  4|2018-08-08|2018-08-08 17:58:...|\n|  5|2018-08-08|2018-08-08 17:58:...|\n|  6|2018-08-08|2018-08-08 17:58:...|\n|  7|2018-08-08|2018-08-08 17:58:...|\n|  8|2018-08-08|2018-08-08 17:58:...|\n|  9|2018-08-08|2018-08-08 17:58:...|\n+---+----------+--------------------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533750513127_-49666574",
      "id": "20180808-174833_20103431",
      "dateCreated": "Aug 8, 2018 5:48:33 PM",
      "dateStarted": "Aug 8, 2018 5:58:07 PM",
      "dateFinished": "Aug 8, 2018 5:58:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{date_add, date_sub}\n\ndateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5))\n      .show(1)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 5:59:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{date_add, date_sub}\n+------------------+------------------+\n|date_sub(today, 5)|date_add(today, 5)|\n+------------------+------------------+\n|        2018-08-03|        2018-08-13|\n+------------------+------------------+\nonly showing top 1 row\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751063617_-117785742",
      "id": "20180808-175743_2069058742",
      "dateCreated": "Aug 8, 2018 5:57:43 PM",
      "dateStarted": "Aug 8, 2018 5:59:01 PM",
      "dateFinished": "Aug 8, 2018 5:59:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{datediff, months_between, to_date}\n\ndateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n      .select(datediff(col(\"week_ago\"), col(\"today\")))\n      .show(1)\n      \ndateDF.select(to_date(lit(\"2016-01-01\")).alias(\"start\"), to_date(lit(\"2017-05-22\")).alias(\"end\"))\n      .select(months_between(col(\"start\"), col(\"end\"))).show(1)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:02:21 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{datediff, months_between, to_date}\n+-------------------------+\n|datediff(week_ago, today)|\n+-------------------------+\n|                       -7|\n+-------------------------+\nonly showing top 1 row\n\n+--------------------------+\n|months_between(start, end)|\n+--------------------------+\n|              -16.67741935|\n+--------------------------+\nonly showing top 1 row\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751141456_-1498362198",
      "id": "20180808-175901_1081094711",
      "dateCreated": "Aug 8, 2018 5:59:01 PM",
      "dateStarted": "Aug 8, 2018 6:02:21 PM",
      "dateFinished": "Aug 8, 2018 6:02:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.to_date\n\nval dateFormat \u003d \"yyyy-dd-MM\"\n\nval cleanDateDF \u003d spark.range(1).select(\n    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n\ncleanDateDF.show()\n\ncleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()\n\ncleanDateDF.filter(col(\"date2\") \u003e lit(\"2017-12-12\")).show()\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:04:46 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.to_date\ndateFormat: String \u003d yyyy-dd-MM\ncleanDateDF: org.apache.spark.sql.DataFrame \u003d [date: date, date2: date]\n+----------+----------+\n|      date|     date2|\n+----------+----------+\n|2017-11-12|2017-12-20|\n+----------+----------+\n\n+----------------------------------+\n|to_timestamp(`date`, \u0027yyyy-dd-MM\u0027)|\n+----------------------------------+\n|               2017-11-12 00:00:00|\n+----------------------------------+\n\n+----------+----------+\n|      date|     date2|\n+----------+----------+\n|2017-11-12|2017-12-20|\n+----------+----------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751341219_-993127240",
      "id": "20180808-180221_1830107198",
      "dateCreated": "Aug 8, 2018 6:02:21 PM",
      "dateStarted": "Aug 8, 2018 6:04:46 PM",
      "dateFinished": "Aug 8, 2018 6:04:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Working with Nulls in Data\n\n- As a best practice, you should always use nulls to represent missing or empty data in your DataFrames. \n- Spark can optimize working with null values more than it can if you use empty strings or other values. \n- The primary way of interacting with null values, at DataFrame scale, is to use the .na subpackage on a DataFrame\n- There are two things you can do with null values: you can explicitly drop nulls or you can fill them with a value (globally or on a per-column basis)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:07:20 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eWorking with Nulls in Data\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAs a best practice, you should always use nulls to represent missing or empty data in your DataFrames.\u003c/li\u003e\n\u003cli\u003eSpark can optimize working with null values more than it can if you use empty strings or other values.\u003c/li\u003e\n\u003cli\u003eThe primary way of interacting with null values, at DataFrame scale, is to use the .na subpackage on a DataFrame\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751414733_-1703065582",
      "id": "20180808-180334_1884825442",
      "dateCreated": "Aug 8, 2018 6:03:34 PM",
      "dateStarted": "Aug 8, 2018 6:06:38 PM",
      "dateFinished": "Aug 8, 2018 6:06:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Coalesce\n- Spark includes a function to allow you to select the first non-null value from a set of columns by using the coalesce function.",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:07:53 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eCoalesce\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSpark includes a function to allow you to select the first non-null value from a set of columns by using the coalesce function.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751598243_477350763",
      "id": "20180808-180638_1842534143",
      "dateCreated": "Aug 8, 2018 6:06:38 PM",
      "dateStarted": "Aug 8, 2018 6:07:53 PM",
      "dateFinished": "Aug 8, 2018 6:07:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.coalesce\n\ndf.select(coalesce(col(\"Description\"), col(\"CustomerId\")))\n  .show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:08:43 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.coalesce\n+---------------------------------+\n|coalesce(Description, CustomerId)|\n+---------------------------------+\n|             WHITE HANGING HEA...|\n|              WHITE METAL LANTERN|\n+---------------------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751673775_135632443",
      "id": "20180808-180753_1888946655",
      "dateCreated": "Aug 8, 2018 6:07:53 PM",
      "dateStarted": "Aug 8, 2018 6:08:43 PM",
      "dateFinished": "Aug 8, 2018 6:08:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### drop\n\n- The simplest function is drop, which removes rows that contain nulls. The default is to drop any row in which any value is null:",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:09:19 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003edrop\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe simplest function is drop, which removes rows that contain nulls. The default is to drop any row in which any value is null:\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751713402_-2076258335",
      "id": "20180808-180833_273067693",
      "dateCreated": "Aug 8, 2018 6:08:33 PM",
      "dateStarted": "Aug 8, 2018 6:09:19 PM",
      "dateFinished": "Aug 8, 2018 6:09:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\ndf.na.drop()\n\ndf.na.drop(\"any\")\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:13:29 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res94: Long \u003d 3108\nres96: org.apache.spark.sql.DataFrame \u003d [InvoiceNo: string, StockCode: string ... 6 more fields]\nres98: Long \u003d 3108\nres100: org.apache.spark.sql.DataFrame \u003d [InvoiceNo: string, StockCode: string ... 6 more fields]\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n|   536367|    22745|POPPY\u0027S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n|   536367|    22748|POPPY\u0027S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751759421_-994451697",
      "id": "20180808-180919_896116768",
      "dateCreated": "Aug 8, 2018 6:09:19 PM",
      "dateStarted": "Aug 8, 2018 6:10:08 PM",
      "dateFinished": "Aug 8, 2018 6:10:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### fill\n- Using the fill function, you can fill one or more columns with a set of values. This can be done by specifying a map—that is a particular value and a set of columns.",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:11:20 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003efill\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUsing the fill function, you can fill one or more columns with a set of values. This can be done by specifying a map—that is a particular value and a set of columns.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751772839_-631218087",
      "id": "20180808-180932_344151389",
      "dateCreated": "Aug 8, 2018 6:09:32 PM",
      "dateStarted": "Aug 8, 2018 6:11:20 PM",
      "dateFinished": "Aug 8, 2018 6:11:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.na.fill(\"All Null values become this string\")\n\ndf.na.fill(5, Seq(\"StockCode\", \"InvoiceNo\"))\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:13:47 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res103: org.apache.spark.sql.DataFrame \u003d [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751880071_758615889",
      "id": "20180808-181120_1257710302",
      "dateCreated": "Aug 8, 2018 6:11:20 PM",
      "dateStarted": "Aug 8, 2018 6:11:23 PM",
      "dateFinished": "Aug 8, 2018 6:11:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val fillColValues \u003d Map(\"StockCode\" -\u003e 5, \"Description\" -\u003e \"No Value\")\ndf.na.fill(fillColValues)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:14:23 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "fillColValues: scala.collection.immutable.Map[String,Any] \u003d Map(StockCode -\u003e 5, Description -\u003e No Value)\nres104: org.apache.spark.sql.DataFrame \u003d [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533751883791_-2111241481",
      "id": "20180808-181123_803185772",
      "dateCreated": "Aug 8, 2018 6:11:23 PM",
      "dateStarted": "Aug 8, 2018 6:14:24 PM",
      "dateFinished": "Aug 8, 2018 6:14:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### replace\n\n- In addition to replacing null values like we did with drop and fill, there are more flexible options that you can use with more than just null values. \n- Probably the most common use case is to replace all values in a certain column according to their current value",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:15:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003ereplace\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIn addition to replacing null values like we did with drop and fill, there are more flexible options that you can use with more than just null values.\u003c/li\u003e\n\u003cli\u003eProbably the most common use case is to replace all values in a certain column according to their current value\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752063938_1276966247",
      "id": "20180808-181423_430408989",
      "dateCreated": "Aug 8, 2018 6:14:23 PM",
      "dateStarted": "Aug 8, 2018 6:15:22 PM",
      "dateFinished": "Aug 8, 2018 6:15:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.na.replace(\"Description\", Map(\"\" -\u003e \"UNKNOWN\"))",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:16:25 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res105: org.apache.spark.sql.DataFrame \u003d [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752122581_1190044168",
      "id": "20180808-181522_1423954258",
      "dateCreated": "Aug 8, 2018 6:15:22 PM",
      "dateStarted": "Aug 8, 2018 6:16:25 PM",
      "dateFinished": "Aug 8, 2018 6:16:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Working with Complex Types\n\n- Complex types can help you organize and structure your data in ways that make more sense for the problem that you are hoping to solve. \n- There are three kinds of complex types: structs, arrays, and maps.\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:16:59 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eWorking with Complex Types\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eComplex types can help you organize and structure your data in ways that make more sense for the problem that you are hoping to solve.\u003c/li\u003e\n\u003cli\u003eThere are three kinds of complex types: structs, arrays, and maps.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752185144_301567012",
      "id": "20180808-181625_1764604166",
      "dateCreated": "Aug 8, 2018 6:16:25 PM",
      "dateStarted": "Aug 8, 2018 6:16:59 PM",
      "dateFinished": "Aug 8, 2018 6:16:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.struct\n\nval complexDF \u003d df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\ncomplexDF.show(5)\n\ncomplexDF.select(\"complex.Description\")\n         .show(2)\n         \ncomplexDF.select(col(\"complex\").getField(\"Description\"))\n         .show(2)\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:18:59 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.struct\ncomplexDF: org.apache.spark.sql.DataFrame \u003d [complex: struct\u003cDescription: string, InvoiceNo: string\u003e]\n+--------------------+\n|             complex|\n+--------------------+\n|[WHITE HANGING HE...|\n|[WHITE METAL LANT...|\n|[CREAM CUPID HEAR...|\n|[KNITTED UNION FL...|\n|[RED WOOLLY HOTTI...|\n+--------------------+\nonly showing top 5 rows\n\n+--------------------+\n|         Description|\n+--------------------+\n|WHITE HANGING HEA...|\n| WHITE METAL LANTERN|\n+--------------------+\nonly showing top 2 rows\n\n+--------------------+\n| complex.Description|\n+--------------------+\n|WHITE HANGING HEA...|\n| WHITE METAL LANTERN|\n+--------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752219258_-134307519",
      "id": "20180808-181659_1875397982",
      "dateCreated": "Aug 8, 2018 6:16:59 PM",
      "dateStarted": "Aug 8, 2018 6:18:28 PM",
      "dateFinished": "Aug 8, 2018 6:18:30 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Arrays\n\ndf.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\n  .selectExpr(\"array_col[0]\")\n  .show(2)\n \n// Array length\n\nimport org.apache.spark.sql.functions.size\n\ndf.select(size(split(col(\"Description\"), \" \"))).show(2) // shows 5 and 3\n\n// Array Contains\n\nimport org.apache.spark.sql.functions.array_contains\n\ndf.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\"))\n  .show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:21:02 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------+\n|array_col[0]|\n+------------+\n|       WHITE|\n|       WHITE|\n+------------+\nonly showing top 2 rows\n\nimport org.apache.spark.sql.functions.size\n+---------------------------+\n|size(split(Description,  ))|\n+---------------------------+\n|                          5|\n|                          3|\n+---------------------------+\nonly showing top 2 rows\n\nimport org.apache.spark.sql.functions.array_contains\n+--------------------------------------------+\n|array_contains(split(Description,  ), WHITE)|\n+--------------------------------------------+\n|                                        true|\n|                                        true|\n+--------------------------------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752262331_1037899786",
      "id": "20180808-181742_375816969",
      "dateCreated": "Aug 8, 2018 6:17:42 PM",
      "dateStarted": "Aug 8, 2018 6:21:02 PM",
      "dateFinished": "Aug 8, 2018 6:21:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### explode\n- The explode function takes a column that consists of arrays and creates one row (with the rest of the values duplicated) per value in the array\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43856464-3c546a28-9b66-11e8-96f2-f03b46c9d9d3.png)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:23:40 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eexplode\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe explode function takes a column that consists of arrays and creates one row (with the rest of the values duplicated) per value in the array\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43856464-3c546a28-9b66-11e8-96f2-f03b46c9d9d3.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752370663_949007466",
      "id": "20180808-181930_795261618",
      "dateCreated": "Aug 8, 2018 6:19:30 PM",
      "dateStarted": "Aug 8, 2018 6:23:40 PM",
      "dateFinished": "Aug 8, 2018 6:23:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{split, explode}\n\ndf.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\n  .withColumn(\"exploded\", explode(col(\"splitted\")))\n  .select(\"Description\", \"InvoiceNo\", \"exploded\")\n  .show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:24:31 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{split, explode}\n+--------------------+---------+--------+\n|         Description|InvoiceNo|exploded|\n+--------------------+---------+--------+\n|WHITE HANGING HEA...|   536365|   WHITE|\n|WHITE HANGING HEA...|   536365| HANGING|\n+--------------------+---------+--------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752620593_2145144179",
      "id": "20180808-182340_1380356564",
      "dateCreated": "Aug 8, 2018 6:23:40 PM",
      "dateStarted": "Aug 8, 2018 6:24:31 PM",
      "dateFinished": "Aug 8, 2018 6:24:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Maps\n\n- Maps are created by using the map function and key-value pairs of columns. \n- You then can select them just like you might select from an array:",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:25:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eMaps\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eMaps are created by using the map function and key-value pairs of columns.\u003c/li\u003e\n\u003cli\u003eYou then can select them just like you might select from an array:\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752671316_419758256",
      "id": "20180808-182431_771260482",
      "dateCreated": "Aug 8, 2018 6:24:31 PM",
      "dateStarted": "Aug 8, 2018 6:25:15 PM",
      "dateFinished": "Aug 8, 2018 6:25:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.map\n\ndf.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n  .show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:25:55 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.map\n+--------------------+\n|         complex_map|\n+--------------------+\n|[WHITE HANGING HE...|\n|[WHITE METAL LANT...|\n+--------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752715540_814788108",
      "id": "20180808-182515_42352513",
      "dateCreated": "Aug 8, 2018 6:25:15 PM",
      "dateStarted": "Aug 8, 2018 6:25:25 PM",
      "dateFinished": "Aug 8, 2018 6:25:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n  .selectExpr(\"complex_map[\u0027WHITE METAL LANTERN\u0027]\")\n  .show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:25:59 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------------------+\n|complex_map[WHITE METAL LANTERN]|\n+--------------------------------+\n|                            null|\n|                          536365|\n+--------------------------------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752725508_-1653760835",
      "id": "20180808-182525_1267036351",
      "dateCreated": "Aug 8, 2018 6:25:25 PM",
      "dateStarted": "Aug 8, 2018 6:25:59 PM",
      "dateFinished": "Aug 8, 2018 6:26:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n  .selectExpr(\"explode(complex_map)\")\n  .show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:39:59 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+------+\n|                 key| value|\n+--------------------+------+\n|WHITE HANGING HEA...|536365|\n| WHITE METAL LANTERN|536365|\n+--------------------+------+\nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752759516_-1742591859",
      "id": "20180808-182559_1132766071",
      "dateCreated": "Aug 8, 2018 6:25:59 PM",
      "dateStarted": "Aug 8, 2018 6:39:59 PM",
      "dateFinished": "Aug 8, 2018 6:40:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### User-Defined Functions\n- One of the most powerful things that you can do in Spark is define your own functions. \n- These user-defined functions (UDFs) make it possible for you to write your own custom transformations using Python or Scala and even use external libraries. \n- UDFs can take and return one or more columns as input. \n- Spark UDFs are incredibly powerful because you can write them in several different programming languages; you do not need to create them in an esoteric format or domain-specific language. \n- They’re just functions that operate on the data, record by record. \n- By default, these functions are registered as temporary functions to be used in that specific SparkSession or Context.\n\n\n#### Although you can write UDFs in Scala, Python, or Java, there are performance considerations that you should be aware of.\n\n- we need to register them with Spark so that we can use them on all of our worker machines.\n- Spark will serialize the function on the driver and transfer it over the network to all executor processes. This happens regardless of language.\n- When you use the function, there are essentially two different things that occur. If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM). \n- This means that there will be little performance penalty aside from the fact that you can’t take advantage of code generation capabilities that Spark has for built-in functions\n\n- If the function is written in Python, something quite different happens. \n- Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43856959-9398a38e-9b67-11e8-9efb-2896f449832e.png)",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:33:26 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eUser-Defined Functions\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOne of the most powerful things that you can do in Spark is define your own functions.\u003c/li\u003e\n\u003cli\u003eThese user-defined functions (UDFs) make it possible for you to write your own custom transformations using Python or Scala and even use external libraries.\u003c/li\u003e\n\u003cli\u003eUDFs can take and return one or more columns as input.\u003c/li\u003e\n\u003cli\u003eSpark UDFs are incredibly powerful because you can write them in several different programming languages; you do not need to create them in an esoteric format or domain-specific language.\u003c/li\u003e\n\u003cli\u003eThey’re just functions that operate on the data, record by record.\u003c/li\u003e\n\u003cli\u003eBy default, these functions are registered as temporary functions to be used in that specific SparkSession or Context.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eAlthough you can write UDFs in Scala, Python, or Java, there are performance considerations that you should be aware of.\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003ewe need to register them with Spark so that we can use them on all of our worker machines.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eSpark will serialize the function on the driver and transfer it over the network to all executor processes. This happens regardless of language.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhen you use the function, there are essentially two different things that occur. If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThis means that there will be little performance penalty aside from the fact that you can’t take advantage of code generation capabilities that Spark has for built-in functions\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIf the function is written in Python, something quite different happens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eSpark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43856959-9398a38e-9b67-11e8-9efb-2896f449832e.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533752782662_-361096476",
      "id": "20180808-182622_633582405",
      "dateCreated": "Aug 8, 2018 6:26:22 PM",
      "dateStarted": "Aug 8, 2018 6:33:15 PM",
      "dateFinished": "Aug 8, 2018 6:33:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.udf\n\nval udfExampleDF \u003d spark.range(5).toDF(\"num\")\n\ndef power3(number:Double):Double \u003d number * number * number\n\npower3(2.0)\n\nval power3udf \u003d udf(power3(_:Double):Double)\n\nudfExampleDF.select(power3udf(col(\"num\"))).show()\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:34:37 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1533753019704_-247859351",
      "id": "20180808-183019_2012627590",
      "dateCreated": "Aug 8, 2018 6:30:19 PM",
      "dateStarted": "Aug 8, 2018 6:34:37 PM",
      "dateFinished": "Aug 8, 2018 6:34:56 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n- At this juncture, we can use this only as a DataFrame function. \n- That is to say, we can’t use it within a string expression, only on an expression.\n- However, we can also register this UDF as a Spark SQL function. This is valuable because it makes it simple to use this function within SQL as well as across languages.",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:36:06 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1533753348759_1789858957",
      "id": "20180808-183548_1545292187",
      "dateCreated": "Aug 8, 2018 6:35:48 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.udf.register(\"power3\", power3(_:Double):Double)\nudfExampleDF.selectExpr(\"power3(num)\").show(2)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 8, 2018 6:36:14 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1533753277161_1944219886",
      "id": "20180808-183437_550719054",
      "dateCreated": "Aug 8, 2018 6:34:37 PM",
      "dateStarted": "Aug 8, 2018 6:36:14 PM",
      "dateFinished": "Aug 8, 2018 6:36:20 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1533753374404_-610742673",
      "id": "20180808-183614_1371248162",
      "dateCreated": "Aug 8, 2018 6:36:14 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "working_with_different_types_of_data",
  "id": "2DM76S2ZK",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}