{
  "paragraphs": [
    {
      "text": "%md\n### Architecture of a Spark Application\n\n- The Spark driver\n    -  The driver is the process “in the driver seat” of your Spark Application. \n    -  It is the controller of the execution of a Spark Application and maintains all of the state of the Spark cluster (the state and tasks of the executors).\n    -  It must interface with the cluster manager in order to actually get physical resources and launch executors.\n    -  At the end of the day, this is just a process on a physical machine that is responsible for maintaining the state of the application running on the cluster.\n\n- The Spark executors\n    - Spark executors are the processes that perform the tasks assigned by the Spark driver. \n    - Executors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results. \n    - Each Spark Application has its own separate executor processes.\n\n- The cluster manager\n    - The Spark Driver and Executors do not exist in a void, and this is where the cluster manager comes in. \n    - The cluster manager is responsible for maintaining a cluster of machines that will run your Spark Application(s). \n    - Somewhat confusingly, a cluster manager will have its own “driver” (sometimes called master) and “worker” abstractions. \n    - The core difference is that these are tied to physical machines rather than processes (as they are in Spark)\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999202-74047dac-9e25-11e8-8336-c7e76c17a438.png)\n\n- When it comes time to actually run a Spark Application, we request resources from the cluster manager to run it. \n- Over the course of Spark Application execution, the cluster manager will be responsible for managing the underlying machines that our application is running on.\n\n### Execution Modes\n\n- An execution mode gives you the power to determine where the resources are physically located when you go to run your application. \n- You have three modes to choose from:Cluster modeClient modeLocal mode\n        - Cluster mode\n        - Client mode\n        - Local mode\n\n#### Cluster mode\n\n- Cluster mode is probably the most common way of running Spark Applications.\n- In cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager. \n- The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. \n- This means that the cluster manager is responsible for maintaining all Spark Application–related processes.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999237-17ec3932-9e26-11e8-8e47-3e846410d129.png)\n\n#### Client mode\n\n- Client mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application. \n- This means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses. \n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999239-20c21496-9e26-11e8-9c2a-8913c04a3705.png)\n\n#### Local mode\n\n- Local mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine. \n- It achieves parallelism through threads on that single machine. \n- This is a common way to learn Spark, to test your applications, or experiment iteratively with local development. \n- However, we do not use local mode for running production applications.\n\n\n### The Life Cycle of a Spark Application (Outside Spark) using spark-submit\n\n#### Client Request\n\n- The first step is for you to submit an actual application. This will be a pre-compiled JAR or library. \n- At this point, you are executing code on your local machine and you’re going to make a request to the cluster manager driver node.\n- Here, we are explicitly asking for resources for the Spark driver process only. \n- We assume that the cluster manager accepts this offer and places the driver onto a node in the cluster. \n- The client process that submitted the original job exits and the application is off and running on the cluster.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999286-2e1c219e-9e27-11e8-8f1f-8e70b7cf8028.png)\n\n\n##### To do this, you’ll run something like the following command in your terminal:\n\n```\n./bin/spark-submit \\  --class \u003cmain-class\u003e \\  --master \u003cmaster-url\u003e \\  --deploy-mode cluster \\  --conf \u003ckey\u003e\u003d\u003cvalue\u003e \\  ... # other options  \u003capplication-jar\u003e \\  [application-arguments]\n\n```\n\n#### Launch\n\n- Now that the driver process has been placed on the cluster, it begins running user code. \n- This code must include a SparkSession that initializes a Spark cluster (e.g., driver + executors). \n- The SparkSession will subsequently communicate with the cluster manager, asking it to launch Spark executor processes across the cluster. \n- The number of executors and their relevant configurations are set by the user via the command-line arguments in the original spark-submit call.\n- The cluster manager responds by launching the executor processes (assuming all goes well) and sends the relevant information about their locations to the driver process. \n- After everything is hooked up correctly, we have a “Spark Cluster”.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999322-cac43d56-9e27-11e8-99c5-5115ceecf157.png)\n\n\n#### Execution\n\n- The driver and the workers communicate among themselves, executing code and moving data around. \n- The driver schedules tasks onto each worker, and each worker responds with the status of those tasks and success or failure.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999337-27978f2e-9e28-11e8-81f7-58392f5f84b0.png)\n\n\n#### Completion\n\n- After a Spark Application completes, the driver processs exits with either success or failure. \n- The cluster manager then shuts down the executors in that Spark cluster for the driver.\n- At this point, you can see the success or failure of the Spark Application by asking the cluster manager for this information.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999339-29e23d92-9e28-11e8-985b-3dabdb97fa2d.pngg)\n\n\n### The Life Cycle of a Spark Application (Inside Spark)\n\n- This is “user-code” (the actual code that you write that defines your Spark Application). Each application is made up of one or more Spark jobs. \n- Spark jobs within an application are executed serially (unless you use threading to launch multiple actions in parallel).\n\n### The SparkSession\n\n- The first step of any Spark Application is creating a SparkSession. \n- In many interactive modes, this is done for you, but in an application, you must do it manually.\n- After you have a SparkSession, you should be able to run your Spark code. \n- From the SparkSession, you can access all of low-level and legacy contexts and configurations accordingly\n\n```\nimport org.apache.spark.sql.SparkSession\n\nval spark \u003d SparkSession.builder().appName(\"Databricks Spark Example\")\n                                  .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\n                                  .getOrCreate()\n```\n\n#### The SparkContext\n\n- A SparkContext object within the SparkSession represents the connection to the Spark cluster. \n- This class is how you communicate with some of Spark’s lower-level APIs, such as RDDs.\n- Through a SparkContext, you can create RDDs, accumulators, and broadcast variables, and you can run code on the cluster.\n- For the most part, you should not need to explicitly initialize a SparkContext.\n- you should just be able to access it through the SparkSession. \n- If you do want to, you should create it in the most general way, through the getOrCreate method:\n\n```\nimport org.apache.spark.SparkContext\n\nval sc \u003d SparkContext.getOrCreate()\n\n```\n\n### Logical instructions to physical execution\n\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999600-79450d92-9e2d-11e8-9b10-624ff5b15dc5.png)\n\n- Each spark application consists of one or more jobs (Each action triggers a spark job)\n- Each spark job contains one or more stages (Number of stages depends on how many shuffle operations need to take place)\n- Each stage contains one more tasks\n- Each task corresponds to one partition\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 7:52:47 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eArchitecture of a Spark Application\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eThe Spark driver\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe driver is the process “in the driver seat” of your Spark Application.\u003c/li\u003e\n\u003cli\u003eIt is the controller of the execution of a Spark Application and maintains all of the state of the Spark cluster (the state and tasks of the executors).\u003c/li\u003e\n\u003cli\u003eIt must interface with the cluster manager in order to actually get physical resources and launch executors.\u003c/li\u003e\n\u003cli\u003eAt the end of the day, this is just a process on a physical machine that is responsible for maintaining the state of the application running on the cluster.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThe Spark executors\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSpark executors are the processes that perform the tasks assigned by the Spark driver.\u003c/li\u003e\n\u003cli\u003eExecutors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results.\u003c/li\u003e\n\u003cli\u003eEach Spark Application has its own separate executor processes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThe cluster manager\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe Spark Driver and Executors do not exist in a void, and this is where the cluster manager comes in.\u003c/li\u003e\n\u003cli\u003eThe cluster manager is responsible for maintaining a cluster of machines that will run your Spark Application(s).\u003c/li\u003e\n\u003cli\u003eSomewhat confusingly, a cluster manager will have its own “driver” (sometimes called master) and “worker” abstractions.\u003c/li\u003e\n\u003cli\u003eThe core difference is that these are tied to physical machines rather than processes (as they are in Spark)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999202-74047dac-9e25-11e8-8336-c7e76c17a438.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhen it comes time to actually run a Spark Application, we request resources from the cluster manager to run it.\u003c/li\u003e\n\u003cli\u003eOver the course of Spark Application execution, the cluster manager will be responsible for managing the underlying machines that our application is running on.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eExecution Modes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAn execution mode gives you the power to determine where the resources are physically located when you go to run your application.\u003c/li\u003e\n\u003cli\u003eYou have three modes to choose from:Cluster modeClient modeLocal mode\u003cpre\u003e\u003ccode\u003e- Cluster mode\n- Client mode\n- Local mode\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eCluster mode\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCluster mode is probably the most common way of running Spark Applications.\u003c/li\u003e\n\u003cli\u003eIn cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager.\u003c/li\u003e\n\u003cli\u003eThe cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes.\u003c/li\u003e\n\u003cli\u003eThis means that the cluster manager is responsible for maintaining all Spark Application–related processes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999237-17ec3932-9e26-11e8-8e47-3e846410d129.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch4\u003eClient mode\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eClient mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application.\u003c/li\u003e\n\u003cli\u003eThis means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999239-20c21496-9e26-11e8-9c2a-8913c04a3705.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch4\u003eLocal mode\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eLocal mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine.\u003c/li\u003e\n\u003cli\u003eIt achieves parallelism through threads on that single machine.\u003c/li\u003e\n\u003cli\u003eThis is a common way to learn Spark, to test your applications, or experiment iteratively with local development.\u003c/li\u003e\n\u003cli\u003eHowever, we do not use local mode for running production applications.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eThe Life Cycle of a Spark Application (Outside Spark) using spark-submit\u003c/h3\u003e\n\u003ch4\u003eClient Request\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eThe first step is for you to submit an actual application. This will be a pre-compiled JAR or library.\u003c/li\u003e\n\u003cli\u003eAt this point, you are executing code on your local machine and you’re going to make a request to the cluster manager driver node.\u003c/li\u003e\n\u003cli\u003eHere, we are explicitly asking for resources for the Spark driver process only.\u003c/li\u003e\n\u003cli\u003eWe assume that the cluster manager accepts this offer and places the driver onto a node in the cluster.\u003c/li\u003e\n\u003cli\u003eThe client process that submitted the original job exits and the application is off and running on the cluster.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999286-2e1c219e-9e27-11e8-8f1f-8e70b7cf8028.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch5\u003eTo do this, you’ll run something like the following command in your terminal:\u003c/h5\u003e\n\u003cpre\u003e\u003ccode\u003e./bin/spark-submit \\  --class \u0026lt;main-class\u0026gt; \\  --master \u0026lt;master-url\u0026gt; \\  --deploy-mode cluster \\  --conf \u0026lt;key\u0026gt;\u003d\u0026lt;value\u0026gt; \\  ... # other options  \u0026lt;application-jar\u0026gt; \\  [application-arguments]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eLaunch\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eNow that the driver process has been placed on the cluster, it begins running user code.\u003c/li\u003e\n\u003cli\u003eThis code must include a SparkSession that initializes a Spark cluster (e.g., driver + executors).\u003c/li\u003e\n\u003cli\u003eThe SparkSession will subsequently communicate with the cluster manager, asking it to launch Spark executor processes across the cluster.\u003c/li\u003e\n\u003cli\u003eThe number of executors and their relevant configurations are set by the user via the command-line arguments in the original spark-submit call.\u003c/li\u003e\n\u003cli\u003eThe cluster manager responds by launching the executor processes (assuming all goes well) and sends the relevant information about their locations to the driver process.\u003c/li\u003e\n\u003cli\u003eAfter everything is hooked up correctly, we have a “Spark Cluster”.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999322-cac43d56-9e27-11e8-99c5-5115ceecf157.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch4\u003eExecution\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eThe driver and the workers communicate among themselves, executing code and moving data around.\u003c/li\u003e\n\u003cli\u003eThe driver schedules tasks onto each worker, and each worker responds with the status of those tasks and success or failure.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999337-27978f2e-9e28-11e8-81f7-58392f5f84b0.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch4\u003eCompletion\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eAfter a Spark Application completes, the driver processs exits with either success or failure.\u003c/li\u003e\n\u003cli\u003eThe cluster manager then shuts down the executors in that Spark cluster for the driver.\u003c/li\u003e\n\u003cli\u003eAt this point, you can see the success or failure of the Spark Application by asking the cluster manager for this information.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999339-29e23d92-9e28-11e8-985b-3dabdb97fa2d.pngg\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch3\u003eThe Life Cycle of a Spark Application (Inside Spark)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThis is “user-code” (the actual code that you write that defines your Spark Application). Each application is made up of one or more Spark jobs.\u003c/li\u003e\n\u003cli\u003eSpark jobs within an application are executed serially (unless you use threading to launch multiple actions in parallel).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eThe SparkSession\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe first step of any Spark Application is creating a SparkSession.\u003c/li\u003e\n\u003cli\u003eIn many interactive modes, this is done for you, but in an application, you must do it manually.\u003c/li\u003e\n\u003cli\u003eAfter you have a SparkSession, you should be able to run your Spark code.\u003c/li\u003e\n\u003cli\u003eFrom the SparkSession, you can access all of low-level and legacy contexts and configurations accordingly\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.sql.SparkSession\n\nval spark \u003d SparkSession.builder().appName(\"Databricks Spark Example\")\n                                  .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\n                                  .getOrCreate()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eThe SparkContext\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eA SparkContext object within the SparkSession represents the connection to the Spark cluster.\u003c/li\u003e\n\u003cli\u003eThis class is how you communicate with some of Spark’s lower-level APIs, such as RDDs.\u003c/li\u003e\n\u003cli\u003eThrough a SparkContext, you can create RDDs, accumulators, and broadcast variables, and you can run code on the cluster.\u003c/li\u003e\n\u003cli\u003eFor the most part, you should not need to explicitly initialize a SparkContext.\u003c/li\u003e\n\u003cli\u003eyou should just be able to access it through the SparkSession.\u003c/li\u003e\n\u003cli\u003eIf you do want to, you should create it in the most general way, through the getOrCreate method:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.SparkContext\n\nval sc \u003d SparkContext.getOrCreate()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eLogical instructions to physical execution\u003c/h3\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999600-79450d92-9e2d-11e8-9b10-624ff5b15dc5.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEach spark application consists of one or more jobs (Each action triggers a spark job)\u003c/li\u003e\n\u003cli\u003eEach spark job contains one or more stages (Number of stages depends on how many shuffle operations need to take place)\u003c/li\u003e\n\u003cli\u003eEach stage contains one more tasks\u003c/li\u003e\n\u003cli\u003eEach task corresponds to one partition\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534054371689_1189419278",
      "id": "20180812-061251_1411858997",
      "dateCreated": "Aug 12, 2018 6:12:51 AM",
      "dateStarted": "Aug 12, 2018 7:52:47 AM",
      "dateFinished": "Aug 12, 2018 7:52:47 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ndf1 \u003d spark.range(2, 10000000, 2)\ndf2 \u003d spark.range(2, 10000000, 4)\n\nstep1 \u003d df1.repartition(5)\nstep12 \u003d df2.repartition(6)\nstep2 \u003d step1.selectExpr(\"id * 5 as id\")\nstep3 \u003d step2.join(step12, [\"id\"])\nstep4 \u003d step3.selectExpr(\"sum(id)\")\n\nstep4.collect()\nstep4.explain()",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 7:52:47 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534055061587_1213086531",
      "id": "20180812-062421_1230145085",
      "dateCreated": "Aug 12, 2018 6:24:21 AM",
      "dateStarted": "Aug 12, 2018 7:52:48 AM",
      "dateFinished": "Aug 12, 2018 7:34:52 AM",
      "status": "RUNNING",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### This job breaks down into the following stages and tasks:\n\n        - Stage 1 with 2 Tasks\n        - Stage 2 with 8 Tasks\n        - Stage 3 with 6 Tasks\n        - Stage 4 with 5 Tasks\n        - Stage 5 with 200 Tasks\n        - Stage 6 with 1 Task\n\n### Stages\n\n- Stages in Spark represent groups of tasks that can be executed together to compute the same operation on multiple machines. \n- In general, Spark will try to pack as much work as possible (i.e., as many transformations as possible inside your job) into the same stage, but the engine starts new stages after operations called shuffles. \n- A shuffle represents a physical repartitioning of the data—for example, sorting a DataFrame, or grouping data that was loaded from a file by key (which requires sending records with the same key to the same node).\n- This type of repartitioning requires coordinating across executors to move data around. \n- Spark starts a new stage after each shuffle, and keeps track of what order the stages must run in to compute the final result.\n- In the job we looked at earlier : \n            - the first two stages correspond to the range that you perform in order to create your DataFrames. By default when you create a DataFrame with range, it has 2 partitions. \n            - The next step is the repartitioning. This changes the number of partitions by shuffling the data. These DataFrames are shuffled into six partitions and five partitions, corresponding to the number of tasks in stages 3 and 4.\n            - Stages 3 and 4 perform on each of those DataFrames and the end of the stage represents the join (a shuffle). Suddenly, we have 200 tasks. This is because of a Spark SQL configuration. \n            - The spark.sql.shuffle.partitions default value is 200, which means that when there is a shuffle performed during execution, it outputs 200 shuffle partitions by default. \n            - You can change this value, and the number of output partitions will change. \n        \n            ```\n              spark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n              \n            ```\n- A good rule of thumb is that the number of partitions should be larger than the number of executors on your cluster, potentially by multiple factors depending on the workload. \n- If you are running code on your local machine, it would behoove you to set this value lower because your local machine is unlikely to be able to execute that number of tasks in parallel.\n- This is more of a default for a cluster in which there might be many more executor cores to use. \n- Regardless of the number of partitions, that entire stage is computed in parallel. \n- The final result aggregates those partitions individually, brings them all to a single partition before finally sending the final result to the driver\n\n\n### Tasks\n\n- Stages in Spark consist of tasks. \n- Each task corresponds to a combination of blocks of data and a set of transformations that will run on a single executor. \n- If there is one big partition in our dataset, we will have one task. \n- If there are 1,000 little partitions, we will have 1,000 tasks that can be executed in parallel. \n- A task is just a unit of computation applied to a unit of data (the partition).\n- Partitioning your data into a greater number of partitions means that more can be executed in parallel. This is not a panacea, but it is a simple place to begin with optimization.\n\n### Execution Details\n\n- Spark automatically pipelines stages and tasks that can be done together, such as a map operation followed by another map operation. \n- For all shuffle operations, Spark writes the data to stable storage (e.g., disk), and can reuse it across multiple jobs. \n\n#### Pipelining\n\n- An important part of what makes Spark an “in-memory computation tool” is that unlike the tools that came before it (e.g., MapReduce), Spark performs as many steps as it can at one point in time before writing data to memory or disk. \n- One of the key optimizations that Spark performs is pipelining, which occurs at and below the RDD level.\n- With pipelining, any sequence of operations that feed data directly into each other, without needing to move it across nodes, is collapsed into a single stage of tasks that do all the operations together. \n- For example, if you write an RDD-based program that does a map, then a filter, then another map, these will result in a single stage of tasks that immediately read each input record, pass it through the first map, pass it through the filter, and pass it through the last map function if needed. \n- This pipelined version of the computation is much faster than writing the intermediate results to memory or disk after each step. \n- The same kind of pipelining happens for a DataFrame or SQL computation that does a select, filter, and select.\n\n#### Shuffle Persistence\n - When Spark needs to run an operation that has to move data across nodes, such as a reduce-by-key operation (where input data for each key needs to first be brought together from many nodes), the engine can’t perform pipelining anymore, and instead it performs a cross-network shuffle. \n - Spark always executes shuffles by first having the “source” tasks (those sending data) write shuffle files to their local disks during their execution stage. \n - Then, the stage that does the grouping and reduction launches and runs tasks that fetch their corresponding records from each shuffle file and performs that computation (e.g., fetches and processes the data for a specific range of keys).\n - Saving the shuffle files to disk lets Spark run this stage later in time than the source stage (e.g., if there are not enough executors to run both at the same time), and also lets the engine re-launch reduce tasks on failure without rerunning all the input tasks.\n - One side effect you’ll see for shuffle persistence is that running a new job over data that’s already been shuffled does not rerun the “source” side of the shuffle. \n - Because the shuffle files were already written to disk earlier, Spark knows that it can use them to run the later stages of the job, and it need not redo the earlier ones. \n - For even better performance you can perform your own caching with the DataFrame or RDD cache method, which lets you control exactly which data is saved and where. \n - You’ll quickly grow accustomed to this behavior after you run some Spark actions on aggregated data and inspect them in the UI",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 7:52:48 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch4\u003eThis job breaks down into the following stages and tasks:\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003e    - Stage 1 with 2 Tasks\n    - Stage 2 with 8 Tasks\n    - Stage 3 with 6 Tasks\n    - Stage 4 with 5 Tasks\n    - Stage 5 with 200 Tasks\n    - Stage 6 with 1 Task\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eStages\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eStages in Spark represent groups of tasks that can be executed together to compute the same operation on multiple machines.\u003c/li\u003e\n\u003cli\u003eIn general, Spark will try to pack as much work as possible (i.e., as many transformations as possible inside your job) into the same stage, but the engine starts new stages after operations called shuffles.\u003c/li\u003e\n\u003cli\u003eA shuffle represents a physical repartitioning of the data—for example, sorting a DataFrame, or grouping data that was loaded from a file by key (which requires sending records with the same key to the same node).\u003c/li\u003e\n\u003cli\u003eThis type of repartitioning requires coordinating across executors to move data around.\u003c/li\u003e\n\u003cli\u003eSpark starts a new stage after each shuffle, and keeps track of what order the stages must run in to compute the final result.\u003c/li\u003e\n\u003cli\u003eIn the job we looked at earlier :\u003cpre\u003e\u003ccode\u003e    - the first two stages correspond to the range that you perform in order to create your DataFrames. By default when you create a DataFrame with range, it has 2 partitions. \n    - The next step is the repartitioning. This changes the number of partitions by shuffling the data. These DataFrames are shuffled into six partitions and five partitions, corresponding to the number of tasks in stages 3 and 4.\n    - Stages 3 and 4 perform on each of those DataFrames and the end of the stage represents the join (a shuffle). Suddenly, we have 200 tasks. This is because of a Spark SQL configuration. \n    - The spark.sql.shuffle.partitions default value is 200, which means that when there is a shuffle performed during execution, it outputs 200 shuffle partitions by default. \n    - You can change this value, and the number of output partitions will change. \n\n    ```\n      spark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n\n    ```\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003eA good rule of thumb is that the number of partitions should be larger than the number of executors on your cluster, potentially by multiple factors depending on the workload.\u003c/li\u003e\n\u003cli\u003eIf you are running code on your local machine, it would behoove you to set this value lower because your local machine is unlikely to be able to execute that number of tasks in parallel.\u003c/li\u003e\n\u003cli\u003eThis is more of a default for a cluster in which there might be many more executor cores to use.\u003c/li\u003e\n\u003cli\u003eRegardless of the number of partitions, that entire stage is computed in parallel.\u003c/li\u003e\n\u003cli\u003eThe final result aggregates those partitions individually, brings them all to a single partition before finally sending the final result to the driver\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eTasks\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eStages in Spark consist of tasks.\u003c/li\u003e\n\u003cli\u003eEach task corresponds to a combination of blocks of data and a set of transformations that will run on a single executor.\u003c/li\u003e\n\u003cli\u003eIf there is one big partition in our dataset, we will have one task.\u003c/li\u003e\n\u003cli\u003eIf there are 1,000 little partitions, we will have 1,000 tasks that can be executed in parallel.\u003c/li\u003e\n\u003cli\u003eA task is just a unit of computation applied to a unit of data (the partition).\u003c/li\u003e\n\u003cli\u003ePartitioning your data into a greater number of partitions means that more can be executed in parallel. This is not a panacea, but it is a simple place to begin with optimization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eExecution Details\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSpark automatically pipelines stages and tasks that can be done together, such as a map operation followed by another map operation.\u003c/li\u003e\n\u003cli\u003eFor all shuffle operations, Spark writes the data to stable storage (e.g., disk), and can reuse it across multiple jobs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003ePipelining\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eAn important part of what makes Spark an “in-memory computation tool” is that unlike the tools that came before it (e.g., MapReduce), Spark performs as many steps as it can at one point in time before writing data to memory or disk.\u003c/li\u003e\n\u003cli\u003eOne of the key optimizations that Spark performs is pipelining, which occurs at and below the RDD level.\u003c/li\u003e\n\u003cli\u003eWith pipelining, any sequence of operations that feed data directly into each other, without needing to move it across nodes, is collapsed into a single stage of tasks that do all the operations together.\u003c/li\u003e\n\u003cli\u003eFor example, if you write an RDD-based program that does a map, then a filter, then another map, these will result in a single stage of tasks that immediately read each input record, pass it through the first map, pass it through the filter, and pass it through the last map function if needed.\u003c/li\u003e\n\u003cli\u003eThis pipelined version of the computation is much faster than writing the intermediate results to memory or disk after each step.\u003c/li\u003e\n\u003cli\u003eThe same kind of pipelining happens for a DataFrame or SQL computation that does a select, filter, and select.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eShuffle Persistence\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eWhen Spark needs to run an operation that has to move data across nodes, such as a reduce-by-key operation (where input data for each key needs to first be brought together from many nodes), the engine can’t perform pipelining anymore, and instead it performs a cross-network shuffle.\u003c/li\u003e\n\u003cli\u003eSpark always executes shuffles by first having the “source” tasks (those sending data) write shuffle files to their local disks during their execution stage.\u003c/li\u003e\n\u003cli\u003eThen, the stage that does the grouping and reduction launches and runs tasks that fetch their corresponding records from each shuffle file and performs that computation (e.g., fetches and processes the data for a specific range of keys).\u003c/li\u003e\n\u003cli\u003eSaving the shuffle files to disk lets Spark run this stage later in time than the source stage (e.g., if there are not enough executors to run both at the same time), and also lets the engine re-launch reduce tasks on failure without rerunning all the input tasks.\u003c/li\u003e\n\u003cli\u003eOne side effect you’ll see for shuffle persistence is that running a new job over data that’s already been shuffled does not rerun the “source” side of the shuffle.\u003c/li\u003e\n\u003cli\u003eBecause the shuffle files were already written to disk earlier, Spark knows that it can use them to run the later stages of the job, and it need not redo the earlier ones.\u003c/li\u003e\n\u003cli\u003eFor even better performance you can perform your own caching with the DataFrame or RDD cache method, which lets you control exactly which data is saved and where.\u003c/li\u003e\n\u003cli\u003eYou’ll quickly grow accustomed to this behavior after you run some Spark actions on aggregated data and inspect them in the UI\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534059511178_2000543586",
      "id": "20180812-073831_367091313",
      "dateCreated": "Aug 12, 2018 7:38:31 AM",
      "dateStarted": "Aug 12, 2018 7:52:48 AM",
      "dateFinished": "Aug 12, 2018 7:52:48 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\n",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 7:53:28 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534059304794_1896774109",
      "id": "20180812-073504_998005059",
      "dateCreated": "Aug 12, 2018 7:35:04 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Spark Execution Model",
  "id": "2DQ1KQ3ZH",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}