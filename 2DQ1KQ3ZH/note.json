{
  "paragraphs": [
    {
      "text": "%md\n### Architecture of a Spark Application\n- high-level components of a Spark Application\n        - The Spark driver\n            -  The driver is the process “in the driver seat” of your Spark Application. \n            -  It is the controller of the execution of a Spark Application and maintains all of the state of the Spark cluster (the state and tasks of the executors).\n            -  It must interface with the cluster manager in order to actually get physical resources and launch executors.\n            -  At the end of the day, this is just a process on a physical machine that is responsible for maintaining the state of the application running on the cluster.\n        - The Spark executors\n            - Spark executors are the processes that perform the tasks assigned by the Spark driver. \n            - Executors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results. \n            - Each Spark Application has its own separate executor processes.\n        - The cluster manager\n           - The Spark Driver and Executors do not exist in a void, and this is where the cluster manager comes in. \n           - The cluster manager is responsible for maintaining a cluster of machines that will run your Spark Application(s). \n           - Somewhat confusingly, a cluster manager will have its own “driver” (sometimes called master) and “worker” abstractions. \n           - The core difference is that these are tied to physical machines rather than processes (as they are in Spark)\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999202-74047dac-9e25-11e8-8336-c7e76c17a438.pn)\n\n- When it comes time to actually run a Spark Application, we request resources from the cluster manager to run it. \n- Over the course of Spark Application execution, the cluster manager will be responsible for managing the underlying machines that our application is running on.\n\n### Execution Modes\n\n- An execution mode gives you the power to determine where the resources are physically located when you go to run your application. \n- You have three modes to choose from:Cluster modeClient modeLocal mode\n        - Cluster mode\n        - Client mode\n        - Local mode\n\n##### Cluster mode\n\n- Cluster mode is probably the most common way of running Spark Applications.\n- In cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager. \n- The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. \n- This means that the cluster manager is responsible for maintaining all Spark Application–related processes.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999237-17ec3932-9e26-11e8-8e47-3e846410d129.png)\n\n##### Client mode\n\n- Client mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application. \n- This means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses. \n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999239-20c21496-9e26-11e8-9c2a-8913c04a3705.png)\n\n##### Local mode\n\n- Local mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine. \n- It achieves parallelism through threads on that single machine. \n- This is a common way to learn Spark, to test your applications, or experiment iteratively with local development. \n- However, we do not use local mode for running production applications.\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 6:23:56 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534054371689_1189419278",
      "id": "20180812-061251_1411858997",
      "dateCreated": "Aug 12, 2018 6:12:51 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Spark Execution Model",
  "id": "2DQ1KQ3ZH",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}