{
  "paragraphs": [
    {
      "text": "%md\n### Architecture of a Spark Application\n\n- The Spark driver\n    -  The driver is the process “in the driver seat” of your Spark Application. \n    -  It is the controller of the execution of a Spark Application and maintains all of the state of the Spark cluster (the state and tasks of the executors).\n    -  It must interface with the cluster manager in order to actually get physical resources and launch executors.\n    -  At the end of the day, this is just a process on a physical machine that is responsible for maintaining the state of the application running on the cluster.\n\n- The Spark executors\n    - Spark executors are the processes that perform the tasks assigned by the Spark driver. \n    - Executors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results. \n    - Each Spark Application has its own separate executor processes.\n\n- The cluster manager\n    - The Spark Driver and Executors do not exist in a void, and this is where the cluster manager comes in. \n    - The cluster manager is responsible for maintaining a cluster of machines that will run your Spark Application(s). \n    - Somewhat confusingly, a cluster manager will have its own “driver” (sometimes called master) and “worker” abstractions. \n    - The core difference is that these are tied to physical machines rather than processes (as they are in Spark)\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999202-74047dac-9e25-11e8-8336-c7e76c17a438.png)\n\n- When it comes time to actually run a Spark Application, we request resources from the cluster manager to run it. \n- Over the course of Spark Application execution, the cluster manager will be responsible for managing the underlying machines that our application is running on.\n\n### Execution Modes\n\n- An execution mode gives you the power to determine where the resources are physically located when you go to run your application. \n- You have three modes to choose from:Cluster modeClient modeLocal mode\n        - Cluster mode\n        - Client mode\n        - Local mode\n\n#### Cluster mode\n\n- Cluster mode is probably the most common way of running Spark Applications.\n- In cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager. \n- The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. \n- This means that the cluster manager is responsible for maintaining all Spark Application–related processes.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999237-17ec3932-9e26-11e8-8e47-3e846410d129.png)\n\n#### Client mode\n\n- Client mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application. \n- This means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses. \n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999239-20c21496-9e26-11e8-9c2a-8913c04a3705.png)\n\n#### Local mode\n\n- Local mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine. \n- It achieves parallelism through threads on that single machine. \n- This is a common way to learn Spark, to test your applications, or experiment iteratively with local development. \n- However, we do not use local mode for running production applications.\n\n\n### The Life Cycle of a Spark Application (Outside Spark) using spark-submit\n\n#### Client Request\n\n- The first step is for you to submit an actual application. This will be a pre-compiled JAR or library. \n- At this point, you are executing code on your local machine and you’re going to make a request to the cluster manager driver node.\n- Here, we are explicitly asking for resources for the Spark driver process only. \n- We assume that the cluster manager accepts this offer and places the driver onto a node in the cluster. \n- The client process that submitted the original job exits and the application is off and running on the cluster.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999286-2e1c219e-9e27-11e8-8f1f-8e70b7cf8028.png)\n\n\n##### To do this, you’ll run something like the following command in your terminal:\n\n```\n./bin/spark-submit \\  --class \u003cmain-class\u003e \\  --master \u003cmaster-url\u003e \\  --deploy-mode cluster \\  --conf \u003ckey\u003e\u003d\u003cvalue\u003e \\  ... # other options  \u003capplication-jar\u003e \\  [application-arguments]\n\n```\n\n#### Launch\n\n- Now that the driver process has been placed on the cluster, it begins running user code. \n- This code must include a SparkSession that initializes a Spark cluster (e.g., driver + executors). \n- The SparkSession will subsequently communicate with the cluster manager, asking it to launch Spark executor processes across the cluster. \n- The number of executors and their relevant configurations are set by the user via the command-line arguments in the original spark-submit call.\n- The cluster manager responds by launching the executor processes (assuming all goes well) and sends the relevant information about their locations to the driver process. \n- After everything is hooked up correctly, we have a “Spark Cluster”.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999322-cac43d56-9e27-11e8-99c5-5115ceecf157.png)\n\n\n#### Execution\n\n- The driver and the workers communicate among themselves, executing code and moving data around. \n- The driver schedules tasks onto each worker, and each worker responds with the status of those tasks and success or failure.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999337-27978f2e-9e28-11e8-81f7-58392f5f84b0.png)\n\n\n#### Completion\n\n- After a Spark Application completes, the driver processs exits with either success or failure. \n- The cluster manager then shuts down the executors in that Spark cluster for the driver.\n- At this point, you can see the success or failure of the Spark Application by asking the cluster manager for this information.\n\n![sv-image](https://user-images.githubusercontent.com/1182329/43999339-29e23d92-9e28-11e8-985b-3dabdb97fa2d.pngg)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 6:37:46 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eArchitecture of a Spark Application\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eThe Spark driver\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe driver is the process “in the driver seat” of your Spark Application.\u003c/li\u003e\n\u003cli\u003eIt is the controller of the execution of a Spark Application and maintains all of the state of the Spark cluster (the state and tasks of the executors).\u003c/li\u003e\n\u003cli\u003eIt must interface with the cluster manager in order to actually get physical resources and launch executors.\u003c/li\u003e\n\u003cli\u003eAt the end of the day, this is just a process on a physical machine that is responsible for maintaining the state of the application running on the cluster.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThe Spark executors\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSpark executors are the processes that perform the tasks assigned by the Spark driver.\u003c/li\u003e\n\u003cli\u003eExecutors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results.\u003c/li\u003e\n\u003cli\u003eEach Spark Application has its own separate executor processes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThe cluster manager\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe Spark Driver and Executors do not exist in a void, and this is where the cluster manager comes in.\u003c/li\u003e\n\u003cli\u003eThe cluster manager is responsible for maintaining a cluster of machines that will run your Spark Application(s).\u003c/li\u003e\n\u003cli\u003eSomewhat confusingly, a cluster manager will have its own “driver” (sometimes called master) and “worker” abstractions.\u003c/li\u003e\n\u003cli\u003eThe core difference is that these are tied to physical machines rather than processes (as they are in Spark)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999202-74047dac-9e25-11e8-8336-c7e76c17a438.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhen it comes time to actually run a Spark Application, we request resources from the cluster manager to run it.\u003c/li\u003e\n\u003cli\u003eOver the course of Spark Application execution, the cluster manager will be responsible for managing the underlying machines that our application is running on.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eExecution Modes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAn execution mode gives you the power to determine where the resources are physically located when you go to run your application.\u003c/li\u003e\n\u003cli\u003eYou have three modes to choose from:Cluster modeClient modeLocal mode\u003cpre\u003e\u003ccode\u003e- Cluster mode\n- Client mode\n- Local mode\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eCluster mode\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCluster mode is probably the most common way of running Spark Applications.\u003c/li\u003e\n\u003cli\u003eIn cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager.\u003c/li\u003e\n\u003cli\u003eThe cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes.\u003c/li\u003e\n\u003cli\u003eThis means that the cluster manager is responsible for maintaining all Spark Application–related processes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999237-17ec3932-9e26-11e8-8e47-3e846410d129.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch4\u003eClient mode\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eClient mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application.\u003c/li\u003e\n\u003cli\u003eThis means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999239-20c21496-9e26-11e8-9c2a-8913c04a3705.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003ch4\u003eLocal mode\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eLocal mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine.\u003c/li\u003e\n\u003cli\u003eIt achieves parallelism through threads on that single machine.\u003c/li\u003e\n\u003cli\u003eThis is a common way to learn Spark, to test your applications, or experiment iteratively with local development.\u003c/li\u003e\n\u003cli\u003eHowever, we do not use local mode for running production applications.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eThe Life Cycle of a Spark Application (Outside Spark) using spark-submit\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe first step is for you to submit an actual application. This will be a pre-compiled JAR or library.\u003c/li\u003e\n\u003cli\u003eAt this point, you are executing code on your local machine and you’re going to make a request to the cluster manager driver node.\u003c/li\u003e\n\u003cli\u003eHere, we are explicitly asking for resources for the Spark driver process only.\u003c/li\u003e\n\u003cli\u003eWe assume that the cluster manager accepts this offer and places the driver onto a node in the cluster.\u003c/li\u003e\n\u003cli\u003eThe client process that submitted the original job exits and the application is off and running on the cluster.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/1182329/43999286-2e1c219e-9e27-11e8-8f1f-8e70b7cf8028.png\" alt\u003d\"sv-image\" /\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTo do this, you’ll run something like the following command in your terminal:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e./bin/spark-submit \\  --class \u0026lt;main-class\u0026gt; \\  --master \u0026lt;master-url\u0026gt; \\  --deploy-mode cluster \\  --conf \u0026lt;key\u0026gt;\u003d\u0026lt;value\u0026gt; \\  ... # other options  \u0026lt;application-jar\u0026gt; \\  [application-arguments]\n\u003c/code\u003e\u003c/pre\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1534054371689_1189419278",
      "id": "20180812-061251_1411858997",
      "dateCreated": "Aug 12, 2018 6:12:51 AM",
      "dateStarted": "Aug 12, 2018 6:30:52 AM",
      "dateFinished": "Aug 12, 2018 6:30:52 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "Aug 12, 2018 6:24:21 AM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1534055061587_1213086531",
      "id": "20180812-062421_1230145085",
      "dateCreated": "Aug 12, 2018 6:24:21 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Spark Execution Model",
  "id": "2DQ1KQ3ZH",
  "angularObjects": {
    "2CHS8UYQQ:shared_process": [],
    "2C8A4SZ9T_livy2:shared_process": [],
    "2CK8A9MEG:shared_process": [],
    "2C4U48MY3_spark2:shared_process": [],
    "2CKAY1A8Y:shared_process": [],
    "2CKEKWY8Z:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}